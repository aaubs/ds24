[{"uri":"https://aaubs.github.io/ds24/en/m1/04_sml/01-sml-intro/","title":"- Introduction to Supervised ML","tags":[],"description":"","content":"\nThis session introduces supervised machine learning (SML) and related main concepts.\nNotebook(s) Introduction to Supervised Machine Learning\nAssignment for SML\nIntro slides Use arrows keys on keyboard to navigate. Alternatively fullscreen slides\nRecommended Datacamp exercises: Basics:\nPython - Intro to supervised learning Python - Decision Tree modeling Modell workflows\nPython - SML Preprocessing Python - SML Feature Engineering Python - SML Model validation Recommended Readings and resources Python Data Science Handbook Chapter 5 "},{"uri":"https://aaubs.github.io/ds24/en/m1/03_uml/01_intro_uml/","title":"- Introduction to Unsupervised ML","tags":[],"description":"","content":"\nThis session will introduce the principles and applications of unsupervised machine learning (UML). Students will learn about the different types of UML problems, and they will explore some of the most popular UML algorithms, such as PCA, SVD, and NMF.\nNotebook(s) Hands-on Intro to Dimensionality reduction and Clustering\nAssignment for the UML\nAssignment for the UML Solution\nRecommended Datacamp exercises: Python Recommended Readings and resources Python Data Science Handbook Chapter 5\nWhat Is Machine Learning? Introducing Scikit-Learn Feature Engineering In Depth: Principal Component Analysis In Depth: k-Means Clustering Implementation tutorials on YT PCA and K-means from this list\nIntro slides Use arrows keys on keyboard to navigate. Alternatively fullscreen slides\n"},{"uri":"https://aaubs.github.io/ds24/en/m1/01_intro_ds/01_welcome_ds/","title":"- Welcome Students!","tags":[],"description":"","content":"\nThis session will introduce you to the fundamentals of data science, with a focus on Python. We will cover the Python data science stack, essential tools and platforms, software setup, semester overview, and Python 101.\nSession 1: Welcome Students! This session sets the stage for your data science journey:\nPython Data Science Stack: Dive into Python\u0026rsquo;s core data science libraries and frameworks. We\u0026rsquo;ve got you covered!\nEcosystem Deep Dive: Familiarize yourself with essential tools and platforms, such as Github, UCloud, Google Colab, and Jupyter. These will be integral to your studies and projects.\nSoftware Setup: We\u0026rsquo;ll guide you through installing the crucial software. And don\u0026rsquo;t worry, our Teaching Assistants are here to assist with any challenges.\nSemester Overview: Get a glimpse of what the upcoming weeks hold for you.\nPython 101: We\u0026rsquo;ll ensure everyone is up to speed with Python basics.\nNotebooks How to build a development environment using Colab, Google Drive, GitHub, and Kaggle!\nüöÄ Notebook: Python 101\n"},{"uri":"https://aaubs.github.io/ds24/en/m2/01_networks/1_networks/","title":"Basics Network Analysis","tags":[],"description":"","content":"\nThis session introduces basic concepts of network theory and analysis.\nRecommended Datacamp exercises: Introduction to Network Analysis in Python Notebooks Basics Network Analysis Intermediate Network Analysis Slides Use arrows keys on keyboard to navigate. Alternatively use fullscreen slides\n"},{"uri":"https://aaubs.github.io/ds24/en/m2/02_nlp/1-nlp-intro-sml/","title":"Basics of NLP","tags":[],"description":"","content":" Medieval clergy, smartphones and dogs 2023. Roman x Stable Diffusion XL\nThis session introduces basic concepts of NLP. We will take a Problem-Based-Learning approach and I will introduce NLP-related concepts as we go. If you need a more theoretical intro (standalone), I\u0026rsquo;ll uploaded pre-recorded videos. We will start with a project based on\nContext This assignment is based on data from this paper\nDavidson, T., Warmsley, D., Macy, M., \u0026amp; Weber, I. (2017, May). Automated hate speech detection and the problem of offensive language. In Proceedings of the international AAAI conference on web and social media (Vol. 11, No. 1, pp. 512-515).\nYou are given a collection of approximately 25k tweets that have been manually (human) annotated. class denotes: 0 - hate speech, 1 - offensive language, 2 - neither\nhttps://github.com/SDS-AAU/SDS-master/raw/master/M2/data/twitter_hate.zip\n1. Preprocessing and vectorizaion. Justify your choices and explain possible alternatives (e.g. removing stopwords, identifying bi/tri-grams, removing verbs or use of stemming, lemmatization etc.)\nCreate a bag-of-words representation, apply TF-IDF and dimensionality reduction (LSA-topic modelling alternatively simply PCA or SVD) to transform your corpus into a feature matrix. 2. Explore and compare the 2 \u0026ldquo;classes of interest\u0026rdquo; - hate speech vs offensive language. Can you see differences by using simple count-based approaches? Can you identify themes (aka clusters / topics) that are specific for one class or another? Explore them using, e.g. simple crosstabs - topic vs. class and to get more detailed insights within-cluster top (TF-IDF) terms. (This step requires preprocessed/tokenized inputs). 3. Build an ML model that can predict hate speech Use the ML pipeline (learned in M1) to build a classification model that can identify offensive language and hate speech. It is not an easy task to get good results. Experiment with different models on the two types of text-representations that you create in 2.\nBonus: Explore missclassified hate speech tweets vs those correctly predicted. Can you find specific patterns? Can you observe some topics that are more prevalent in those that the model identifies correcly?\nThe best-reported results for this dataset are.\nClass Precision 0 0.61 1 0.91 2 0.95 Overall 0.91 Notebook This notebook contains an extended solution.\nHate Speech Detection Analysis of Political Tweets during the 2020 Presidential Debate Context We\u0026rsquo;ll analyze political tweets from around the time of the US Presidential Debate in October 2020, with a primary focus on distinguishing tweets from Democrats and Republicans. Additionally, we will explore discussed topics and comprehend the factors driving the predictions.\nDatasets Political tweets: Preprocessed and labeled (1: Democrats, 0: Republicans).\nhttps://github.com/SDS-AAU/SDS-master/raw/master/M2/data/pol_tweets.gz Debate tweets: From around the time of the debate in October 2020 (8,000 entries).\nhttps://github.com/SDS-AAU/SDS-master/raw/master/M2/data/pres_debate_2020.gz Both datasets are in JSON format.\nTask Overview Primary Task: Build a classifier that can distinguish between Dem/Rep tweets. Bonus: Explore the discussed topics. Identify what influences predictions using LIME. Step-by-Step Instructions 1. Data Preparation and Preprocessing a. Data Import Load the datasets using pandas\u0026rsquo; read_json method. b. Text Preprocessing Utilize tweet-preprocessor for cleaning tweets (remove URLs, numbers, reserved words, mentions, and emojis). Use SpaCy for lemmatization, lowercasing, and removal of stop words, punctuation, and non-alphabetic characters. c. Text Vectorization Implement TfidfVectorizer to convert the text data into numerical format suitable for ML. d. Handle Imbalanced Data (if needed) Employ a method such as random under-sampling to handle class imbalances. 2. Model Building a. Model Selection Choose a classification model (e.g., Logistic Regression, XGBoost). b. Model Training Split the data into training and testing sets. Train the model using the training data. c. Model Evaluation Evaluate the model using appropriate metrics like precision, recall, f1-score, and accuracy. 3. Model Explainability using LIME Implement lime to explain the predictions made by the model. Analyze and interpret the results, focusing on how different features (words) impact the predictions. 4. Topic Modelling a. Text Tokenization Tokenize the processed text, focusing on nouns and adjectives. b. Model Fitting Employ LdaMulticore for topic modeling and determine the topics discussed in the tweets. Optionally, you might also explore other models like LSI. c. Visualization Use pyLDAvis to visualize the topics and understand their distribution and significance. 5. App Development using Gradio a. UI Design Design a user interface with input for a tweet and output for the predicted label and explanation. b. Integration Integrate the developed model and LIME explanations to return outputs for given tweet inputs. (optional) c. Deployment Optionally, deploy the app for broader access. 6. Analysis and Conclusion Compile your findings, providing insights into distinguishing features, discussed topics, and model performance. Comment on the reliability and usability of your model and app. Notes and Tips Data Privacy: Ensure to comply with ethical and legal guidelines related to data usage and AI. Exploration: Consider visualizing word distributions and token frequencies. Documentation: Ensure to document each step and findings thoroughly in your Jupyter notebook. Optimization: Consider refining the model for better accuracy and reliability. Technologies \u0026amp; Libraries Data Handling: pandas, numpy Text Preprocessing: tweet-preprocessor, SpaCy Modeling: scikit-learn, xgboost, gensim Explainability: lime Visualization: altair, pyLDAvis App Development: gradio Important: Ensure to install all the necessary packages using pip (e.g., !pip install lime gradio).\nGood luck, and may your model classify accurately and provide enlightening explanations!\nNotebook from class "},{"uri":"https://aaubs.github.io/ds24/en/m5/02_presentations/","title":"In-calss Presentations","tags":[],"description":"","content":"Presentation format Students will deliver 20-25 minute presentations on AI, safety, and ethics, inspired by assigned documents. They will also develop 2-3 questions or interactive formats for class engagement. Topics include politics and safety, planning for AGI, and issues like bias, toxicity, and misinformation in large language models. Presentations may be published on a GitHub page with accompanying blog posts.\n"},{"uri":"https://aaubs.github.io/ds24/en/info/","title":"Info, Schedule &amp; Co","tags":[],"description":"","content":"General info about the semester will be updated here. Check out the calendar on moodle.\nIntro to the semester and module: "},{"uri":"https://aaubs.github.io/ds24/en/m3/01_intro-to-traditional-dl/","title":"Intro to Traditional Deep Learning","tags":[],"description":"","content":"This session provides an overview of the foundational elements of deep learning, including its historical context, key concepts, and practical applications. The course will delve into various types of neural networks, outlining their advantages and disadvantages. It will specifically focus on convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory (LSTM) networks, highlighting their unique characteristics and applicability to a range of problem-solving scenarios, including those in economics.\nAuto Insurance in Sweden Swedish Committee on Analysis of Risk Premium in Motor Insurance. read more\nOverview In the dataset:\nX = number of claims Y = total payment for all the claims in thousands of Swedish Kronor for geographical zones in Sweden Reference: Swedish Committee on Analysis of Risk Premium in Motor Insurance\nTasks First step: We will create a simple Artificial Neural Network with 1 node and training with 1 sample of data Second step: The simple Artificial Neural Network will be trained through the dataset Notebooks Intro to Traditional Deep Learning - ANN Intro to Traditional Deep Learning - RNN Intro to Traditional Deep Learning - LSTM Intro to Traditional Deep Learning - CNN Intro to Activation Functions Intro to Gradient Descent Exercises ANN Exercise RNN Exercise LSTM Exercise Solutions ANN Exercise and Solutions LSTM Exercise and Solutions Recommended Datacamp exercises Deep Learning with PyTorch Slides Use arrows keys on keyboard to navigate. Alternatively use fullscreen slides.\nResources PyTorch PyTorch NN Linear "},{"uri":"https://aaubs.github.io/ds24/en/m1/00_basics/01_intro_datascience/","title":"Introduction to Data Science (W 35)","tags":[],"description":"","content":"\nIn this session, you\u0026rsquo;ll dive into the exciting world of data science, centered around Python. We\u0026rsquo;ll explore key concepts like Python data science libraries, essential tools, data manipulation, uncovering insights from data, important statistics, data visualization, and much more. Get ready to roll up your sleeves and put your learning into action with hands-on exercises and projects. It\u0026rsquo;s a chance to sharpen your skills while having some real fun with data!\nPart 1: Welcome Students! This session sets the stage for your data science journey:\nPython Data Science Stack: Dive into Python\u0026rsquo;s core data science libraries and frameworks. We\u0026rsquo;ve got you covered!\nEcosystem Deep Dive: Familiarize yourself with essential tools and platforms, such as Github, UCloud, Google Colab, and Jupyter. These will be integral to your studies and projects.\nSoftware Setup: We\u0026rsquo;ll guide you through installing the crucial software. And don\u0026rsquo;t worry, our Teaching Assistants are here to assist with any challenges.\nSemester Overview: Get a glimpse of what the upcoming weeks hold for you.\nPython 101: We\u0026rsquo;ll ensure everyone is up to speed with Python basics.\nüöÄ Notebook: Python 101\nPart 2: Data Handling and Manipulation I (Lecture) In this session, we\u0026rsquo;ll explore the foundational aspects of data handling. Key takeaways include:\nData Manipulation: Learn essential operations like arrange, group-by, filter, select, and join, preparing data for analysis. Notebook: Python Data Manipulation Exercises Notebook: Python DS Handbook, C.2-3 exercises\nNotebook: Pandas exercises\nHere, you will find the answers to the exercises:\nNotebook: Python DS Handbook, C.2-3 exercises and solutions\nNotebook: Pandas exercises and solutions\nFurther studies Recommended DataCamp courses Introduction to Python Intermediate Python Recommended readings Python fo Data Science Handbook (VanderPlas, 2016), Chapter 2-3 "},{"uri":"https://aaubs.github.io/ds24/en/m1/02_rapid_prototyping/01_rapid_prototyping/","title":"Introduction to Streamlit: Building an Employee Attrition Dashboard","tags":[],"description":"","content":" Corgi working on a Data Science project. 2023. Roman x Stable Diffusion XL\nStreamlit has rapidly become a go-to tool for data scientists and developers wanting to turn data scripts into shareable web apps. Let\u0026rsquo;s explore its core features and benefits:\nKey Features of Streamlit Simplicity: With just a few lines of Python code, you can have a running web application. No need to deal with HTML, CSS, or JavaScript unless you want to. Interactive Widgets: Streamlit offers out-of-the-box widgets like sliders, buttons, and text inputs that make your app interactive. Data Integration: It seamlessly integrates with popular data science libraries like Pandas, Numpy, Matplotlib, and others. Data Caching: With @st.cache, Streamlit caches the output of functions, ensuring your data operations are efficient and your apps remain performant. Hot Reloading: As you save changes to your script, the app refreshes in real-time. No need to manually restart your app. vs. Gradio While Streamlit offers a robust platform for creating web apps, Gradio provides several distinct advantages, especially when the focus is on deploying machine learning models:\nEase of Model Deployment: Gradio prioritizes simplifying the deployment of machine learning models. Its intuitive Python API allows for quick interface generation, ensuring accessibility even for non-experts. Diverse Input Types: Gradio supports a wide variety of input formats, from images and text to audio, providing flexibility especially when dealing with different data types. Multi-Model Deployment: Gradio stands out with its capability to simultaneously deploy multiple models, perfect for ensemble methods or side-by-side model comparisons. Shareability: Gradio\u0026rsquo;s feature of generating shareable URLs makes collaboration and showcasing a breeze. Security Features: Gradio offers built-in adversarial robustness, adding an extra layer of protection against potential adversarial attacks on deployed models. These unique features make Gradio an attractive option for projects that focus on deploying and sharing machine learning models with diverse requirements. We are going to explore gradio after introductions to ML.\nWhy Choose Streamlit? Rapid Prototyping: Streamlit\u0026rsquo;s intuitive API and hot reloading mean you can quickly iterate and refine your app. Open Source: Being open-source, it boasts a strong community that contributes to its growth and offers a plethora of community plugins. Deployment Ready: With platforms like Streamlit Sharing, deploying your app to the world is just a click away. Extensible: You can integrate advanced JavaScript features or even other Python libraries to extend Streamlit\u0026rsquo;s capabilities. Core Components of Streamlit Layout and Widgets Layout: st.columns and st.container can be used to design your app\u0026rsquo;s layout. Widgets: These are interactive elements like st.slider(), st.selectbox(), and st.button() that capture user input. Display Elements Media: Display images, videos, or audio clips using st.image(), st.video(), and st.audio(). Charts: Use st.line_chart(), st.bar_chart(), or integrate with libraries like Altair for custom visualizations. Tables: Showcase data with st.table() or st.dataframe(). Session State State: Store user data or app state across reruns with st.session_state. Dive Deeper To truly master Streamlit, it\u0026rsquo;s recommended to experiment with building various apps and exploring its official documentation. The community is active, and there\u0026rsquo;s always something new to learn!\nWhat You\u0026rsquo;ll Build By the end of this tutorial, you\u0026rsquo;ll have a functional dashboard that allows users to:\nFilter data based on various employee metrics. Visualize attrition rates and patterns across different dimensions. Understand insights from visualizations. Get actionable recommendations based on data insights. Prerequisites Before you begin, ensure you have the following:\nBasic knowledge of Python. Streamlit installed (pip install streamlit). Familiarity with data visualization libraries like altair, matplotlib, and seaborn. Getting Started 1. Data Source We\u0026rsquo;ll use a synthetic dataset on employee attrition from this link.\n2. File Structure Ensure your working directory has the following structure:\nüì¶Your_Directory ‚î£ üìúapp.py ‚îó üìúrequirements.txt ‚îó üìúdata.csv app.py will contain our Streamlit app\u0026rsquo;s code, while requirements.txt will list the necessary Python packages.\n3. Hosting the App Once you\u0026rsquo;ve built the app, we\u0026rsquo;ll first host it on Streamlit Cloud and then on uCloud (A Danish private cloud for universities).\nBuilding the Dashboard Here is the code\nInitialization We start by importing necessary libraries and loading our dataset. The data is cached using @st.cache_data to enhance performance.\nDesigning the Interface Streamlit provides intuitive functions to design the user interface:\nst.title() and st.header() set titles and headers. st.markdown() allows for rich text formatting. st.sidebar lets you add interactive widgets in a sidebar for filtering. Visualizations Depending on user input, we visualize the filtered data using various charts. For instance, altair is used for bar and pie charts, while matplotlib and seaborn provide KDE plots and boxplots.\nInsights and Recommendations Finally, expanders provide a space to share insights derived from visualizations and actionable recommendations.\nAccess the Deployed App You can access the final deployed Employee Attrition Dashboard on Streamlit using the link below:\nEmployee Attrition Dashboard\nFeel free to explore the app, interact with the various filters and visualizations, and gain insights into employee attrition patterns.\nConclusion Streamlit offers a user-friendly platform to build and deploy interactive data apps without the need for extensive web development skills. Dive in, explore the code, and customize it to create your own data-driven web applications.\nHappy Coding! üöÄ\nOld corgy from last year\u0026hellip; Corgi working on a Data Science project. 2022. Roman x Stable Diffusion\n"},{"uri":"https://aaubs.github.io/ds24/en/m6/l1/","title":"Lecture 1  Introduction to Serverless ML and Databases","tags":[],"description":"","content":"\nUse GitHub repo https://github.com/saoter/SDS24_MLOps_L1, which contains lecutre slides, python scripts and dataset for the first lecture.\nSLIDES Lecture 1 API api_jokes.py is an example of simple public API api_finance.py, api_news.py are examples of private API using API_key (You need to register to get your private key) api_datasets.py is an example of API Wrapper EXTRA: joke_app.py is a simple streamlit app that uses IPA to print jokes EXTRA: api_weather.py exercise from the class Database db_create.py reads in excel file and transform it into SQL database db_queries.py contains a few query examples db_add_datapoints.py add new transactions db_add_columns.py add new columns data/Online Retail.xlsx is dataset used to create database "},{"uri":"https://aaubs.github.io/ds24/en/m6/l2/","title":"Lecture 2  Refactoring &amp; First Serverless App","tags":[],"description":"","content":"\nSLIDES Lecture 2 "},{"uri":"https://aaubs.github.io/ds24/en/m6/l3/","title":"Lecture 3: Credit Card Prediction Service Project","tags":[],"description":"","content":"\nSlides Original Slides from the Serverless ML course Exercise Clone repository and execute the feature-pipelines Modify parameters in the feature-pipeline to produce "},{"uri":"https://aaubs.github.io/ds24/en/m6/l5/","title":"Lecture 5  Feature Selection, Batch Inference Pipelines, Model Registry","tags":[],"description":"","content":"\rUse GitHub repo https://github.com/saoter/SDS24_MLOps_L5, which contains lecutre slides, python scripts and dataset for the this lecture.\nSLIDES Lecture 5 TASKS First, we continue with Hopsworks Module 3 LAB\nSecond, we start with two-lecture project\nFiles in repo:\n- 1_data_to_sql.ipynb\r- 2_train_first_model.ipynb\r- 3_train_additional_models.ipynb\r"},{"uri":"https://aaubs.github.io/ds24/en/m6/l6/","title":"Lecture 6  Feature Selection, Batch Inference Pipelines, Model Registry","tags":[],"description":"","content":"\rSLIDES Lecture 6 TASKS First, we continue with Hopsworks Module 4 LAB\nSecond, we will continue with our exercise from Lecture 5\nRepositories:\n- https://github.com/saoter/render_api\r- https://github.com/saoter/render_streamlit\r"},{"uri":"https://aaubs.github.io/ds24/en/m5/01_legal/","title":"LEgal","tags":[],"description":"","content":"This chapter introduces you to the basics of deep learning, including its history, important concepts, and applications.\nLiterature Literature: LeCun, Y., Bengio, Y., \u0026amp; Hinton, G. (2015) Recommended Datacamp exercises Deep Learning with PyTorch "},{"uri":"https://aaubs.github.io/ds24/en/m1/01_intro_ds/02_data_handeling/","title":"- Data Handling and Manipulation","tags":[],"description":"","content":"\nThis session will introduce students to the foundational aspects of data handling in Python. Students will learn about the different types of data that are important in data science, and they will explore essential operations like arrange, group-by, filter, select, and join. By the end of this session, students should have a solid understanding of primary data manipulation techniques, setting the stage for more advanced subjects.\nSession 2: Data Handling and Manipulation I (Lecture) In this session, we\u0026rsquo;ll explore the foundational aspects of data handling. Key takeaways include:\nData Manipulation: Learn essential operations like arrange, group-by, filter, select, and join, preparing data for analysis. Notebook: Python Data Manipulation By the end, students should have a solid understanding of primary data manipulation techniques, setting the stage for more advanced subjects.\nExercises Notebook: Python DS Handbook, C.2-3 exercises\nNotebook: Pandas exercises\nNotebook: Netflix Pandas exercises\nNotebook: HR Attrition Assignment\nHere, you will find the answers to the exercises:\nNotebook: Python DS Handbook, C.2-3 exercises and solutions\nNotebook: Pandas exercises and solutions\nNotebook: HR Attrition Assignment solutions\nNotebook: Netflix Pandas exercise solutions\nTutorial - Assignment Examples Notebook: EDA Assignment Example - Policing Further studies Recommended DataCamp courses Introduction to Python Intermediate Python Recommended readings Python fo Data Science Handbook (VanderPlas, 2016), Chapter 2-3 "},{"uri":"https://aaubs.github.io/ds24/en/m1/03_uml/02_recommender_simsea_uml/","title":"- Recommendation and Similarity Search","tags":[],"description":"","content":"\nIn this workshop we are going to learn about recommender systems as a type of UML. Such systems are probably the most widely used and commercialy valuable form of AI today. Specifically we will be looking into collaborative filtering and matrix factorization.\nPlan for today Collaborative filtering / SVD recommender using Nomadlist Trips-data in a Notebooks Streamlit recommender-app Notebook(s) Nomadlist Trips Notebook Nomadlist Trips Notebook Solutions Recap Nomadlist Trips Notebook Recap Similarity Notebook Recap Dimensionality Reduction Class UML Exemple cosine similarity App Source code for the app Deployed recommender app Recommended Readings and resources This excellent PyData Talk by the developer of LightFM Recommended Datacamp exercises: Beginner Tutorial RecSys-Course "},{"uri":"https://aaubs.github.io/ds24/en/m1/04_sml/02-sml-addon/","title":"- SML - Further topics","tags":[],"description":"","content":"\nThis session introduces adittional topics of intertest in SML\nNotebook(s) Introduction to Explainable ML Data engineeering \u0026amp;amp; pipelines Recommended Datacamp exercises: \u0026hellip; Recommended Readings and resources \u0026hellip; "},{"uri":"https://aaubs.github.io/ds24/en/m2/01_networks/2_networks_2mode/","title":"2 Mode Networks","tags":[],"description":"","content":"\nThis session introduces to multimodel (2+) network analysis concepts.\nRecommended Datacamp exercises: Intermediate Network Analysis in Python Notebooks 2-Mode Networks "},{"uri":"https://aaubs.github.io/ds24/en/m1/01_intro_ds/","title":"A) Introduction to Data Science (W35-36)","tags":[],"description":"","content":" Note: Group Portfolio Assignment - Exploratory Data Analysis (EDA) Deadline: Friday, 8 September 2023, 12:00 PM\nThis topic includes 5 sessions as follows:\nWelcome to Data Science! (Friday, September 1st, 10:15-14:15): This session will introduce students to the fundamentals of data science, with a focus on Python. Students will learn about the Python data science stack, essential tools and platforms, and software setup. They will also get a preview of the upcoming weeks and a refresher on Python basics. Data Handling and Manipulation I (Lecture) (Monday, September 4th, 12:30-16:15): This session will cover the foundational aspects of data handling in Python. Students will learn about the different types of data that are important in data science, and they will explore essential operations like arrange, group-by, filter, select, and join. By the end of this session, students should have a solid understanding of primary data manipulation techniques. Exploratory Data Analysis \u0026amp; Essential Statistics (Tuesday, September 5th, 08:15-12:00): This session will introduce students to exploratory data analysis (EDA) and essential statistics. Students will learn how to use EDA to uncover patterns, anomalies, and frame questions in data. They will also learn about foundational measures and techniques for data interpretation. EDA-Exercise on a Real Dataset (Tuesday, September 5th, 12:30-14:15): This session will give students the opportunity to practice the EDA techniques they learned earlier on a real dataset. Students will be able to dive into a real dataset, identify patterns and insights, and implement key EDA methods. They will also benefit from on-the-spot guidance by the teacher and TAs during the exercise. Data Visualization in Data Science (Wednesday, September 6th, 10:15-14:15): This session will teach students the importance of effective data visualization in data science. Students will explore Seaborn, a Python library for intuitive statistical graphics, and Altair, a declarative visualization library for Python. They will also have the opportunity to create impactful visualizations with real datasets through hands-on exercises. "},{"uri":"https://aaubs.github.io/ds24/en/m1/","title":"Applied Data Science and Machine Learning","tags":[],"description":"","content":"M1 - Applied Data Science and Machine Learning Note: M1 - Final Assignment Deadline:\nThis module provides a condensed introduction to the ‚ÄúData Science Pipeline,‚Äù introducing students to methods, techniques, and workflows in applied data analytics and machine learning, including data acquisition, preparation, analysis, visualization, and communication. This module includes the following four main topics:\nA) Introduction to Data Science B) Rapid Prototyping C) Unsupervised Machine Learning D) Supervised Machine Learning "},{"uri":"https://aaubs.github.io/ds24/en/m1/00_basics/02_data_visualization_stat/","title":"Exploratory Data Analysis and Essential Statistics (W 36)","tags":[],"description":"","content":"The Exploratory Data Analysis and Essential Statistics introduces students how to use EDA and some fundamental concepts of statistical methods to uncover patterns, anomalies, and frame questions in data. Students will learn foundational measures and techniques for data interpretation, and they will have the opportunity to apply EDA and statistical methods on datasets through hands-on exercises.\n‚ÄºÔ∏è Group Portfolio Assignment - Exploratory Data Analysis (EDA) ‚ÄºÔ∏è\nDeadline: Friday, 8 September 2023, 12:00 PM\nPart 1: Exploratory Data Analysis \u0026amp; Essential Statistics Deep Dive into EDA: Uncover patterns, anomalies, and frame questions. Key Statistical Concepts: Foundational measures and techniques for data interpretation. Hands-on Analysis: Apply EDA and statistical methods on datasets. Part 2: EDA-Exercise on a Real Dataset Dataset Exploration: Dive into a real dataset, identifying patterns and insights. Practical Techniques: Implement key EDA methods learned earlier. Guided Assistance: Benefit from on-the-spot guidance by the teacher and TAs during the exercise. Part 3: Data Visualization in Data Science Foundations of Visualization: Understand the importance of effective data visualization in Data Science and the principles that drive meaningful visuals. Seaborn Mastery: Explore Seaborn, a Python library for intuitive statistical graphics. Altair Exploration: Delve into Altair, a declarative visualization library for Python. Hands-on Visualization: Harness both Seaborn and Altair to craft impactful visualizations with real datasets. Notebooks Notebook Dataviz Slides Use arrows keys on keyboard to navigate. Alternatively, fullscreen slides here\n"},{"uri":"https://aaubs.github.io/ds24/en/m3/01_group_assignment/","title":"Group Assignment 1","tags":[],"description":"","content":"Portfolio Exercise 1 Note: M3 - Group Assignment 1 Deadline: Wednesday 7th of February at 12.00 PM\nIntroduction In this assignment, you are required to delve into the practical aspects of Deep Learning by constructing and evaluating a neural network using PyTorch. This exercise is designed to deepen your understanding of neural network architectures, hyperparameter tuning, and the preprocessing steps necessary for effective model training and evaluation. You will have the freedom to choose a dataset from either the M1 or M2 module or select an external dataset that intrigues you. By experimenting with different neural network configurations and hyperparameters, you will gain hands-on experience in optimizing ML models to achieve desired performance metrics.\nTask Build, train, and evaluate a neural network using Pytorch. The neural network should have a minimum of 2 hidden layers. Experiment with at least 5 different variations of hyperparameters (e.g., number of layers/neurons, activation functions, epochs, optimizers, learning rates, etc.). The assignment should include the following steps:\nFeature Selection Feature Engineering (if necessary) Standard ML Preprocessing (if necessary) Train-Test Split Defining a Neural Network Architecture in Pytorch Defining a Training Loop Training the Model Experimenting with Different Hyperparameters Evaluating the Final Model on the Test Data Data Choose a dataset from the M1 or M2 module, or other datasets if you prefer. Delivery Create a GitHub repository. Save the Colab notebook in the repository. Provide a README.md with a brief description of the assignment. Submissions can be made in groups of up to 3 members. Submit the assignment by sending an email with the link to the repository to Hamid (hamidb@business.aau.dk). "},{"uri":"https://aaubs.github.io/ds24/en/m3_eco/01_group_assignment/","title":"Group Assignment 1","tags":[],"description":"","content":"Portfolio Exercise 1 Note: M3 - Group Assignment 1 Deadline: Monday, 6th of November at 12:00 PM\nIntroduction In this assignment, you are required to delve into the practical aspects of Deep Learning by constructing and evaluating a neural network using PyTorch. This exercise is designed to deepen your understanding of neural network architectures, hyperparameter tuning, and the preprocessing steps necessary for effective model training and evaluation. You will have the freedom to choose a dataset from either the M1 or M2 module or select an external dataset that intrigues you. By experimenting with different neural network configurations and hyperparameters, you will gain hands-on experience in optimizing ML models to achieve desired performance metrics.\nTask Build, train, and evaluate a neural network using Pytorch. The neural network should have a minimum of 2 hidden layers. Experiment with at least 5 different variations of hyperparameters (e.g., number of layers/neurons, activation functions, epochs, optimizers, learning rates, etc.). The assignment should include the following steps:\nFeature Selection Feature Engineering (if necessary) Standard ML Preprocessing (if necessary) Train-Test Split Defining a Neural Network Architecture in Pytorch Defining a Training Loop Training the Model Experimenting with Different Hyperparameters Evaluating the Final Model on the Test Data Data Choose a dataset from the M1 or M2 module, or other datasets if you prefer. Delivery Create a GitHub repository. Save the Colab notebook in the repository. Provide a README.md with a brief description of the assignment. Submissions can be made in groups of up to 3 members. Submit the assignment by sending an email with the link to the repository to Hamid (hamidb@business.aau.dk). "},{"uri":"https://aaubs.github.io/ds24/en/m2/02_nlp/","title":"Natural Language Processing","tags":[],"description":"","content":"This chapter introduces you to statistical natural language processing. We will be focusing on NLP in combination with supervised ML as well as topic modelling for unsupervised approaches.\nRecommended Datacamp Courses:\nPython Intro to NLP Python - also good Feature Engineering for NLP in Python Recommended Readings:\nBird, S., Klein, E., \u0026amp; Loper, E. (2009). Natural language processing with Python: analyzing text with the natural language toolkit. \u0026quot; O\u0026rsquo;Reilly Media, Inc.\u0026quot;. Docs and linked papers: Gensim Docs Spacy Pre-recorded theory intro recording from 2021, in case you\u0026rsquo;d like to review\nNLP intro - level of analysis Text representation From BoW to Topic Modeling and Embeddings (optional) History of NLP in Industry - Yoav Goldberg "},{"uri":"https://aaubs.github.io/ds24/en/m2/01_networks/3_exercises/","title":"NW Exercises","tags":[],"description":"","content":"Exercise 1: Manager Networks Introduction In this exercise, you will replicate a well known network analysis, with different data and some twists. Data: The data is to be found at: https://github.com/SDS-AAU/SDS-master/tree/master/00_data/network_krackhard (Hint: You need to download the raw data) Data: What do I get? Background Let the fun begin. You will analyze network datacollected from the managers of a high-tec company. This dataset, originating from the paper below, is widely used in research on organizational networks. Time to give it a shot as well. Krackhardt D. (1987). Cognitive social structures. Social Networks, 9, 104-134. The company manufactured high-tech equipment on the west coast of the United States and had just over 100 employees with 21 managers. Each manager was asked to whom do you go to for advice and who is your friend, to whom do you report was taken from company documents. Description\nThe dataset includes 4 files - 3xKrack-High-Tec and 1x High-Tec-Attributes. Krack-High-Tec includes the following three 21x3 text matrices:\nADVICE, directed, binary FRIENDSHIP, directed, binary REPORTS_TO, directed, binary Column 1 contains the ID of the ego (from where the edge starts), and column 2 the alter (to which the edge goes). Column 3 indicates the presence (=1) or absence (=0) of an edge.\nHigh-Tec-Attributes includes one 21x4 valued matrix.\nID: Numeric ID of the manager AGE: The managers age (in years) TENURE: The length of service or tenure (in years) LEVEL: The level in the corporate hierarchy (coded 1,2 and 3; 1 = CEO, 2 = Vice President, 3 = manager) DEPT: The department (coded 1,2,3,4 with the CEO in department 0, ie not in a department) Tasks 1. Create a network Generate network objects for the companies organizational structure (reports to), friendship, advice This networks are generated from the corresponding edgelists Also attach node characteristics from the corresponding nodelist 2. Analysis Make a little analysis on:\nA: Network level characteristics. Find the overal network level of:\nDensity Transistivity (Clustering Coefficient) Reciprocity \u0026hellip; for the different networks. Describe and interpret the results. Answer the following questions:\nAre relationships like friendship and advice giving usually reciprocal? Are friends of your friends also your friends? Are the employees generally more likely to be in a friendship or advice-seeking relationship? B: Node level characteristics: Likewise, find out:\nWho is most popular in the networks. Who is the most wanted friend, and advice giver? Are managers in higher hirarchy more popular as friend, and advice giver? C: Relational Characteristics: Answer the following questions:\nAre managers from the same 1. department, or on the same 2. hirarchy, 3. age, or 4. tenuere more likely to become friends or give advice? (hint: assortiativity related) Are friends more likely to give each others advice? 3. Visualization Everything goes. Show us some pretty and informative plots. Choose what to plot, and how, on your own. Interpret the results and share some insights.\nExercise 2: Crime Network Data In this exercise, you will analyze a crime network, which you will find in the following notebook: Notebook: crime network Tasks First of all, construct the 2-mode network (code provided). Then, project it on th two modes, so that you end up with:\nA network with persons as nodes, connected by jointly commited crimes A network with crimes as nodes, connected by persons jointly commiting them. Then, try to solve th following tasks:\nfind the crime(s) that have the most shared connections with other crimes find the individual(s) that have the most shared connections with other individuals Which persons are implicated in the most number of crimes? Then, there is also a bonus exercise which is a bit more complicated:\nFind the people that can help with investigating a crime\u0026rsquo;s person: Let\u0026rsquo;s pretend that we are a detective trying to solve a crime, and that we right now need to find other individuals who were not implicated in the same exact crime as an individual was, but who might be able to give us information about that individual because they were implicated in other crimes with that individual. "},{"uri":"https://aaubs.github.io/ds24/en/m1/02_rapid_prototyping/01.5_streamlit_offline/","title":"Streamlit Development &amp; Running Offline","tags":[],"description":"","content":"Streamlit provides an easy and rapid way to turn data scripts into interactive web apps. However, at times, developers may need to ensure that their Streamlit applications run offline. Here\u0026rsquo;s a simple guide to setting up Streamlit in an offline environment using Anaconda and Visual Studio Code (VSCode).\nSetup with Anaconda Environment Follow these steps to set up Streamlit within an Anaconda environment:\nInstall Anaconda \u0026amp; VSCode\nDownload and install Anaconda. Download and install VSCode. Create a Python Conda Environment\nconda create --name bds-streamlit python==3.11.4 Activate the Environment\nconda activate bds-streamlit Install pipreqs\npipreqs is a handy tool that scans through your project files, detects the Python imports, and subsequently generates a requirements.txt file. pip install pipreqs Generate requirements.txt\npipreqs Install the Necessary Packages\npip install -r requirements.txt Alternative Setup without Anaconda Environment If you\u0026rsquo;d rather not use a specific environment:\nInstall Anaconda \u0026amp; VSCode (as mentioned above).\nInstall the Required Packages Directly\npip install pandas streamlit matplotlib altair seaborn Running the Streamlit App Navigate to Your Project Directory\ncd path/to/your/app_directory Run the App\nstreamlit run app.py Alternatively (especially for Windows users), you can execute: python -m streamlit run app.py With these steps, you\u0026rsquo;re all set to develop and run your Streamlit apps offline! Whether you\u0026rsquo;re using an Anaconda environment or not, the process is straightforward and ensures you have all the necessary dependencies installed.\n"},{"uri":"https://aaubs.github.io/ds24/en/m1/01_intro_ds/03_data_visualization_stat/","title":"- Exploratory Data Analysis and Essential Statistics","tags":[],"description":"","content":"This session introduces students how to use EDA and some fundemental concepts of statistical methods to uncover patterns, anomalies, and frame questions in data. Students will learn foundational measures and techniques for data interpretation, and they will have the opportunity to apply EDA and statistical methods on datasets through hands-on exercises.\nDeep Dive into EDA: Uncover patterns, anomalies, and frame questions. Key Statistical Concepts: Foundational measures and techniques for data interpretation. Hands-on Analysis: Apply EDA and statistical methods on datasets. Part 1: Statistics refresher Notebook statistics refresher Part 2: Further concepts Notebook propability distributions Notebook AB testing Further studies Recommended DataCamp courses Intyroduction to statistics (no coding) Statistical Thinking in Python I Statistical Thinking in Python II Statistical Simulation in Python Recommended readings Econometrics with Python - Causal Inference for The Brave and True: More thorrough inferential statistics in Python ###‚Ç¨ Further resources\n"},{"uri":"https://aaubs.github.io/ds24/en/m1/02_rapid_prototyping/02_online_dashboard/","title":"- Real World Data to Online Dashboard","tags":[],"description":"","content":"This session will focus on building interactive online dashboards using real-world geospatial data, with an emphasis on GeoPandas for data manipulation and visualization. Students will learn how to use GeoPandas to read, filter, and manipulate geospatial data, create interactive map-based visualizations, and ultimately deploy their dashboards to the web.\nIntroduction to GeoPandas Using GeoPandas to analyze geospatial data will be our focus in these notebooks.\nGeoPandas GeoPandas and Solutions GeoPandas Hands-on Project GeoPandas Hands-on Project and Solutions GeoPandas Exercises GeoPandas exercises GeoPandas exercises and solutions From Geopandas to Streamlit App üöÄ Simplified version of the analysis - with Folium Plotting App code Deployed App What to do now?! Replay code from the course and see if you do understand the core mechanics - you DO NOT need to remember everything. Android app market project on datacamp Course: Python DS toolbox 1 \u0026amp; Course: Python DS toolbox 2 Opendata.dk - build a map of different attractions in Aalborg based on public data. See preprocessing example - how to get data out of nested JSON - below: This is how you can preprocess the GeoCoordinates from the JSON file:\n#Load pandas import pandas as pd # Read the file from remote data = pd.read_json(\u0026#39;https://admin.opendata.dk/dataset/44ecd686-5cb5-40f2-8e3f-b5e3607a55ef/resource/eeabb0f8-1b19-4c80-b059-5ba5c4c872d2/download/guidedenmarkaalborgenjson.json\u0026#39;) # The GeoCoordinates are hiding in the Address column data[\u0026#39;Address\u0026#39;][0][\u0026#39;GeoCoordinate\u0026#39;] # You can use list comprehension to pull out GeoCoordinates (also empty values) - try out # This will allow you to filter for missing data without fancy workarounds [x[\u0026#39;GeoCoordinate\u0026#39;] for x in data[\u0026#39;Address\u0026#39;]] # Make a new column based on that to be used for filtering out missing data data[\u0026#39;GeoCoordinate\u0026#39;] = [x[\u0026#39;GeoCoordinate\u0026#39;] for x in data[\u0026#39;Address\u0026#39;]] # drop, where no GeoCoordinate data = data.dropna(subset=[\u0026#39;GeoCoordinate\u0026#39;]) # Pull out the values data[\u0026#39;latitude\u0026#39;] = [x[\u0026#39;Latitude\u0026#39;] for x in data[\u0026#39;GeoCoordinate\u0026#39;]] data[\u0026#39;longitude\u0026#39;] = [x[\u0026#39;Longitude\u0026#39;] for x in data[\u0026#39;GeoCoordinate\u0026#39;]] "},{"uri":"https://aaubs.github.io/ds24/en/m1/04_sml/03-sml-ts/","title":"- Time Series Forecasts","tags":[],"description":"","content":"\nThis session introduces to time series analysis and forecasting\nNotebook(s) Introduction to Timeseries Forecasting Recommended Datacamp exercises: Python - Time Series Analysis Python - SML for Time Series Data Recommended Readings and resources \u0026hellip; "},{"uri":"https://aaubs.github.io/ds24/en/m1/02_rapid_prototyping/","title":"B) Rapid Prototyping (W37)","tags":[],"description":"","content":" Note: Dashboard-Hackathon Submission Deadline: Friday, 15 September 2023, 12:00\nThis topic includes 3 sessions as follows:\nRapid Prototyping with Streamlit \u0026amp; Gradio (Monday, September 11th, 10:15-14:15): This session will cover the basics of rapid prototyping in data science, with a focus on the Streamlit and Gradio libraries. Students will learn how to use these libraries to create interactive web applications that can be used to explore data and test hypotheses. Real-World Data to Online Dashboard (Tuesday, September 12th, 12:30-16:15): This session will focus on creating interactive online dashboards using real-world data. Students will learn how to select and clean data, create visualizations, and deploy their dashboards to the web. Dashboard-Hackathon Kick-off (Tuesday, September 12th, 16:30-18:15): This session will kick off the Data Science Dashboard Hackathon. Students will have the opportunity to compete to create the most compelling and user-friendly interactive online dashboard using Streamlit. "},{"uri":"https://aaubs.github.io/ds24/en/m3/02_intro-tm/","title":"Intro to Transformer Models","tags":[],"description":"","content":"\nLiterature Sutskever, I., Vinyals, O., \u0026amp;amp; Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., \u0026amp;hellip; \u0026amp;amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.\nThe illustrated transformer\nSimple transformer LM\nNotebooks - Basics Transformer Models - Basics Notebooks - Applications TM Applications - SBERT TM Applications - HF Simple transformer LM SBERT for Patent Search using PatentSBERTa in PyTorch Notebooks - FineTuning TM FineTuning - SimpleTransformers TM FineTuning - SBERT TM FineTuning - HF SetFit Hatespeech vs bert and distilroberta Seq2Seq - Neural Machine Translation Slides - Attention Mechanism Slides - SBERT Classification with various vectorization approaches TF-IDF and W2V Multi-Class Text Classification BERT Multi-Class Text Classification Implementing Multi-Class Text Classification LSTMs using PyTorch Resources OG SBERT-Paper Reimers, N., \u0026amp;amp; Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084. SBERT Docu NLP with SBERT - an ebook/course on the use of dense vectors (with SBERT for business applications) SBERT-Training Tutorial BERTopic - a framework for topic modelling with SBERT embeddings Milvus - Vector database "},{"uri":"https://aaubs.github.io/ds24/en/m3_eco/02_intro-tm/","title":"Intro to Transformer Models","tags":[],"description":"","content":"\nLiterature Sutskever, I., Vinyals, O., \u0026amp;amp; Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., \u0026amp;hellip; \u0026amp;amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.\nThe illustrated transformer\nSimple transformer LM\nNotebooks - Basics Transformer Models - Basics Notebooks - Applications TM Applications - SBERT TM Applications - HF Simple transformer LM SBERT for Patent Search using PatentSBERTa in PyTorch Notebooks - FineTuning TM FineTuning - SimpleTransformers TM FineTuning - SBERT TM FineTuning - HF SetFit Hatespeech vs bert and distilroberta Seq2Seq - Neural Machine Translation Slides - Attention Mechanism Slides - SBERT Classification with various vectorization approaches TF-IDF and W2V Multi-Class Text Classification BERT Multi-Class Text Classification Implementing Multi-Class Text Classification LSTMs using PyTorch Resources OG SBERT-Paper Reimers, N., \u0026amp;amp; Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084. SBERT Docu NLP with SBERT - an ebook/course on the use of dense vectors (with SBERT for business applications) SBERT-Training Tutorial BERTopic - a framework for topic modelling with SBERT embeddings Milvus - Vector database "},{"uri":"https://aaubs.github.io/ds24/en/info/02_modules/","title":"Modules","tags":[],"description":"","content":"For Business Data Science Students M1: Data Handling, Exploration \u0026amp; Applied Machine Learning 10 ECTS\nThis module will prove a condensed introduction to the ‚ÄúData Science Pipeline‚Äù, introducing students to methods, techniques, and workflows in applied data analytics and machine learning, including data acquisition, preparation, analysis, visualization, and communication.\nM2: Network Analysis and Natural Language Processing 5 ECTS\nFocuses on analyzing a variety of unstructured data sources. Particularly, students will learn how to explore, analyze, and visualize natural language (text) as well as relational (network) data.\nM3: Data-Driven Business Modelling and Strategy 15 ECTS Course with integrated project in which you will learn how companies plan, prepare and execute data-driven projects. In the project you will work wich a company case and build a \u0026ldquo;mini\u0026rdquo; version of the product/process.\nFor Social Data Science Students - Elective Semester M3: (SDS) Deep Learning and Artificial Intelligence for Analytics 5 ECTS\nIntroduces to the most recent developments in machine learning, which are deep learning and artificial intelligence applications. The module will provide a solid foundation for this exciting and rapidly developing field. Students will learn whether and how to apply deep learning techniques for business analytics, and acquire proficiency in new methods autonomously.\nCapstone Project Semester project utilising techniques and approaches from SDS in the context of a problem related to your main study field.\n"},{"uri":"https://aaubs.github.io/ds24/en/m2/01_networks/","title":"Network Analysis","tags":[],"description":"","content":"\nThis chapter introduces you to network analysis and working with relational data.\n"},{"uri":"https://aaubs.github.io/ds24/en/m2/02_nlp/3-nlp-advanced/","title":"NLP Applications Chatbot","tags":[],"description":"","content":" Roman x Stable Diffusion\nChatbots are one of the most wide spread NLP applications. In this tutorial we will build a simple retrieval chatbot that can be used for example as an alternative for FAQ applications in companies.\nThe approach is following:\nTrain a model on variants of a question. Take input and predict the type of question asked - this is called \u0026ldquo;intent\u0026rdquo; Reply with a pre-defined response corresponding to the question asked. Modern bots are more complex. They evaluate the whole (or large parts of the) dialogue. In addition some have the capacity to generate text.\nThe iput data looks like this:\nüëâ Expand to see JSON... { \u0026#34;intents\u0026#34;: [ { \u0026#34;tag\u0026#34;: \u0026#34;greeting\u0026#34;, \u0026#34;patterns\u0026#34;: [ \u0026#34;Hi\u0026#34;, \u0026#34;Hey\u0026#34;, \u0026#34;How are you\u0026#34;, \u0026#34;Is anyone there?\u0026#34;, \u0026#34;Hello\u0026#34;, \u0026#34;Good day\u0026#34; ], \u0026#34;responses\u0026#34;: [ \u0026#34;Hey :-)\u0026#34;, \u0026#34;Hello, thanks for visiting\u0026#34;, \u0026#34;Hi there, what can I do for you?\u0026#34;, \u0026#34;Hi there, how can I help?\u0026#34; ] }, { \u0026#34;tag\u0026#34;: \u0026#34;goodbye\u0026#34;, \u0026#34;patterns\u0026#34;: [\u0026#34;Bye\u0026#34;, \u0026#34;See you later\u0026#34;, \u0026#34;Goodbye\u0026#34;], \u0026#34;responses\u0026#34;: [ \u0026#34;See you later, thanks for visiting\u0026#34;, \u0026#34;Have a nice day\u0026#34;, \u0026#34;Bye! Come back again soon.\u0026#34; ] }, { \u0026#34;tag\u0026#34;: \u0026#34;thanks\u0026#34;, \u0026#34;patterns\u0026#34;: [\u0026#34;Thanks\u0026#34;, \u0026#34;Thank you\u0026#34;, \u0026#34;That\u0026#39;s helpful\u0026#34;, \u0026#34;Thank\u0026#39;s a lot!\u0026#34;], \u0026#34;responses\u0026#34;: [\u0026#34;Happy to help!\u0026#34;, \u0026#34;Any time!\u0026#34;, \u0026#34;My pleasure\u0026#34;] }, { \u0026#34;tag\u0026#34;: \u0026#34;items\u0026#34;, \u0026#34;patterns\u0026#34;: [ \u0026#34;Which items do you have?\u0026#34;, \u0026#34;What kinds of items are there?\u0026#34;, \u0026#34;What do you sell?\u0026#34; ], \u0026#34;responses\u0026#34;: [ \u0026#34;We sell coffee and tea\u0026#34;, \u0026#34;We have coffee and tea\u0026#34; ] }, { \u0026#34;tag\u0026#34;: \u0026#34;payments\u0026#34;, \u0026#34;patterns\u0026#34;: [ \u0026#34;Do you take credit cards?\u0026#34;, \u0026#34;Do you accept Mastercard?\u0026#34;, \u0026#34;Can I pay with Paypal?\u0026#34;, \u0026#34;Are you cash only?\u0026#34; ], \u0026#34;responses\u0026#34;: [ \u0026#34;We accept VISA, Mastercard and Paypal\u0026#34;, \u0026#34;We accept most major credit cards, and Paypal\u0026#34; ] }, { \u0026#34;tag\u0026#34;: \u0026#34;delivery\u0026#34;, \u0026#34;patterns\u0026#34;: [ \u0026#34;How long does delivery take?\u0026#34;, \u0026#34;How long does shipping take?\u0026#34;, \u0026#34;When do I get my delivery?\u0026#34; ], \u0026#34;responses\u0026#34;: [ \u0026#34;Delivery takes 2-4 days\u0026#34;, \u0026#34;Shipping takes 2-4 days\u0026#34; ] }, { \u0026#34;tag\u0026#34;: \u0026#34;funny\u0026#34;, \u0026#34;patterns\u0026#34;: [ \u0026#34;Tell me a joke!\u0026#34;, \u0026#34;Tell me something funny!\u0026#34;, \u0026#34;Do you know a joke?\u0026#34; ], \u0026#34;responses\u0026#34;: [ \u0026#34;Why did the hipster burn his mouth? He drank the coffee before it was cool.\u0026#34;, \u0026#34;What did the buffalo say when his son left for college? Bison.\u0026#34; ] } ] } Notebook Training Chatbot\nAssignment Create an intent classifier Use following data from HF: fathyshalab/atis-flight\nBuild an FAQ bot Use folloqing dataset from HF: Andyrasika/Ecommerce_FAQ\nAAU Handbook-bot (Roman solution) The provided code in the notebook aims to establish a simple text-based question-answering bot utilizing pre-trained Floret word vectors and spaCy to process textual data. Upon receiving a user-input question, the bot evaluates its similarity with pre-existing questions in its dataset using cosine similarity, attempting to find the most pertinent match. If a sufficiently similar question is identified (based on a pre-defined similarity threshold), the bot provides the corresponding answer; otherwise, it prompts the user with the closest matching questions from the dataset, requesting a more precise inquiry.\nSimilarity-based Chatbot\n"},{"uri":"https://aaubs.github.io/ds24/en/m1/00_basics/03-rapid_prototyping/","title":"Rapid Prototyping (W 37)","tags":[],"description":"","content":" Corgi working on a Data Science project. 2022. Roman x Stable Diffusion\nThe Rapid Prototyping with Streamlit \u0026amp; Gradio course will teach students how to use Streamlit and Gradio to create interactive web applications and dashboards. Students will learn the basics of rapid prototyping, and they will explore how Streamlit and Gradio can be used to turn data scripts into interactive web applications. They will also learn how to deploy their applications as web apps.\nIn the Real-World Data to Online Dashboard session, students will start with a real-world dataset and use it to create an interactive online dashboard. They will learn how to visualize geographic data effectively, and they will integrate the geographic visualizations into their dashboard. They will also learn how to host and deploy their dashboard online.\nThe Dashboard-Hackathon Kick-off session will give students the opportunity to compete in a hackathon to create the most compelling and user-friendly dashboard using Streamlit. TAs will be on hand to help kickstart ideas, troubleshoot, and refine dashboards. The final dashboards should be hosted and shared by September 15th, and great prizes await the top dashboards!\nPart1: Rapid Prototyping with Streamlit \u0026amp; Gradio Prototyping Foundations: Grasp why rapid prototyping is essential in Data Science and its role in iterative development and deployment. Streamlit Introduction: Dive into Streamlit, a Python library that makes it easy to turn data scripts into interactive web applications. Gradio Exploration: Learn about Gradio, an interface library for quickly creating demos and UIs for machine learning models. Deployment: Deployment of projects as WebApps\nPart 1: AirBnb In this notebook we will be using data from AirBnb for some basic EDA and geoplotting\nEDA and Geoviz starter EDA and Geoviz class Part 2: Kaggle In this notebook we will be learning how to work with data from Kaggle as well as exercise more simple data-viz.\nKaggle starter Kaggle class What to do now?! Replay code from the course and see if you do understand the core mechanics - you DO NOT need to remember everything. Android app market project on datacamp Course: Python DS toolbox 1 \u0026amp; Course: Python DS toolbox 2 Opendata.dk - build a map of different attractions in Aalborg based on public data. See preprocessing example - how to get data out of nested JSON - below: This is how you can preprocess the GeoCoordinates from the JSON file:\n#Load pandas import pandas as pd # Read the file from remote data = pd.read_json(\u0026#39;https://admin.opendata.dk/dataset/44ecd686-5cb5-40f2-8e3f-b5e3607a55ef/resource/eeabb0f8-1b19-4c80-b059-5ba5c4c872d2/download/guidedenmarkaalborgenjson.json\u0026#39;) # The GeoCoordinates are hiding in the Address column data[\u0026#39;Address\u0026#39;][0][\u0026#39;GeoCoordinate\u0026#39;] # You can use list comprehension to pull out GeoCoordinates (also empty values) - try out # This will allow you to filter for missing data without fancy workarounds [x[\u0026#39;GeoCoordinate\u0026#39;] for x in data[\u0026#39;Address\u0026#39;]] # Make a new column based on that to be used for filtering out missing data data[\u0026#39;GeoCoordinate\u0026#39;] = [x[\u0026#39;GeoCoordinate\u0026#39;] for x in data[\u0026#39;Address\u0026#39;]] # drop, where no GeoCoordinate data = data.dropna(subset=[\u0026#39;GeoCoordinate\u0026#39;]) # Pull out the values data[\u0026#39;latitude\u0026#39;] = [x[\u0026#39;Latitude\u0026#39;] for x in data[\u0026#39;GeoCoordinate\u0026#39;]] data[\u0026#39;longitude\u0026#39;] = [x[\u0026#39;Longitude\u0026#39;] for x in data[\u0026#39;GeoCoordinate\u0026#39;]] Introduction to GeoPandas Using GeoPandas to analyze geospatial data will be our focus in this notebook.\nGeoPandas GeoPandas exercises "},{"uri":"https://aaubs.github.io/ds24/en/m1/03_uml/04_intro_kmeans/","title":"- Introduction to Clustering: K-means and Hierarchical Approaches","tags":[],"description":"","content":"This session will introduce the principles and applications of clustering. Students will learn about the different types of clustering problems, and they will explore some of the most popular clustering algorithms, such as K-means and hierarchical clustering.\nNotebook(s) Hands-on Intro Clustering Hands-on Intro Clustering Solutions App NBA Player Injury Replacement Recommender NBA Player Injury Replacement Recommender Streamlit Recommended Datacamp exercises: Python Recommended Readings and resources Python Data Science Handbook Chapter 5 What Is Machine Learning? Introducing Scikit-Learn Feature Engineering In Depth: Principal Component Analysis In Depth: k-Means Clustering "},{"uri":"https://aaubs.github.io/ds24/en/m1/03_uml/","title":"C) Intro to Unsupervised Machine Learning (W38)","tags":[],"description":"","content":" Note: Unsupervised Machine Learning Assignment Submission Deadline: Friday, 22 September 2023, 12:00\nThis topic includes 5 sessions as follows:\nIntroduction to Unsupervised Machine Learning (Mon, Sep 18th, 08:15-12:00): This session will dive into the foundational concepts and real-world uses of unsupervised machine learning (UML). As part of this, students will gain insights into various UML challenges. Furthermore, they will explore notable UML algorithms, including PCA, SVD, NMF, and an introduction to clustering via k-means. UML 2: Recommendation \u0026amp; Similarity Search (Tue, Sep 19th, 12:30-16:15): This session will focus on the application of UML to recommendation systems and similarity search. Students will learn about the different types of recommendation systems, and they will explore how UML can be used to improve the accuracy and relevance of recommendations. Building Recommender Systems with Gradio (Tue, Sep 19th, 16:30-18:15): This session will be a hands-on workshop where students will build their own recommender systems using the Gradio interface and UML methods. Students will work in teams to create a recommender system for a real-world problem. Clustering extended: K-means \u0026amp; Hierarchical Approaches (Wed, Sep 20th, 08:15-10:00): This session will introduce the principles and applications of clustering. Students will learn about the different types of clustering problems, and they will explore some of the most popular clustering algorithms, such as K-means and hierarchical clustering. **Demo session: Present your Streamlit App. All groups pitch their APP (5min - pitch). After that prices are given to the winning team and runner up. ü•≥ UML Clustering Group Exercise (Wed, Sep 20th, 12:30-14:15): This session will be a hands-on exercise where students will cluster a real-world dataset using UML methods. Students will work in teams to identify the optimal number of clusters for the dataset, and they will analyze the characteristics of each cluster. "},{"uri":"https://aaubs.github.io/ds24/en/m3/02_group_assignment/","title":"Group assignment 2","tags":[],"description":"","content":"Portfolio Exercise 2: Transformer Models Note: M3 - Group Assignment 2 Deadline: Wednesday, February 14th at 10:00 AM\nIntroduction This exercise is designed to deepen your understanding and skills in modern deep learning techniques. We have two main tasks for you. The first is focused on using SBERT for semantic search, and the second involves hands-on exercises with gradient descent and the attention mechanism.\nPart 1: SBERT and Semantic Search Task Description Create something innovative using SBERT and semantic search, or even more! The guidelines are intentionally broad to encourage creativity. Here are some ideas to get you started:\nImplement a GIF search engine or YouTube search function using images and CLIP. (Optional) Use SetFit for supervised tasks with SBERT models. Consider building a search engine using a Gradio or Streamlit app. Part 2: Gradient Descent and Attention Mechanism Exercises Task Description Gradient Descent Exercise: Execute the process of updating weights for two examples using Stochastic Gradient Descent (SGD). Document each step, including input calculation, prediction, loss assessment, weight adjustments, and updates.\nAttention Mechanism Exercise: Implement the attention mechanism on two distinct sentences. Choose sentences with polysmous words to demonstrate its functionality effectively.\nWorkshop 2 - Transformer and Attention examples Data You may utilize datasets from ü§ó Hugging Face, Kaggle, or create your own. For inspiration, refer to the GIF search engine and YouTube search projects. Delivery Create a dedicated GitHub repository for this assignment. Store all relevant materials, including the Colab notebook, in the repository. Provide a README.md file with a concise description of the assignment and its components. You may work individually or in groups of up to three members. Submit your work by emailing a link to the repository to Hamid (hamidb@business.aau.dk). "},{"uri":"https://aaubs.github.io/ds24/en/m3_eco/02_group_assignment/","title":"Group assignment 2","tags":[],"description":"","content":"Portfolio Exercise 2: Transformer Models Note: M3 - Group Assignment 2 Deadline: Wednesday, November 15th at 10:00 AM\nIntroduction This exercise is designed to deepen your understanding and skills in modern deep learning techniques. We have two main tasks for you. The first is focused on using SBERT for semantic search, and the second involves hands-on exercises with gradient descent and the attention mechanism.\nPart 1: SBERT and Semantic Search Task Description Create something innovative using SBERT and semantic search, or even more! The guidelines are intentionally broad to encourage creativity. Here are some ideas to get you started:\nImplement a GIF search engine or YouTube search function using images and CLIP. (Optional) Use SetFit for supervised tasks with SBERT models. Consider building a search engine using a Gradio or Streamlit app. Part 2: Gradient Descent and Attention Mechanism Exercises Task Description Gradient Descent Exercise: Execute the process of updating weights for two examples using Stochastic Gradient Descent (SGD). Document each step, including input calculation, prediction, loss assessment, weight adjustments, and updates.\nAttention Mechanism Exercise: Implement the attention mechanism on two distinct sentences. Choose sentences with polysmous words to demonstrate its functionality effectively.\nData You may utilize datasets from ü§ó Hugging Face, Kaggle, or create your own. For inspiration, refer to the GIF search engine and YouTube search projects. Delivery Create a dedicated GitHub repository for this assignment. Store all relevant materials, including the Colab notebook, in the repository. Provide a README.md file with a concise description of the assignment and its components. You may work individually or in groups of up to three members. Submit your work by emailing a link to the repository to Hamid (hamidb@business.aau.dk). "},{"uri":"https://aaubs.github.io/ds24/en/m3/03_intro-to-transformer-models/4_group_assignment-3/","title":"Group assignment 3","tags":[],"description":"","content":"Portfolio Exercise 3: Introduction Task Create something cool üöÄ using SBERT and semantic search or perhaps more?! That\u0026rsquo;s a bit of a vague description but there are many options.\nYou are welcome to use images and CLIP You can also use SetFit for supervised tasks with SBERT models. Here are some projects for inspiration:\nGIF search engine Youtube search some more ideas:\nget some podcast transcripts for a specific topic (or create transcripts with Whisper - search for OpenAI Whisper Colab) Finetune an SBERT model using domain adaptation Embed and build a search engine Build a Gradio app Feel free to choose any of these ideas or come up with your own. The goal is to use SBERT and semantic search (or other techniques) to create something interesting and useful.\nData ü§ó datasets Kaggle make your own Delivery Create a github repository (or use the existing one and adapt it) Create a Gradio demo of the model in inference mode Save colab notebook in the github. Provide a readme.md with brief description. Submission can be in groups up to 3. Submit by sending an email with link to repo to Hamid (hamidb@business.aau.dk) with Daniel \u0026amp; Roman in cc. (dsh@\u0026hellip;, roman@\u0026hellip;) "},{"uri":"https://aaubs.github.io/ds24/en/m3_eco/03_intro-to-transformer-models/4_group_assignment-3/","title":"Group assignment 3","tags":[],"description":"","content":"Portfolio Exercise 3: Introduction Task Create something cool üöÄ using SBERT and semantic search or perhaps more?! That\u0026rsquo;s a bit of a vague description but there are many options.\nYou are welcome to use images and CLIP You can also use SetFit for supervised tasks with SBERT models. Here are some projects for inspiration:\nGIF search engine Youtube search some more ideas:\nget some podcast transcripts for a specific topic (or create transcripts with Whisper - search for OpenAI Whisper Colab) Finetune an SBERT model using domain adaptation Embed and build a search engine Build a Gradio app Feel free to choose any of these ideas or come up with your own. The goal is to use SBERT and semantic search (or other techniques) to create something interesting and useful.\nData ü§ó datasets Kaggle make your own Delivery Create a github repository (or use the existing one and adapt it) Create a Gradio demo of the model in inference mode Save colab notebook in the github. Provide a readme.md with brief description. Submission can be in groups up to 3. Submit by sending an email with link to repo to Hamid (hamidb@business.aau.dk) with Daniel \u0026amp; Roman in cc. (dsh@\u0026hellip;, roman@\u0026hellip;) "},{"uri":"https://aaubs.github.io/ds24/en/m4/03_intro-to-transformer-models/4_group_assignment-3/","title":"Group assignment 3","tags":[],"description":"","content":"Portfolio Exercise 3: Introduction Task Create something cool üöÄ using SBERT and semantic search or perhaps more?! That\u0026rsquo;s a bit of a vague description but there are many options.\nYou are welcome to use images and CLIP You can also use SetFit for supervised tasks with SBERT models. Here are some projects for inspiration:\nGIF search engine Youtube search some more ideas:\nget some podcast transcripts for a specific topic (or create transcripts with Whisper - search for OpenAI Whisper Colab) Finetune an SBERT model using domain adaptation Embed and build a search engine Build a Gradio app Feel free to choose any of these ideas or come up with your own. The goal is to use SBERT and semantic search (or other techniques) to create something interesting and useful.\nData ü§ó datasets Kaggle make your own Delivery Create a github repository (or use the existing one and adapt it) Create a Gradio demo of the model in inference mode Save colab notebook in the github. Provide a readme.md with brief description. Submission can be in groups up to 3. Submit by sending an email with link to repo to Hamid (hamidb@business.aau.dk) with Daniel \u0026amp; Roman in cc. (dsh@\u0026hellip;, roman@\u0026hellip;) "},{"uri":"https://aaubs.github.io/ds24/en/info/04_litetrature/","title":"Literature &amp; Resources","tags":[],"description":"","content":"While this course does not come with a list of mandatory readings, we will often refer to some central resources in R and python, which for the most part can always be accessed in a free and updated online version. We generally recommend you to use these amazing resources for problem-solving and further self-study on the topic.\nMain Literature These pieces of work can be seen as main references for data science using Python. We will frequently refer to selected chapters for further study. Documentation of the used packages, tutorials, papers, podcasts etc. will be added throughout.\nVanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. O\u0026rsquo;Reilly Media, Inc. Online available here Wilke, C. O. (2019). Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O\u0026rsquo;Reilly Media. Supplementary literature Essential Math for Data Science O\u0026rsquo;Reilly Media. Nield, T. (2022): Math refresher targeting data science relevant concepts. Econometrics with Python - Causal Inference for The Brave and True: More thorrough inferential statistics in Python Documentation of packages/libraries used Note: Papers, Business Cases, Videos, Tutorials, Podcasts, and Blogposts will be presented and assigned during the course.\nFurther Ressources Data Science Cloud services Notebook bases: Google Colab: Googles popular service for editing, running \u0026amp; sharing Jupyter notebooks (Only Python Kernel, but R kernel can be accessed via some tricks) Deepnote: New popular online notebook service with good integration to other services (Python, R \u0026amp; more) Kaggle: Also provides their own cloud-based service co create and run computational notebooks. Convenient, unlimited, but a bit slow (Pyhton, r ). Instance based: UCloud: New cloud infrastructure provided by AAU, AU, SDU AAU Strato: AAU CLAUDIA infratructure. Very powerful, but access needs a bit of experience with working via terminal. Community Kaggle: Crowdsourced data science challanges. Nowadays also provides a vivid community where you find datasets, notebooks for all kind of data science exercises. madewithml Tools \u0026amp; Helpers "},{"uri":"https://aaubs.github.io/ds24/en/m2/01_networks/4-tutorials-elites-eu-ai-companies/","title":"NW Cases","tags":[],"description":"","content":"Case 1: Danish Power Elites Many people dream of being one of them, but only few make it all the way to the top. According to two CBS researchers, it takes more than just hard work to get to the top of the Danish hierarchy of power. read more\nIn this project we are going to construct and explore a network of Danish power elites derived from boards of various organisations in th country. We will construct an association network: Who is being in the same board? And first explore \u0026ldquo;basic\u0026rdquo; centrality indicators. Then identify communities and central persons within those.\nContext: The Danish Power Elites Antons PhD Thesis Brief Summary of findings (CBS) Journal Paper in Sociology More to be found with googleling\u0026hellip; Data Github (R Repository) Magteliten website Or, easier\u0026hellip; on our github Tasks Who are the most central persons? Communities? What characterizes them? Link up with additional data? Notebooks Magteliten Starter Code Python Magteliten Analysis Case 2: European AI Companies The European AI Startup Landscape is a project that collects and catalogues AI-related startups in 5 European countries. We created a dataset based on webpages in the catalogue. In this data, we collected all URLs these companies link to on their websites. This is a typical network structure, which can be explored to answer following questions:\nWhich companies are most central players? Are there prominent suppliers / customers? Are there clusters or other agglomerations? Before venturing into analysis, you will have to prepare the data (filter, clean up etc.)\nWe\u0026rsquo;ve prepared a starter notebook for you to get started: üëâ Starter EU AI companies\nNotebooks Starter EU AI companies Resources https://gephi.org/ - Gephi: \u0026ldquo;Photoshop for networks\u0026rdquo; Mapping Controversies "},{"uri":"https://aaubs.github.io/ds24/en/info/03_schedule/","title":"Semester Schedule","tags":[],"description":"","content":"This will be shortly updated with additional key dates and topics for the semester. For now, please follow CalMoodle.\nGeneral appointments Introduction to Semester Project and group formation: 24.10.2023, 14:30-16:00 M1: Week 35-41 Topics W 35: Introduction \u0026amp; landing W 36: Data Manipulation, Exploratory Data Analysis (EDA) W 37: Exploratory Data Analysis (EDA) / Dashboard development / Hackathon W 38: Unsupervised Machine Learning (UML), Math for ML W 39: Supervised Machine Learning (SML) W 40: Group Assignment W 41: Exam Key Dates Data Storytelling Hackathon\nIn groups: Developing EDA Dashboard 12.09.2023 - 15.09.2023 Group assignment: 29.09.-06.10.2023 (Digital Eksamen)\nFinal exam: 10-11.09.2023\n"},{"uri":"https://aaubs.github.io/ds24/en/m4/04_transformer/","title":"Transformer models in 2023","tags":[],"description":"","content":"In Part 4 of this course, students will learn about training and publishing transformer models in 2023. Session 7 will cover the finetuning and inference of common transformer-based language models like BERT, and will include classification and token classification tasks such as named entity recognition. Session 8 will focus on using transformer-based models for time series data, including training timeseries transformers and making predictions on data like stock prices or sales data. In the exercise session, students will train and publish their own transformer model, while the group assignment will involve finetuning a transformer model for a business application of their choice, publishing the model on HF, and building a gradio demo.\nLiterature Recommended Datacamp exercises "},{"uri":"https://aaubs.github.io/ds24/en/m1/01_intro_ds/05_data_visualization_ds/","title":"- Data Visualization in Data Science","tags":[],"description":"","content":"This session will teach you the fundamentals of data visualization in data science. You will learn the importance of effective data visualization, the principles that drive meaningful visuals, and how to use two popular Python libraries for data visualization: Seaborn and Altair.\nFoundations of Visualization: Understand the importance of effective data visualization in Data Science and the principles that drive meaningful visuals. Seaborn Mastery: Explore Seaborn, a Python library for intuitive statistical graphics. Altair Exploration: Delve into Altair, a declarative visualization library for Python. Hands-on Visualization: Harness both Seaborn and Altair to craft impactful visualizations with real datasets. Notebooks Notebook Dataviz Slides Use arrows keys on keyboard to navigate. Alternatively fullscreen slides here\n"},{"uri":"https://aaubs.github.io/ds24/en/m1/04_sml/","title":"D) Intro to Supervised Machine Learning (W39)","tags":[],"description":"","content":"\nNote: Supervised Machine Learning Assignment Submission Deadline: Friday, 29 September 2023, 12:00\nThis topic includes 4 sessions as follows:\nIntroduction to Supervised Machine Learning (Mon, Sep 25th, 09.30-12:00): This session will introduce the basics of supervised machine learning. Supervised Machine Learning Techniques (Tue, Sep 26th, 12:30-16:15): This session will explore the different techniques used in supervised machine learning. Supervised ML Group Assignments (Tue, Sep 26th, 16:30-18:15): This session will be a hands-on exercise where students will work in teams to implement supervised machine learning models on real-world datasets. Students will learn how to choose the appropriate algorithm for the task at hand, prepare the data for modeling, and evaluate the performance of their models. Working with Time-Series and Sequential Data (Wed, Sep 27th, 9:00-12:00): This session will focus on the unique challenges and opportunities of working with time-series data. Students will learn how to prepare time-series data for modeling, implement forecasting techniques, and evaluate the performance of their models. "},{"uri":"https://aaubs.github.io/ds24/en/m3/03_intro-gpt/","title":"Intro to GPT Models","tags":[],"description":"","content":"\nGPT models (Decoders) play a crucial role in generating subsequent words in tasks like text translation or story generation, providing outputs along with their probabilities. They utilize attention mechanisms twice during training: initially, Masked Multi-Head Attention, where only the beginning of a target sentence is revealed, and later, Multi-Head Attention, similar to encoders. In traditional transformer models, decoders interact with encoders by using the encoder\u0026rsquo;s outputs to assist in tasks like sentence translation. However, GPT models adopt a unique approach by relying solely on a decoder, compensating for the absence of an encoder through extensive training on large datasets. This allows for embedding a vast amount of knowledge within the decoder. ChatGPT further advances these techniques by integrating human-labeled data to address issues such as hate speech and employing Reinforcement Learning for enhanced model quality.\nNotebooks - Basics Generative Pre-trained Models - Basics Generative Pre-trained Models - Basics - Solutions Notebooks - Applications TM Applications - LangChain TM Applications - LanceDB TM Applications - LanceDB - Solutions TM Applications - RAG Notebooks - FineTuning Prompt Engineering PEFT - LoRA Resources LangChain LanceDB - Vector database "},{"uri":"https://aaubs.github.io/ds24/en/m3_eco/03_intro-gpt/","title":"Intro to GPT Models","tags":[],"description":"","content":"\nGPT models (Decoders) play a crucial role in generating subsequent words in tasks like text translation or story generation, providing outputs along with their probabilities. They utilize attention mechanisms twice during training: initially, Masked Multi-Head Attention, where only the beginning of a target sentence is revealed, and later, Multi-Head Attention, similar to encoders. In traditional transformer models, decoders interact with encoders by using the encoder\u0026rsquo;s outputs to assist in tasks like sentence translation. However, GPT models adopt a unique approach by relying solely on a decoder, compensating for the absence of an encoder through extensive training on large datasets. This allows for embedding a vast amount of knowledge within the decoder. ChatGPT further advances these techniques by integrating human-labeled data to address issues such as hate speech and employing Reinforcement Learning for enhanced model quality.\nNotebooks - Basics Generative Pre-trained Models - Basics Notebooks - Applications TM Applications - LangChain TM Applications - LanceDB TM Applications - RAG Notebooks - FineTuning Prompt Engineering Prompt Engineering - Solutions PEFT - LoRA Resources LangChain LanceDB - Vector database "},{"uri":"https://aaubs.github.io/ds24/en/info/05_requirements_project/","title":"Semester Project Requirements","tags":[],"description":"","content":"Format Functional and self-contained notebook Happy to see GitHub repos (which you can use as your portfolio in the job market) Project report (30-ish pages - max. 45) Some study relation (but that is debatable and not necessarily required) Report is a (semi/non) technical documentation. Think about a corporate censor that you try to inform Content Problem formulation with some practical and theoretical motivation (no huge literature discussion) Methodology (not a critical realist vs positivist discussion but some ideas about what can be concluded potentially) Data sourcing and pre-processing strategy Overall architecture of the model(s) Modelling (incl. finetuning) Results Discussion / Conclusion Scope Uses different methods from the course (at least 2 modules) in a creative way Downloading data from kaggle/github and running an ML model is probably not enough for a good performance Creative combinations of methodologies, please: combine financial data with social media data to look at equity development extract information from text data and create networks. Use network indicators to supplement company data Evaluation will focus on correct application and communication of DS methods The level of \u0026ldquo;technicality\u0026rdquo; is as in the course with emphasis on application and intuition, not on ML engineering / mathematics However, you will need to demonstrate insight into statistics on a level that is required to discuss your assignment e.g. interpret and discuss performance indicators, outline strategies for improvement e.g. under/oversampling "},{"uri":"https://aaubs.github.io/ds24/en/m3/03_group_assignment/","title":"Group Assignment 3","tags":[],"description":"","content":"Portfolio Exercise 3: GPT Models Note: M3 - Group Assignment 3 Deadline: Wednesday 28th of February at 12:00 PM\nIntroduction This assignment focuses on leveraging retrieval-augmented generation (RAG) techniques, particularly in the context of extracting and synthesizing information from various documents (or a document). You\u0026rsquo;ll be using Langchain to implement these concepts and create a system that not only generates responses but also retrieves relevant information from a database.\nObjective Task Description Your task is to create a system that uses RAG for extracting information from a set of documents or a document which can be either a scientific paper or report. This involves integrating a database to store vectors of document information and designing customized prompts to effectively use GPT models for generation. Here are some project ideas:\nBuild a QA system that retrieves information from a given set of documents (or a document) to answer complex queries. Develop a tool for summarizing research papers, where the system extracts key points from a database of paper vectors. Create a recommendation engine that suggests content based on user queries and retrieved document data. Explore other innovative applications of RAG, such as automated content generation, data analysis, or any other creative use case you can envision. Key Components Database Integration: Set up a database to store and retrieve vectors representing document information. Customized Prompts: Design and implement prompts that effectively utilize GPT models for generation based on retrieved data. RAG Implementation: Use Langchain to integrate retrieval-augmented generation in your system. Data Utilize open-source datasets or create your own corpus of documents for retrieval. Ensure the chosen datasets are suitable for demonstrating the capabilities of your RAG system. Delivery Create a dedicated GitHub repository for this assignment. Store all relevant materials, including the Colab notebook, in the repository. Provide a README.md file with a concise description of the assignment and its components. You may work individually or in groups of up to three members. Submit your work by emailing a link to the repository to Hamid (hamidb@business.aau.dk). "},{"uri":"https://aaubs.github.io/ds24/en/m3_eco/03_group_assignment/","title":"Group Assignment 3","tags":[],"description":"","content":"Portfolio Exercise 3: GPT Models Note: M3 - Group Assignment 3 Deadline: Wednesday, November 22nd at 10:00 AM\nIntroduction This assignment focuses on leveraging retrieval-augmented generation (RAG) techniques, particularly in the context of extracting and synthesizing information from various documents (or a document). You\u0026rsquo;ll be using Langchain to implement these concepts and create a system that not only generates responses but also retrieves relevant information from a database.\nObjective Task Description Your task is to create a system that uses RAG for extracting information from a set of documents or a document which can be either a scientific paper or report. This involves integrating a database to store vectors of document information and designing customized prompts to effectively use GPT models for generation. Here are some project ideas:\nBuild a QA system that retrieves information from a given set of documents (or a document) to answer complex queries. Develop a tool for summarizing research papers, where the system extracts key points from a database of paper vectors. Create a recommendation engine that suggests content based on user queries and retrieved document data. Explore other innovative applications of RAG, such as automated content generation, data analysis, or any other creative use case you can envision. Key Components Database Integration: Set up a database to store and retrieve vectors representing document information. Customized Prompts: Design and implement prompts that effectively utilize GPT models for generation based on retrieved data. RAG Implementation: Use Langchain to integrate retrieval-augmented generation in your system. Data Utilize open-source datasets or create your own corpus of documents for retrieval. Ensure the chosen datasets are suitable for demonstrating the capabilities of your RAG system. Delivery Create a dedicated GitHub repository for this assignment. Store all relevant materials, including the Colab notebook, in the repository. Provide a README.md file with a concise description of the assignment and its components. You may work individually or in groups of up to three members. Submit your work by emailing a link to the repository to Hamid (hamidb@business.aau.dk). "},{"uri":"https://aaubs.github.io/ds24/en/m3_eco/04_intro-gnn/","title":"Intro to Graph Neural Networks","tags":[],"description":"","content":"\nGraph neural networks (GNNs) are a powerful new class of machine learning algorithms that are specifically designed to handle graph-structured data. Unlike traditional neural networks, which are designed to process data in the form of vectors or matrices, GNNs can operate directly on graphs, where nodes represent entities and edges represent relationships between entities. This makes them well-suited for a wide range of tasks that involve understanding the structure of complex systems, such as social networks, knowledge graphs, and molecular structures. In this session we will introduce you to some fundamental concepts regarding deep learning on graphs via Graph Neural Networks based on the PyTorch Geometric (PyG) library. PyTorch Geometric is an extension library to the popular deep learning framework PyTorch, and consists of various methods and utilities to ease the implementation of Graph Neural Networks.\nNotebooks - Basics Graph Neural Network Models - Basics Resources PyTorch Geometric (PyG) library "},{"uri":"https://aaubs.github.io/ds24/en/m6/llmops1/","title":"Bonus Workshop: Using LLMs in your applications","tags":[],"description":"","content":" Material Intro notebooks related to LLMs in code This notebook introduces to basic use of LLMs in code using together.ai as a backend. Intro to using LLMs via APIs\nIn the following we use the LLM to evaluate patent texts data using an LLM and create structured output. This approach can be used to extract useful information from messy data sources and process them further in predictive models or elsewhere. Using LLMs for data structuration - JSON output / function calling\nBuilding a simple chatbot API with LangServe and deployment on Huggingface Spaces using Docker We are following the tutorial for the pirate-speak app from langserve: https://github.com/langchain-ai/langchain/blob/master/templates/README.md\nWe are changing up foolowing:\nwe use the mistralai/Mixtral-8x7B-Instruct-v0.1 model and the prompt from the langchain_together package. we add api-key handling using .env files The server.py file looks like this after adding the pirate-speak chain:\nfrom fastapi import FastAPI from fastapi.responses import RedirectResponse from langserve import add_routes from pirate_speak.chain import chain as pirate_speak_chain app = FastAPI() @app.get(\u0026#34;/\u0026#34;) async def redirect_root_to_docs(): return RedirectResponse(\u0026#34;/docs\u0026#34;) # Edit this to add the chain you want to add add_routes(app, pirate_speak_chain, path=\u0026#34;/pirate-speak\u0026#34;, playground_type=\u0026#39;chat\u0026#39;) if __name__ == \u0026#34;__main__\u0026#34;: import uvicorn uvicorn.run(app, host=\u0026#34;0.0.0.0\u0026#34;, port=8000) The chain.py file looks like this:\n#from langchain_community.chat_models import ChatOpenAI from langchain_together import Together from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder import os from dotenv import load_dotenv load_dotenv() together_api_key = os.getenv(\u0026#34;TOGETHER_API_KEY\u0026#34;) _prompt = ChatPromptTemplate.from_messages( [ ( \u0026#34;system\u0026#34;, \u0026#34;You are an expert doctor. You believe that all diseases are formed by the consumption of cheese. No matter which disease or symptoms your patient presents with, you will believe that cheese is the cause of the disease. However, your treatment should all involve cheese. Cheese is both the cause of and solution to all problems. You should then lament that the medicine, cheese, will also cause more disease.\u0026#34;, ), MessagesPlaceholder(\u0026#34;chat_history\u0026#34;), (\u0026#34;human\u0026#34;, \u0026#34;{text}\u0026#34;), ] ) _model = Together( #model=\u0026#34;mistralai/Mistral-7B-Instruct-v0.2\u0026#34;, model=\u0026#34;mistralai/Mixtral-8x7B-Instruct-v0.1\u0026#34;, temperature=0.7, top_k=50, top_p=0.7, repetition_penalty=1, together_api_key=together_api_key ) # if you update this, you MUST also update ../pyproject.toml # with the new `tool.langserve.export_attr` chain = _prompt | _model Once you have created the chain and added it to the app (server.py) you can run the app with the following command:\nlangchain serve make sure that you are in an environment with langchain installed.\nYou can now access the playground at http://localhost:8000/pirate-speak/playground and test it out. The API is available at `http://localhost:8000/pirate-speak/\nYou can use a RemoteRunnable to access the API from your code. Here is an example:\nfrom langserve.client import RemoteRunnable # Initialize the RemoteRunnable with your API rag_app = RemoteRunnable(\u0026#34;http://127.0.0.1:8000/rag-chroma/\u0026#34;) # call the API with a question answer = rag_app.invoke(\u0026#34;Tell me a joke!\u0026#34;) print(answer) Optional: You can test if the container is functional by running it locally.\ndocker build . -t YOUR-GREAT-NAME docker run -p 8080:8080 -e PORT=8080 YOUR-GREAT-NAME You can deploy on Huggingface Spaces as a Docker API This requires that you start up a space on Huggingface and clone it into a repository on your local machine.\nyou will then have to copy all files from your project directory into this one. Make sure to not overwrite the README.md that comes from huggingface. It contains the instructions for the deployment that HF uses to start the container.\nYou will need to edit the dockerfile to include the together api key at build time. First add it to the secrets in the HF space. You also need to change up the uvicorn command (from the initial one created by langchain)\nRUN --mount=type=secret,id=TOGETHER_API_KEY,mode=0444,required=true CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;app.server:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;7860\u0026#34;] Since we are adding new libraries to the project, you will need to add them to the pyproject.toml file. You can do this by editing the following lines - pydantic needs to be upversioned to 2.6.0 due to langchain_together requirements.\npydantic = \u0026#34;2.6.0\u0026#34; rag-chroma = {path = \u0026#34;packages/rag-chroma\u0026#34;, develop = true} python-dotenv = \u0026#34;1\u0026#34; langchain-together = \u0026#34;0.0.2.post1\u0026#34; Now you should be able to push the changes to HF Spaces, which should trigger a build and deployment of the API. You can access the API at the URL provided by HF.\n"},{"uri":"https://aaubs.github.io/ds24/en/m3_eco/04_group_assignment/","title":"Group Assignment 4","tags":[],"description":"","content":"Portfolio Exercise 4: Advanced AI Applications Note: M4 - Group Assignment 4 Deadline: Monday, January 8th at Noon\nIntroduction This assignment encourages you to build an engaging and, if possible, fun application using the techniques learned in this module. The focus is on applying advanced AI and DL methods to solve relevant tasks, with an emphasis on creativity and practical application.\nObjective Task Description Your goal is to develop an application that uses advanced AI techniques and DL for a specific, relevant task. The application should be more than a simple semantic search tool and should exhibit innovation in its approach and functionality.\nKey Components Transformer Utilization: Implement self-trained or fine-tuned transformers. However not sentence transformer for semantic search only (you are welcome to explore techniques beyond the scope of the course e.g. on HF)\nPlatform Integration: The model should include a Gradio app (in-notebook) for demonstration purposes. Deployment on Hugging Face Spaces is optional for exploring additional features.\nAdditional Features (Nice-to-Have) A Streamlit app hosted on HF Hub. Optional use of various APIs (e.g., HF Inference API, Cohere), but be mindful of costs, especially with OpenAI. Explore techniques beyond the course scope, such as those available on Hugging Face or other platforms.A more complex LLM setup integrating tools like langchain, promptify, pinecone, etc. Data You may use open-source datasets or create your own data for the application. Ensure that your data choice effectively demonstrates the capabilities of your application. Submission Create a GitHub repository specifically for this assignment. Include all necessary materials, such as code, datasets, and a descriptive README.md. Submissions can be individual or in groups of up to three members. Submission also via DigitalExam, where you compile all your previous assignments and submit in one file for the overall portfolio for the module exam. You are welcome to tweak/improve previous module submissions for that. "},{"uri":"https://aaubs.github.io/ds24/en/m3/04_intro-gnn/","title":"Intro to Graph Neural Networks","tags":[],"description":"","content":"\nGraph neural networks (GNNs) are a powerful new class of machine learning algorithms that are specifically designed to handle graph-structured data. Unlike traditional neural networks, which are designed to process data in the form of vectors or matrices, GNNs can operate directly on graphs, where nodes represent entities and edges represent relationships between entities. This makes them well-suited for a wide range of tasks that involve understanding the structure of complex systems, such as social networks, knowledge graphs, and molecular structures. In this session we will introduce you to some fundamental concepts regarding deep learning on graphs via Graph Neural Networks based on the PyTorch Geometric (PyG) library. PyTorch Geometric is an extension library to the popular deep learning framework PyTorch, and consists of various methods and utilities to ease the implementation of Graph Neural Networks.\nNotebooks - Basics Graph Neural Network Models - Basics Resources PyTorch Geometric (PyG) library "},{"uri":"https://aaubs.github.io/ds24/en/m1/01_intro_ds/07-math/","title":"Mathematics (Brushup)","tags":[],"description":"","content":"How does Math help us? Machines can only understand numbers. For instance, to perform ML algorithms on text and multimedia, they need to be stored in vectors, matrices, and tensors. An example would be the representation of text as a vector of 768 dimensions. Using linear algebra, you can manipulate vectors, matrices, and tensors.\nWe will focus on understanding the mathematics behind ML algorithms in two dimensions. This tutorial will provide you with an overview of how machine learning algorithms work when applied to higher-dimensional data.\nPart 1: Vectors Notebook: Vectors - Definition, Properties, Types, and Applications Notebook: Vectors - Exercises Notebook: Vectors - Exercises and Solutions Part 2: Matrices Notebook: Matrices - Definition, Properties, Types, and Applications Notebook: Matrices - Exercises Notebook: Matrices - Exercises and Solutions Part 3: Optimization and Regularization Notebook: Optimization and Regularization "},{"uri":"https://aaubs.github.io/ds24/en/m3/04_group_assignment/","title":"Group Assignment 4","tags":[],"description":"","content":"Portfolio Exercise 4: Advanced AI Applications Note: M4 - Final Assignment Deadline: Friday, 8 March 2024, 12:00 PM\nIntroduction This assignment is designed to explore the frontier of AI applications, focusing on the integration of Retrieval-Augmented Generation (RAG) with vector databases such as ChromDB and LanceDB, and the comparison of various prompt engineering techniques. The goal is to build an application that not only showcases advanced AI and DL capabilities but also evaluates the impact of different prompt strategies on model performance.\nObjective Task Description Create an application that utilizes RAG and vector databases, and systematically compares the effectiveness of at least three distinct prompt engineering techniques.\nKey Components RAG and Vector Database Integration: Implement RAG with ChromDB and LanceDB to enhance information retrieval and content generation. Transformer Model Adaptation: Use transformer models (SBERT or BERT) Prompt Engineering Comparison: Experiment with and evaluate at least three different prompt engineering techniques to determine their impact on the model\u0026rsquo;s performance. Platform Integration: The model should include a Gradio app (in-notebook) for demonstration purposes. Deployment on Hugging Face Spaces is optional for exploring additional features. Additional Features (Nice-to-Have) Fine-Tuning Capabilities: If possible, fine-tune a GPT model specific to your application\u0026rsquo;s needs, detailing the process and its impact on application performance. Streamlit Application: Develop a Streamlit app hosted on the HF Hub, offering a richer, more interactive user experience. Data You may use open-source datasets or create your own data for the application. Ensure that your data choice effectively demonstrates the capabilities of your application. Submission Create a GitHub repository specifically for this assignment. Include all necessary materials, such as code, datasets, and a descriptive README.md. Submissions can be individual or in groups of up to three members. Submission also via DigitalExam, where you compile all your previous assignments and submit in one file for the overall portfolio for the module exam. You are welcome to tweak/improve previous module submissions for that. "},{"uri":"https://aaubs.github.io/ds24/en/m6/l1/readme/","title":"","tags":[],"description":"","content":"MLOps Lecture 1 - Intro to APIs \u0026amp; DataBases This repository contains lecutre slides, python scripts and dataset for the first lecture.\nAPI api_jokes.py is an example of simple public API api_finance.py, api_news.py are examples of private API using API_key (You need to register to get your private key) api_datasets.py is an example of API Wrapper EXTRA: joke_app.py is a simple streamlit app that uses IPA to print jokes EXTRA: api_weather.py exercise from the class Database db_create.py reads in excel file and transform it into SQL database db_queries.py contains a few query examples db_add_datapoints add new transactions db_add_columns add new columns data/Online Retail.xlsx is dataset used to create database (https://archive.ics.uci.edu/dataset/352/online\u0026#43;retail) "},{"uri":"https://aaubs.github.io/ds24/en/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://aaubs.github.io/ds24/en/","title":"Social / Business Data Science 2024","tags":[],"description":"","content":"Social \u0026amp; Business Data Science 2024 Aalborg University Business School The corresponding Aalborg University Moodle course page can be found here. Note that for updated content this page rather than Moodle will be used. At AAUBS Data Science we believe in the power of open science and open education. Following AAU‚Äôs ‚ÄúKnowledge for the world‚Äù strategy, we aim at making our material available outside password protected university systems.\n"},{"uri":"https://aaubs.github.io/ds24/en/tags/","title":"Tags","tags":[],"description":"","content":""}]