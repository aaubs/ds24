[{"uri":"https://aaubs.github.io/ds24/en/m1/04_sml/01-sml-intro/","title":"- Introduction to Supervised ML","tags":[],"description":"","content":"\nThis session introduces supervised machine learning (SML) and related main concepts.\nNotebook(s) Introduction to Supervised Machine Learning\nSupervised Machine Learning Regression\nSupervised Machine Learning Classification\nAssignment for SML\nIntro slides Use arrows keys on keyboard to navigate. Alternatively fullscreen slides\nRecommended Datacamp exercises: Basics:\nPython - Intro to supervised learning Python - Decision Tree modeling Modell workflows\nPython - SML Preprocessing Python - SML Feature Engineering Python - SML Model validation Recommended Readings and resources Python Data Science Handbook Chapter 5 "},{"uri":"https://aaubs.github.io/ds24/en/m1/03_uml/01_intro_uml/","title":"- Introduction to Unsupervised ML","tags":[],"description":"","content":"\nThis session will introduce the principles and applications of unsupervised machine learning (UML). Students will learn about the different types of UML problems, and they will explore some of the most popular UML algorithms, such as PCA, SVD, and NMF.\nNotebook(s) Hands-on Intro to Dimensionality reduction and Clustering\nAssignment for the UML\nAssignment for the UML Solution\nRecommended Datacamp exercises: Python Recommended Readings and resources Python Data Science Handbook Chapter 5\nWhat Is Machine Learning? Introducing Scikit-Learn Feature Engineering In Depth: Principal Component Analysis In Depth: k-Means Clustering Implementation tutorials on YT PCA and K-means from this list\nIntro slides Use arrows keys on keyboard to navigate. Alternatively fullscreen slides\n"},{"uri":"https://aaubs.github.io/ds24/en/m1/01_intro_ds/01_welcome_ds/","title":"- Welcome Students!","tags":[],"description":"","content":"\nThis session will introduce you to the fundamentals of data science, with a focus on Python. We will cover the Python data science stack, essential tools and platforms, software setup, semester overview, and Python 101.\nSession 1: Welcome Students! This session sets the stage for your data science journey:\nPython Data Science Stack: Dive into Python\u0026rsquo;s core data science libraries and frameworks. We\u0026rsquo;ve got you covered!\nEcosystem Deep Dive: Familiarize yourself with essential tools and platforms, such as Github, UCloud, Google Colab, and Jupyter. These will be integral to your studies and projects.\nSoftware Setup: We\u0026rsquo;ll guide you through installing the crucial software. And don\u0026rsquo;t worry, our Teaching Assistants are here to assist with any challenges.\nSemester Overview: Get a glimpse of what the upcoming weeks hold for you.\nPython 101: We\u0026rsquo;ll ensure everyone is up to speed with Python basics.\nNotebooks How to build a development environment using Colab, Google Drive, GitHub, and Kaggle!\nüöÄ Notebook: Python 101\nPart 4: Cloning and Pushing to GitHub Using VS Code This tutorial provides step-by-step instructions on how to install Visual Studio Code and Git, and how to use them to clone and push to a GitHub repository. The instructions cover both macOS and Windows.\nStep 4.1: Install Visual Studio Code For macOS: Visit the VS Code official website and download the stable build for macOS. Open the downloaded .zip file and extract VS Code. Drag Visual Studio Code.app to the Applications folder, making it available in the Launchpad. For Windows: Visit the VS Code official website and download the stable build for Windows. Run the downloaded .exe file and follow the installation prompts. Ensure you select ‚ÄúAdd to PATH‚Äù during installation to enable launching from the command line. Step 4.2: Install Git For macOS: 4.2.1. Check if Homebrew is installed:\nOpen Terminal and type the following command: brew --version If Homebrew is installed, you will see the version number. If not, proceed with the next step to install Homebrew. 4.2.2. Install Homebrew (if not installed):\nIn Terminal, run the following command to install Homebrew: /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; Follow the on-screen instructions to complete the installation. 4.2.3. Install Git using Homebrew:\nAfter Homebrew is installed, run the following command in Terminal: brew install git 4.2.4. Verify the installation:\nCheck if Git is installed correctly by typing: git --version You should see the Git version number if the installation was successful. For Windows: Download the latest Git for Windows installer from the Git website. Run the downloaded .exe file and follow the setup instructions. Make sure to choose the recommended settings, especially for adjusting your PATH environment. Step 4.3: Configure Git Open a terminal (macOS) or command prompt/Git Bash (Windows) and set your user name and email address with the following commands:\ngit config --global user.name \u0026#34;Your Name\u0026#34; git config --global user.email \u0026#34;your.email@example.com\u0026#34; Step 4.4: Clone a Repository Using VS Code Open VS Code. Access the Command Palette by going to the View menu and clicking on Command Palette Type Git: Clone in the Command Palette and select it. Enter the URL of the GitHub repository you want to clone and press Enter. Select the directory where you want to save the repository and click Select Repository Location. After the repository has been cloned, VS Code will ask if you want to open the cloned repository. Click Open. Step 4.5: Make Changes and Push to GitHub Open the folder of the cloned repository in VS Code. Make your desired changes to the files or add new files. Commit your changes by entering a commit message in the message box and then clicking the checkmark icon at the top of the Source Control sidebar. Push your changes to GitHub by clicking the ... button in the Source Control sidebar, selecting Push from the dropdown menu. Notes Ensure that you have the necessary permissions to push to the repository if it is not owned by you. If you are pushing to GitHub for the first time, you may be prompted to authenticate with your GitHub credentials. Part 5: How to Use Codespaces on GitHub Codespaces is a cloud-hosted development environment provided by GitHub, allowing you to code directly within your repository without setting up a local environment.\n5.1. Navigate to Your Repository Go to the repository you want to work on. 5.2. Open the Codespaces Tab Once you\u0026rsquo;re in your repository, locate the Code dropdown button near the top-right corner. Click on the dropdown to reveal the Codespaces section. If you don\u0026rsquo;t have any active Codespaces, it will show \u0026ldquo;No codespaces.\u0026rdquo; 5.3. Create a New Codespace If no Codespaces are created, click on the Create codespace on main button. This will initiate a new Codespace environment based on the main branch of your repository. 5.4. Start Coding After creating the Codespace, it will launch in a new tab with a fully functional VS Code interface. From here, you can start coding, running, and debugging your project directly in the cloud. "},{"uri":"https://aaubs.github.io/ds24/en/m2/01_networks/1_networks/","title":"Basics Network Analysis","tags":[],"description":"","content":"\nThis session introduces basic concepts of network theory and analysis.\nNotebooks Lecture\nBasics Network Analysis\nIntermediate Network Analysis\nBipartite and Similarity Graph\nSlides Use arrows keys on keyboard to navigate. Alternatively use fullscreen slides\n"},{"uri":"https://aaubs.github.io/ds24/en/m2/02_nlp/1-nlb-intro/","title":"Basics of NLP","tags":[],"description":"","content":"NOTEBOOKS:\nBag Of Words TF-IDF ============================================\nHow \u0026lsquo;computer\u0026rsquo; understands text? ============================================\nImagine someone who cannot hear and uses sign language to communicate. To understand them, you need to learn the specific signs they use to express words and ideas.\nThe signs represent the meaning of words, but they aren\u0026rsquo;t the words themselves‚Äîjust symbols that convey the message.\nSimilarly, computers don\u0026rsquo;t understand human language directly. Instead, we translate text into a format they understand‚Äîlike sign language for machines. This \u0026lsquo;machine sign language\u0026rsquo; is a numerical representation of text, called \u0026lsquo;vectors\u0026rsquo; or \u0026rsquo;embeddings.\u0026rsquo;\nEach word, phrase, or sentence is converted into a series of numbers that capture its meaning.\n============================================\nProblems? Language is weird! [1] ============================================\nSomething that is easily understood by humans is not always easily understood by computers‚Äîand vice versa! Placing additional word in to sentences can change the main message, but it doesn\u0026rsquo; always convey the same message when converted into a numerical representation of text‚Äîand vice versa!\nLanguages are full of synonims and words that have several meanings. Humans understand the correct meaning by knowing the context.\nPunctuation marks can change the meaning of the sentence.\n============================================\nLevel of analysis - One Big Text (Corpus) vs Many Short Texts ============================================\nThe difference between analyzing a large corpus vs. multiple short texts\nOne Big Text or Corpus of Texts:\nFocuses on extracting meaning, patterns, or structure from a long or cohesive text. Examples: Books, academic articles, or a collection of documents. Example: Analyzing academic article abstracts for topic modeling or sentiment analysis. Many Short Texts:\nInvolves processing multiple, often disconnected, texts that are short and concise. Examples: Social media posts, product reviews, or comments. Example: Processing Amazon reviews to determine customer sentiment or product popularity. What Are We Analyzing in NLP? ============================================\n1. Text as such Analyzing broad characteristics of the entire text or corpus. Examples: Topics: Identifying themes or subjects within the text (e.g., topic modeling). Sentiment: Understanding the emotional tone (e.g., positive, negative, neutral). Language Modeling: Predicting the likelihood of text sequences. Similarity: Measuring how similar two texts or documents are (e.g., document similarity, semantic similarity). 2. Elements Within Text Analyzing smaller components or structures in the text. Examples: Entities: Identifying named entities (e.g., people, organizations, locations) within the text. Relations: Understanding relationships between entities (e.g., person works at a company). N-grams: Extracting consecutive word sequences (e.g., bigrams, trigrams) to analyze co-occurrences or phrase structures. ==========================================\nFrom simple towards more complicated methods ==========================================\nIn the next few slides, we will look at different methods used in NLP:\n1. Bag of Words (BoW) 2. TF-IDF 3. Word2Vec Bag of Words (BoW) ============================================\nWhat is it?\nIt is very simple and easy to use.\nIt uses the numerical representation of text that can be easily understood by machines. It has a wide range of applications in many different fields like Natural Language Processing, Machine Learning etc. It provides an efficient way for computers to understand human language. Steps:\n1. Tokenize the input sentence into individual tokens (words).\n2. Remove stop words from the tokenized list if required.\n3. Create a dictionary of each unique word.\n4. Represent each text as a vector of word counts or frequencies.\nTF-IDF (Term Frequency - Inverse Document Frequency) ============================================\nA statistical measure used to evaluate how important a word is to a document within a corpus.\nTerm Frequency (TF): Measures how often a word appears in a document.\nInverse Document Frequency (IDF): Measures how common or rare a word is across all documents.\nAdvantages:\nHighlights important words specific to a document while down-weighting frequent terms More informative than BoW for distinguishing relevant terms in large corpora. Example\nBoW: A word like \"the\" might be very frequent, but it's not important.\nTF-IDF: Words like \"science\" or \"research\" in an article about technology would get a higher score because they are less frequent in general but more important for the specific document.\nWord2Vec ============================================\nA deep learning-based model that transforms words into continuous, dense vectors (embeddings) that capture semantic meaning.\nAdvantages:\nCaptures word meaning and context. Efficient for representing large vocabularies in a low-dimensional space. Can perform word arithmetic (e.g., \u0026ldquo;king\u0026rdquo; - \u0026ldquo;man\u0026rdquo; + \u0026ldquo;woman\u0026rdquo; ‚âà \u0026ldquo;queen\u0026rdquo;). Limitations:\nRequires large corpora for good performance. Doesn‚Äôt directly handle out-of-vocabulary words. Context is limited to a fixed window size. Comparison: Word2Vec vs. TF-IDF vs. BoW ============================================\nFeature Word2Vec TF-IDF BoW Representation Dense vector embeddings Weighted word frequencies Word counts Context Awareness Yes, considers surrounding words No, treats words independently No, treats words independently Semantic Meaning Captures semantic relationships No, only counts importance based on corpus No, only counts word frequencies Dimensionality Low (dense vectors) High (sparse matrix) High (sparse matrix) Common Use Cases Similarity, analogy, NLP tasks Text classification, relevance scoring Text classification, simple tasks "},{"uri":"https://aaubs.github.io/ds24/en/m5/02_presentations/","title":"In-calss Presentations","tags":[],"description":"","content":"Presentation format Students will deliver 20-25 minute presentations on AI, safety, and ethics, inspired by assigned documents. They will also develop 2-3 questions or interactive formats for class engagement. Topics include politics and safety, planning for AGI, and issues like bias, toxicity, and misinformation in large language models. Presentations may be published on a GitHub page with accompanying blog posts.\n"},{"uri":"https://aaubs.github.io/ds24/en/info/","title":"Info, Schedule &amp; Co","tags":[],"description":"","content":"General info about the semester will be updated here. Check out the calendar on moodle.\nIntro to the semester and module: Intro Slides (pdf) üöÄüêç Python 101 "},{"uri":"https://aaubs.github.io/ds24/en/m3/01_intro-to-traditional-dl/","title":"Intro to Traditional Deep Learning","tags":[],"description":"","content":"This session provides an overview of the foundational elements of deep learning, including its historical context, key concepts, and practical applications. The course will delve into various types of neural networks, outlining their advantages and disadvantages. It will specifically focus on convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory (LSTM) networks, highlighting their unique characteristics and applicability to a range of problem-solving scenarios, including those in economics.\nAuto Insurance in Sweden Swedish Committee on Analysis of Risk Premium in Motor Insurance. read more\nOverview In the dataset:\nX = number of claims Y = total payment for all the claims in thousands of Swedish Kronor for geographical zones in Sweden Reference: Swedish Committee on Analysis of Risk Premium in Motor Insurance\nTasks First step: We will create a simple Artificial Neural Network with 1 node and training with 1 sample of data Second step: The simple Artificial Neural Network will be trained through the dataset Notebooks Intro to Traditional Deep Learning - ANN Intro to Traditional Deep Learning - RNN Intro to Traditional Deep Learning - LSTM Intro to Traditional Deep Learning - CNN Intro to Activation Functions Intro to Gradient Descent Exercises ANN Exercise RNN Exercise LSTM Exercise Solutions ANN Exercise and Solutions LSTM Exercise and Solutions Recommended Datacamp exercises Deep Learning with PyTorch Slides Use arrows keys on keyboard to navigate. Alternatively use fullscreen slides.\nResources PyTorch PyTorch NN Linear "},{"uri":"https://aaubs.github.io/ds24/en/m1/02_rapid_prototyping/01_rapid_prototyping/","title":"Introduction to Streamlit: Building an Employee Attrition Dashboard","tags":[],"description":"","content":" Corgi working on a Data Science project. 2023. Roman x Flux Dev\nStreamlit has rapidly become a go-to tool for data scientists and developers wanting to turn data scripts into shareable web apps. Let\u0026rsquo;s explore its core features and benefits:\nKey Features of Streamlit Simplicity: With just a few lines of Python code, you can have a running web application. No need to deal with HTML, CSS, or JavaScript unless you want to. Interactive Widgets: Streamlit offers out-of-the-box widgets like sliders, buttons, and text inputs that make your app interactive. Data Integration: It seamlessly integrates with popular data science libraries like Pandas, Numpy, Matplotlib, and others. Data Caching: With @st.cache, Streamlit caches the output of functions, ensuring your data operations are efficient and your apps remain performant. Hot Reloading: As you save changes to your script, the app refreshes in real-time. No need to manually restart your app. vs. Gradio While Streamlit offers a robust platform for creating web apps, Gradio provides several distinct advantages, especially when the focus is on deploying machine learning models:\nEase of Model Deployment: Gradio prioritizes simplifying the deployment of machine learning models. Its intuitive Python API allows for quick interface generation, ensuring accessibility even for non-experts. Diverse Input Types: Gradio supports a wide variety of input formats, from images and text to audio, providing flexibility especially when dealing with different data types. Multi-Model Deployment: Gradio stands out with its capability to simultaneously deploy multiple models, perfect for ensemble methods or side-by-side model comparisons. Shareability: Gradio\u0026rsquo;s feature of generating shareable URLs makes collaboration and showcasing a breeze. Security Features: Gradio offers built-in adversarial robustness, adding an extra layer of protection against potential adversarial attacks on deployed models. These unique features make Gradio an attractive option for projects that focus on deploying and sharing machine learning models with diverse requirements. We are going to explore gradio after introductions to ML.\nWhy Choose Streamlit? Rapid Prototyping: Streamlit\u0026rsquo;s intuitive API and hot reloading mean you can quickly iterate and refine your app. Open Source: Being open-source, it boasts a strong community that contributes to its growth and offers a plethora of community plugins. Deployment Ready: With platforms like Streamlit Sharing, deploying your app to the world is just a click away. Extensible: You can integrate advanced JavaScript features or even other Python libraries to extend Streamlit\u0026rsquo;s capabilities. Core Components of Streamlit Layout and Widgets Layout: st.columns and st.container can be used to design your app\u0026rsquo;s layout. Widgets: These are interactive elements like st.slider(), st.selectbox(), and st.button() that capture user input. Display Elements Media: Display images, videos, or audio clips using st.image(), st.video(), and st.audio(). Charts: Use st.line_chart(), st.bar_chart(), or integrate with libraries like Altair for custom visualizations. Tables: Showcase data with st.table() or st.dataframe(). Session State State: Store user data or app state across reruns with st.session_state. Dive Deeper To truly master Streamlit, it\u0026rsquo;s recommended to experiment with building various apps and exploring its official documentation. The community is active, and there\u0026rsquo;s always something new to learn!\nWhat You\u0026rsquo;ll Build By the end of this tutorial, you\u0026rsquo;ll have a functional dashboard that allows users to:\nFilter data based on various employee metrics. Visualize attrition rates and patterns across different dimensions. Understand insights from visualizations. Get actionable recommendations based on data insights. Prerequisites Before you begin, ensure you have the following:\nBasic knowledge of Python. Streamlit installed (pip install streamlit). Familiarity with data visualization libraries like altair, matplotlib, and seaborn. Getting Started 1. Data Source We\u0026rsquo;ll use a synthetic dataset on employee attrition from this link.\n2. File Structure Ensure your working directory has the following structure:\nüì¶Your_Directory ‚î£ üìúapp.py ‚îó üìúrequirements.txt ‚îó üìúdata.csv app.py will contain our Streamlit app\u0026rsquo;s code, while requirements.txt will list the necessary Python packages.\n3. Hosting the App Once you\u0026rsquo;ve built the app, we\u0026rsquo;ll first host it on Streamlit Cloud and then on uCloud (A Danish private cloud for universities).\nBuilding the Dashboard Here is the code\nInitialization We start by importing necessary libraries and loading our dataset. The data is cached using @st.cache_data to enhance performance.\nDesigning the Interface Streamlit provides intuitive functions to design the user interface:\nst.title() and st.header() set titles and headers. st.markdown() allows for rich text formatting. st.sidebar lets you add interactive widgets in a sidebar for filtering. Visualizations Depending on user input, we visualize the filtered data using various charts. For instance, altair is used for bar and pie charts, while matplotlib and seaborn provide KDE plots and boxplots.\nInsights and Recommendations Finally, expanders provide a space to share insights derived from visualizations and actionable recommendations.\nHosting In 2024, the best free option is Hugging Face.\nCreate an empty Streamlit Space. Log into Hugging Face from your development platform (local, uCloud, Codespaces) using the Command Line Interface (CLI) with a token that you obtain from Hugging Face. Then add your email and name for Git to connect. Clone your empty repository. Add your app.py and requirements.txt files into the repository folder. Finally, push and deploy your app. Happy Coding! üöÄ\nOld corgy from last years\u0026hellip; Corgi working on a Data Science project. 2023. Roman x Stable Diffusion XL\nCorgi working on a Data Science project. 2022. Roman x Stable Diffusion\n"},{"uri":"https://aaubs.github.io/ds24/en/m6/l1/","title":"Lecture 1  Introduction to Serverless ML and Databases","tags":[],"description":"","content":"\nUse GitHub repo https://github.com/saoter/SDS24_MLOps_L1, which contains lecutre slides, python scripts and dataset for the first lecture.\nSLIDES Lecture 1 API api_jokes.py is an example of simple public API api_finance.py, api_news.py are examples of private API using API_key (You need to register to get your private key) api_datasets.py is an example of API Wrapper EXTRA: joke_app.py is a simple streamlit app that uses IPA to print jokes EXTRA: api_weather.py exercise from the class Database db_create.py reads in excel file and transform it into SQL database db_queries.py contains a few query examples db_add_datapoints.py add new transactions db_add_columns.py add new columns data/Online Retail.xlsx is dataset used to create database "},{"uri":"https://aaubs.github.io/ds24/en/m6/l2/","title":"Lecture 2  Refactoring &amp; First Serverless App","tags":[],"description":"","content":"\nSLIDES Lecture 2 "},{"uri":"https://aaubs.github.io/ds24/en/m6/l3/","title":"Lecture 3: Credit Card Prediction Service Project","tags":[],"description":"","content":"\nSlides Original Slides from the Serverless ML course Exercise Clone repository and execute the feature-pipelines Modify parameters in the feature-pipeline to produce "},{"uri":"https://aaubs.github.io/ds24/en/m6/l5/","title":"Lecture 5  Feature Selection, Batch Inference Pipelines, Model Registry","tags":[],"description":"","content":"\rUse GitHub repo https://github.com/saoter/SDS24_MLOps_L5, which contains lecutre slides, python scripts and dataset for the this lecture.\nSLIDES Lecture 5 TASKS First, we continue with Hopsworks Module 3 LAB\nSecond, we start with two-lecture project\nFiles in repo:\n- 1_data_to_sql.ipynb\r- 2_train_first_model.ipynb\r- 3_train_additional_models.ipynb\r"},{"uri":"https://aaubs.github.io/ds24/en/m6/l6/","title":"Lecture 6  Feature Selection, Batch Inference Pipelines, Model Registry","tags":[],"description":"","content":"\rSLIDES Lecture 6 TASKS First, we continue with Hopsworks Module 4 LAB\nSecond, we will continue with our exercise from Lecture 5\nRepositories:\n- https://github.com/saoter/render_api\r- https://github.com/saoter/render_streamlit\r"},{"uri":"https://aaubs.github.io/ds24/en/m5/01_legal/","title":"LEgal","tags":[],"description":"","content":"This chapter introduces you to the basics of deep learning, including its history, important concepts, and applications.\nLiterature Literature: LeCun, Y., Bengio, Y., \u0026amp; Hinton, G. (2015) Recommended Datacamp exercises Deep Learning with PyTorch "},{"uri":"https://aaubs.github.io/ds24/en/m1/01_intro_ds/02_data_handeling/","title":"- Data Handling and Manipulation","tags":[],"description":"","content":"\nThis session will introduce students to the foundational aspects of data handling in Python. Students will learn about the different types of data that are important in data science, and they will explore essential operations like arrange, group-by, filter, select, and join. By the end of this session, students should have a solid understanding of primary data manipulation techniques, setting the stage for more advanced subjects.\nSession 2: Data Handling and Manipulation I (Lecture) In this session, we\u0026rsquo;ll explore the foundational aspects of data handling. Key takeaways include:\nData Manipulation: Learn essential operations like arrange, group-by, filter, select, and join, preparing data for analysis. Notebook: Python Data Manipulation By the end, students should have a solid understanding of primary data manipulation techniques, setting the stage for more advanced subjects.\nNotebook: Python Data Manipulation and solutions\nExercises Notebook: Python DS Handbook, C.2-3 exercises\nNotebook: Pandas exercises\nNotebook: Netflix Pandas exercises\nNotebook: HR Attrition exercises\nHere, you will find the answers to the exercises:\nNotebook: Python DS Handbook, C.2-3 exercises and solutions\nNotebook: Pandas exercises and solutions\nNotebook: HR Attrition exercises solutions\nNotebook: Netflix Pandas exercise solutions\nTutorial - Assignment Examples Further studies Recommended DataCamp courses Introduction to Python Intermediate Python Recommended readings Python fo Data Science Handbook (VanderPlas, 2016), Chapter 2-3 "},{"uri":"https://aaubs.github.io/ds24/en/m1/03_uml/02_recommender_simsea_uml/","title":"- Recommendation and Similarity Search","tags":[],"description":"","content":"\nIn this workshop we are going to learn about recommender systems as a type of UML. Such systems are probably the most widely used and commercialy valuable form of AI today. Specifically we will be looking into collaborative filtering and matrix factorization.\nPlan for today Collaborative filtering / SVD recommender using Nomadlist Trips-data in a Notebooks Streamlit recommender-app Notebook(s) Recap Similarity Notebook\nRecap Similarity Notebook - Solutions\nNomadlist Trips Notebook\nNomadlist Trips Notebook Solutions\nRecap Nomadlist Trips Notebook\nRecap Dimensionality Reduction\nClass UML Example cosine similarity\nApp Source code for the app Deployed recommender app Recommended Readings and resources This excellent PyData Talk by the developer of LightFM Recommended Datacamp exercises: Beginner Tutorial RecSys-Course "},{"uri":"https://aaubs.github.io/ds24/en/m1/04_sml/02-sml-addon/","title":"- SML - Further topics","tags":[],"description":"","content":"\nThis session introduces adittional topics of intertest in SML\nNotebook(s) Introduction to Explainable ML Model training for deployment - Airbnb Case Inference - Airbnb Case Repos (Deployment) Streamlit Repo FastAPI Repo Addon Data engineeering and pipelines "},{"uri":"https://aaubs.github.io/ds24/en/m1/01_intro_ds/","title":"A) Introduction to Data Science (W35-36)","tags":[],"description":"","content":" Note: Group Portfolio Assignment - Exploratory Data Analysis (EDA) Deadline: Friday, 13 September 2024, 12:00 PM\nThis topic includes 5 sessions as follows:\nWelcome to Data Science! (Monday, September 2nd, 08:15-14:15): This session will introduce students to the fundamentals of data science, with a focus on Python. Students will learn about the Python data science stack, essential tools and platforms, and software setup. They will also get a preview of the upcoming weeks and a refresher on Python basics. Data Handling and Manipulation I (Lecture) (Wednesday, September 4th, 10:15-14:15): This session will cover the foundational aspects of data handling in Python. Students will learn about the different types of data that are important in data science, and they will explore essential operations like arrange, group-by, filter, select, and join. By the end of this session, students should have a solid understanding of primary data manipulation techniques. Exploratory Data Analysis \u0026amp; Essential Statistics (Thursday, September 5th, 08:15-12:00): This session will introduce students to exploratory data analysis (EDA) and essential statistics. Students will learn how to use EDA to uncover patterns, anomalies, and frame questions in data. They will also learn about foundational measures and techniques for data interpretation. Data Visualization in Data Science (Friday, September 6th, 08:15-12:00): This session will teach students the importance of effective data visualization in data science. Students will explore Seaborn, a Python library for intuitive statistical graphics, and Altair, a declarative visualization library for Python. They will also have the opportunity to create impactful visualizations with real datasets through hands-on exercises. Building an Employee Attrition Dashboard (Monday, September 9th, 10:15-14:15): This session will guide students through creating an interactive Employee Attrition Dashboard using Streamlit, a powerful Python framework for building data-driven web applications. Participants will learn how to integrate data processing, visualization, and interactivity into a cohesive dashboard. "},{"uri":"https://aaubs.github.io/ds24/en/m1/","title":"Applied Data Science and Machine Learning","tags":[],"description":"","content":"M1 - Applied Data Science and Machine Learning Note: M1 - Final Assignment Deadline:\nThis module provides a condensed introduction to the ‚ÄúData Science Pipeline,‚Äù introducing students to methods, techniques, and workflows in applied data analytics and machine learning, including data acquisition, preparation, analysis, visualization, and communication. This module includes the following four main topics:\nA) Introduction to Data Science B) Rapid Prototyping C) Unsupervised Machine Learning D) Supervised Machine Learning "},{"uri":"https://aaubs.github.io/ds24/en/m3/01_group_assignment/","title":"Group Assignment 1","tags":[],"description":"","content":"Portfolio Exercise 1 Note: M3 - Group Assignment 1 Deadline: Wednesday 7th of February at 12.00 PM\nIntroduction In this assignment, you are required to delve into the practical aspects of Deep Learning by constructing and evaluating a neural network using PyTorch. This exercise is designed to deepen your understanding of neural network architectures, hyperparameter tuning, and the preprocessing steps necessary for effective model training and evaluation. You will have the freedom to choose a dataset from either the M1 or M2 module or select an external dataset that intrigues you. By experimenting with different neural network configurations and hyperparameters, you will gain hands-on experience in optimizing ML models to achieve desired performance metrics.\nTask Build, train, and evaluate a neural network using Pytorch. The neural network should have a minimum of 2 hidden layers. Experiment with at least 5 different variations of hyperparameters (e.g., number of layers/neurons, activation functions, epochs, optimizers, learning rates, etc.). The assignment should include the following steps:\nFeature Selection Feature Engineering (if necessary) Standard ML Preprocessing (if necessary) Train-Test Split Defining a Neural Network Architecture in Pytorch Defining a Training Loop Training the Model Experimenting with Different Hyperparameters Evaluating the Final Model on the Test Data Data Choose a dataset from the M1 or M2 module, or other datasets if you prefer. Delivery Create a GitHub repository. Save the Colab notebook in the repository. Provide a README.md with a brief description of the assignment. Submissions can be made in groups of up to 3 members. Submit the assignment by sending an email with the link to the repository to Hamid (hamidb@business.aau.dk). "},{"uri":"https://aaubs.github.io/ds24/en/m3_eco/01_group_assignment/","title":"Group Assignment 1","tags":[],"description":"","content":"Portfolio Exercise 1 Note: M3 - Group Assignment 1 Deadline: Monday, 6th of November at 12:00 PM\nIntroduction In this assignment, you are required to delve into the practical aspects of Deep Learning by constructing and evaluating a neural network using PyTorch. This exercise is designed to deepen your understanding of neural network architectures, hyperparameter tuning, and the preprocessing steps necessary for effective model training and evaluation. You will have the freedom to choose a dataset from either the M1 or M2 module or select an external dataset that intrigues you. By experimenting with different neural network configurations and hyperparameters, you will gain hands-on experience in optimizing ML models to achieve desired performance metrics.\nTask Build, train, and evaluate a neural network using Pytorch. The neural network should have a minimum of 2 hidden layers. Experiment with at least 5 different variations of hyperparameters (e.g., number of layers/neurons, activation functions, epochs, optimizers, learning rates, etc.). The assignment should include the following steps:\nFeature Selection Feature Engineering (if necessary) Standard ML Preprocessing (if necessary) Train-Test Split Defining a Neural Network Architecture in Pytorch Defining a Training Loop Training the Model Experimenting with Different Hyperparameters Evaluating the Final Model on the Test Data Data Choose a dataset from the M1 or M2 module, or other datasets if you prefer. Delivery Create a GitHub repository. Save the Colab notebook in the repository. Provide a README.md with a brief description of the assignment. Submissions can be made in groups of up to 3 members. Submit the assignment by sending an email with the link to the repository to Hamid (hamidb@business.aau.dk). "},{"uri":"https://aaubs.github.io/ds24/en/m2/02_nlp/","title":"Natural Language Processing","tags":[],"description":"","content":"This chapter introduces you to statistical natural language processing. We will be focusing on NLP in combination with supervised ML as well as topic modelling for unsupervised approaches.\nRecommended Readings: Bird, S., Klein, E., \u0026amp; Loper, E. (2009). Natural language processing with Python: analyzing text with the natural language toolkit. \u0026quot; O\u0026rsquo;Reilly Media, Inc.\u0026quot;. Docs and linked papers: Gensim Docs Spacy Pre-recorded theory intro recording from 2021, in case you\u0026rsquo;d like to review\nNLP intro - level of analysis Text representation From BoW to Topic Modeling and Embeddings (optional) History of NLP in Industry - Yoav Goldberg "},{"uri":"https://aaubs.github.io/ds24/en/m2/02_nlp/2-seq2seq/","title":"Seq2Seq &amp; Transformers","tags":[],"description":"","content":"NOTEBOOKS:\nTrans learning with BERT Seq2Seq: The Next Step in NLP After Word2Vec ==================================\nWord2Vec was great at learning word representations, but it lacked the ability to process or generate sequences (e.g., sentences). What Seq2Seq Introduced:\nSeq2Seq models are designed for tasks where both input and output are sequences, such as: Machine translation (English to French) Text summarization Chatbot responses How Seq2Seq Works: ==================================\nEncoder-Decoder Architecture: Encoder: Processes the input sequence (e.g., a sentence) and converts it into a fixed-size vector (context vector).\nDecoder: Takes the context vector and generates the output sequence (e.g., the translated sentence).\n==================================\nFrom RNN to LSTM to Transformers (seq2seq) ==================================\n1. RNN (Recurrent Neural Networks) Introduced: 1980s\nStrength: First model to handle sequences by using a \u0026ldquo;hidden state\u0026rdquo; to retain information.\nLimitation:\nVanishing Gradient Problem: Struggles to retain information over long sequences. Difficult to process long sentences or paragraphs. Sequential processing makes them slow to train. 2. LSTM (Long Short-Term Memory) Introduced: Paper published 1997\nStrength: Improved version of RNN designed to solve the vanishing gradient problem.\nKey Feature:\nGating Mechanism (input, forget, output gates) allows selective retention of information. Better at handling long-term dependencies. Limitation:\nStill processes words sequentially, limiting training speed and efficiency. 3. Transformers Introduced: 2017 (by Google researchers)\nStrength: Revolutionized sequence processing with self-attention and parallelization.\nKey Features:\nSelf-Attention: Enables the model to focus on all parts of the input sequence at once, handling long-range dependencies effectively. Parallel Processing: Unlike RNNs/LSTMs, Transformers process all words simultaneously, making them much faster to train. Results: Can handle massive datasets and generate state-of-the-art results in translation, summarization, and text generation (e.g., GPT-3, BERT).\nWhy Transformers Are Important ==================================\nTransformers overcame these challenges:\nHighly parallelizable, allowing efficient training on large datasets. Scales well with data‚Äîmodels like GPT-3 were trained on 45 terabytes of text! Key elements of Transformers [1] ==================================\nPositional Encodings:\nInstead of sequential processing like RNNs, Transformers use positional encodings to capture the order of words. Words are tagged with a position number (e.g., 1, 2, 3), and the model learns how to interpret the order during training. Example: \u0026lsquo;hot dog\u0026rsquo; together have differnt meaning as \u0026lsquo;dog that is hot\u0026rsquo;\nAttention Mechanism:\nAttention allows the model to focus on specific parts of the input sentence when making predictions\nIt works like a heatmap for how much focus the model gives to certain words.\nSelf-Attention: An extension of attention where the model looks at the whole input sequence to understand the meaning of a word in context. Example:\nThe animal didn\u0026rsquo;t cross the street because it was too tired.\nThe animal didn\u0026rsquo;t cross the street because it was too wide.\nSelf-attention allows the model to recognize that \u0026ldquo;it\u0026rdquo; refers to \u0026ldquo;the animal\u0026rdquo; or \u0026ldquo;the street\u0026rdquo;!\nTransfer Learning: Fine-tuning Pre-trained Models ==================================\nTransfer learning is a crucial technique in NLP, especially with the advent of large pre-trained models like BERT (Bidirectional Encoder Representations from Transformers), GPT, and others. These models are trained on massive datasets and can be fine-tuned on specific tasks such as binary classification (e.g., classifying reviews as positive or negative) by leveraging their learned knowledge.\nHow Transfer Learning Works: Pre-training:\nA large model like BERT is first trained on a massive corpus of text to learn general language patterns. The model learns rich representations of language, such as grammar, semantics, and context. Fine-tuning:\nAfter pre-training, the model is fine-tuned on your specific dataset (e.g., sentiment analysis or classification tasks). During this phase, the model adjusts its weights based on the specific examples in your dataset to learn the target task. Example: Binary Classification with BERT Dataset Preparation: Prepare a dataset with two classes (e.g., positive and negative reviews). Model Setup: Use a pre-trained BERT model from libraries like HuggingFace\u0026rsquo;s Transformers. Fine-tuning: Train the BERT model on your dataset, adjusting the final layer for a binary classification task. Evaluation: After fine-tuning, evaluate the model on unseen test data to check performance. Advantages of Transfer Learning: Faster Training: Pre-trained models already know a lot about language, so they require less data and time to train on specific tasks. Better Accuracy: Transfer learning often leads to improved performance, especially when data is scarce. "},{"uri":"https://aaubs.github.io/ds24/en/m1/01_intro_ds/03_data_visualization_stat/","title":"- Exploratory Data Analysis and Essential Statistics","tags":[],"description":"","content":"This session introduces students how to use EDA and some fundemental concepts of statistical methods to uncover patterns, anomalies, and frame questions in data. Students will learn foundational measures and techniques for data interpretation, and they will have the opportunity to apply EDA and statistical methods on datasets through hands-on exercises.\nDeep Dive into EDA: Uncover patterns, anomalies, and frame questions. Key Statistical Concepts: Foundational measures and techniques for data interpretation. Hands-on Analysis: Apply EDA and statistical methods on datasets. Part 1: Statistics refresher Notebook statistics refresher Notebook statistics refresher - Solutions Part 2: Further concepts Notebook propability distributions Notebook AB testing Further studies Recommended DataCamp courses Intyroduction to statistics (no coding) Statistical Thinking in Python I Statistical Thinking in Python II Statistical Simulation in Python Recommended readings Econometrics with Python - Causal Inference for The Brave and True: More thorrough inferential statistics in Python "},{"uri":"https://aaubs.github.io/ds24/en/m1/02_rapid_prototyping/","title":"B) Rapid Prototyping (W37)","tags":[],"description":"","content":"This topic includes 2 sessions as follows:\nRapid Prototyping with Streamlit (Monday, September 9th 8:15-12:00, Kst3-2A (4.130)): This session will cover the basics of rapid prototyping in data science, with a focus on Streamlit. Students will learn how to use this library to create interactive web applications that can be used to explore data and test hypotheses. We will look into hosting on Huggingface. Real-World Data to Online Dashboard (Tuesday, September 11th, 8:15-12:00, Kst3-2A (4.130)): This session will focus on creating interactive online dashboards using real-world data - and extend to more advanced use-cases. After the lecture/workshop TAs will be available to help with the assignment of the week - creating a dashboard with your group-dataset. "},{"uri":"https://aaubs.github.io/ds24/en/m1/02_rapid_prototyping/02_online_dashboard/","title":"GeoPandas","tags":[],"description":"","content":"This session will focus on building interactive online dashboards using real-world geospatial data, with an emphasis on GeoPandas for data manipulation and visualization. Students will learn how to use GeoPandas to read, filter, and manipulate geospatial data, create interactive map-based visualizations, and ultimately deploy their dashboards to the web.\nIntroduction to GeoPandas Using GeoPandas to analyze geospatial data will be our focus in these notebooks.\nGeoPandas GeoPandas and Solutions GeoPandas Hands-on Project GeoPandas Hands-on Project and Solutions GeoPandas Exercises GeoPandas exercises GeoPandas exercises and solutions From Geopandas to Streamlit App üöÄ Simplified version of the analysis - with Folium Plotting App code Deployed App What to do now?! Replay code from the course and see if you do understand the core mechanics - you DO NOT need to remember everything. Android app market project on datacamp Course: Python DS toolbox 1 \u0026amp; Course: Python DS toolbox 2 Opendata.dk - build a map of different attractions in Aalborg based on public data. See preprocessing example - how to get data out of nested JSON - below: This is how you can preprocess the GeoCoordinates from the JSON file:\n#Load pandas import pandas as pd # Read the file from remote data = pd.read_json(\u0026#39;https://admin.opendata.dk/dataset/44ecd686-5cb5-40f2-8e3f-b5e3607a55ef/resource/eeabb0f8-1b19-4c80-b059-5ba5c4c872d2/download/guidedenmarkaalborgenjson.json\u0026#39;) # The GeoCoordinates are hiding in the Address column data[\u0026#39;Address\u0026#39;][0][\u0026#39;GeoCoordinate\u0026#39;] # You can use list comprehension to pull out GeoCoordinates (also empty values) - try out # This will allow you to filter for missing data without fancy workarounds [x[\u0026#39;GeoCoordinate\u0026#39;] for x in data[\u0026#39;Address\u0026#39;]] # Make a new column based on that to be used for filtering out missing data data[\u0026#39;GeoCoordinate\u0026#39;] = [x[\u0026#39;GeoCoordinate\u0026#39;] for x in data[\u0026#39;Address\u0026#39;]] # drop, where no GeoCoordinate data = data.dropna(subset=[\u0026#39;GeoCoordinate\u0026#39;]) # Pull out the values data[\u0026#39;latitude\u0026#39;] = [x[\u0026#39;Latitude\u0026#39;] for x in data[\u0026#39;GeoCoordinate\u0026#39;]] data[\u0026#39;longitude\u0026#39;] = [x[\u0026#39;Longitude\u0026#39;] for x in data[\u0026#39;GeoCoordinate\u0026#39;]] "},{"uri":"https://aaubs.github.io/ds24/en/m3/02_intro-tm/","title":"Intro to Transformer Models","tags":[],"description":"","content":"\nLiterature Sutskever, I., Vinyals, O., \u0026amp; Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., \u0026hellip; \u0026amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.\nThe illustrated transformer\nSimple transformer LM\nNotebooks - Basics Transformer Models - Basics Notebooks - Applications TM Applications - SBERT TM Applications - HF Simple transformer LM SBERT for Patent Search using PatentSBERTa in PyTorch Notebooks - FineTuning TM FineTuning - SimpleTransformers TM FineTuning - SBERT TM FineTuning - HF SetFit Hatespeech vs bert and distilroberta Seq2Seq - Neural Machine Translation Slides - Attention Mechanism Slides - SBERT Classification with various vectorization approaches TF-IDF and W2V Multi-Class Text Classification BERT Multi-Class Text Classification Implementing Multi-Class Text Classification LSTMs using PyTorch Resources OG SBERT-Paper Reimers, N., \u0026amp; Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084. SBERT Docu NLP with SBERT - an ebook/course on the use of dense vectors (with SBERT for business applications) SBERT-Training Tutorial BERTopic - a framework for topic modelling with SBERT embeddings Milvus - Vector database "},{"uri":"https://aaubs.github.io/ds24/en/m3_eco/02_intro-tm/","title":"Intro to Transformer Models","tags":[],"description":"","content":"\nLiterature Sutskever, I., Vinyals, O., \u0026amp; Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., \u0026hellip; \u0026amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.\nThe illustrated transformer\nSimple transformer LM\nNotebooks - Basics Transformer Models - Basics Notebooks - Applications TM Applications - SBERT TM Applications - HF Simple transformer LM SBERT for Patent Search using PatentSBERTa in PyTorch Notebooks - FineTuning TM FineTuning - SimpleTransformers TM FineTuning - SBERT TM FineTuning - HF SetFit Hatespeech vs bert and distilroberta Seq2Seq - Neural Machine Translation Slides - Attention Mechanism Slides - SBERT Classification with various vectorization approaches TF-IDF and W2V Multi-Class Text Classification BERT Multi-Class Text Classification Implementing Multi-Class Text Classification LSTMs using PyTorch Resources OG SBERT-Paper Reimers, N., \u0026amp; Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084. SBERT Docu NLP with SBERT - an ebook/course on the use of dense vectors (with SBERT for business applications) SBERT-Training Tutorial BERTopic - a framework for topic modelling with SBERT embeddings Milvus - Vector database "},{"uri":"https://aaubs.github.io/ds24/en/info/02_modules/","title":"Modules","tags":[],"description":"","content":"For Business Data Science Students M1: Data Handling, Exploration \u0026amp; Applied Machine Learning 10 ECTS\nThis module will prove a condensed introduction to the ‚ÄúData Science Pipeline‚Äù, introducing students to methods, techniques, and workflows in applied data analytics and machine learning, including data acquisition, preparation, analysis, visualization, and communication.\nM2: Network Analysis and Natural Language Processing 5 ECTS\nFocuses on analyzing a variety of unstructured data sources. Particularly, students will learn how to explore, analyze, and visualize natural language (text) as well as relational (network) data.\nM3: Advanced Innovation Management (Project - Data-Driven Business Modelling and Strategy) 15 ECTS Course with integrated project in which you will learn how organizations manage innovation and technological change. In the project you will work wich a company case and build a \u0026ldquo;mini\u0026rdquo; version of the product/process.\nFor Social Data Science Students - Elective Semester M3: (SDS) Deep Learning and Artificial Intelligence for Analytics 5 ECTS\nIntroduces to the most recent developments in machine learning, which are deep learning and artificial intelligence applications. The module will provide a solid foundation for this exciting and rapidly developing field. Students will learn whether and how to apply deep learning techniques for business analytics, and acquire proficiency in new methods autonomously.\nCapstone Project Semester project utilising techniques and approaches from SDS in the context of a problem related to your main study field.\n"},{"uri":"https://aaubs.github.io/ds24/en/m2/01_networks/","title":"Network Analysis","tags":[],"description":"","content":"\nThis chapter introduces you to network analysis and working with relational data.\n"},{"uri":"https://aaubs.github.io/ds24/en/m2/","title":"NLP &amp; Network Analysis","tags":[],"description":"","content":"M2 - NLP \u0026amp; Network Analysis This module provides a condensed introduction analyzing two popular forms of unstructured data, namely relational and text data.\n"},{"uri":"https://aaubs.github.io/ds24/en/m1/03_uml/04_intro_kmeans/","title":"- Introduction to Clustering: K-means and Hierarchical Approaches","tags":[],"description":"","content":"This session will introduce the principles and applications of clustering. Students will learn about the different types of clustering problems, and they will explore some of the most popular clustering algorithms, such as K-means and hierarchical clustering.\nNotebook(s) Hands-on Intro Clustering Hands-on Intro Clustering Solutions App NBA Player Injury Replacement Recommender NBA Player Injury Replacement Recommender Streamlit Recommended Datacamp exercises: Python Recommended Readings and resources Python Data Science Handbook Chapter 5 What Is Machine Learning? Introducing Scikit-Learn Feature Engineering In Depth: Principal Component Analysis In Depth: k-Means Clustering "},{"uri":"https://aaubs.github.io/ds24/en/m1/03_uml/","title":"C) Intro to Unsupervised Machine Learning (W38)","tags":[],"description":"","content":" Note: Note: Unsupervised Machine Learning Assignment Submission Deadline: Friday, 20 September 2024, 12:00\nThis topic includes 5 sessions as follows:\nIntroduction to Unsupervised Machine Learning (Mon, Sep 16th, 12:30-16:15): This session will dive into the foundational concepts and real-world uses of unsupervised machine learning (UML). As part of this, students will gain insights into various UML challenges. Furthermore, they will explore notable UML algorithms, including PCA, SVD, NMF, and an introduction to clustering via k-means.\nUML 2: Geospatial Data (Tue, Sep 17th, 08:15-12:00): This session will focus on geospatial data and its application to UML. Students will learn about different methods to analyze and visualize spatial data using UML techniques.\nUML 3: Recommendation \u0026amp; Similarity Search (Fri, Sep 20th, 08:15-12:00): This session will explore the application of UML to recommendation systems and similarity search. Students will learn about different types of recommendation systems and explore how UML can improve recommendation accuracy and relevance.\nClustering Techniques: K-means \u0026amp; Hierarchical (Fri, Sep 20th, 08:15-12:00): This session will introduce the principles and applications of clustering, including popular algorithms like K-means and hierarchical clustering.\n"},{"uri":"https://aaubs.github.io/ds24/en/m3/02_group_assignment/","title":"Group assignment 2","tags":[],"description":"","content":"Portfolio Exercise 2: Transformer Models Note: M3 - Group Assignment 2 Deadline: Wednesday, February 14th at 10:00 AM\nIntroduction This exercise is designed to deepen your understanding and skills in modern deep learning techniques. We have two main tasks for you. The first is focused on using SBERT for semantic search, and the second involves hands-on exercises with gradient descent and the attention mechanism.\nPart 1: SBERT and Semantic Search Task Description Create something innovative using SBERT and semantic search, or even more! The guidelines are intentionally broad to encourage creativity. Here are some ideas to get you started:\nImplement a GIF search engine or YouTube search function using images and CLIP. (Optional) Use SetFit for supervised tasks with SBERT models. Consider building a search engine using a Gradio or Streamlit app. Part 2: Gradient Descent and Attention Mechanism Exercises Task Description Gradient Descent Exercise: Execute the process of updating weights for two examples using Stochastic Gradient Descent (SGD). Document each step, including input calculation, prediction, loss assessment, weight adjustments, and updates.\nAttention Mechanism Exercise: Implement the attention mechanism on two distinct sentences. Choose sentences with polysmous words to demonstrate its functionality effectively.\nWorkshop 2 - Transformer and Attention examples Data You may utilize datasets from ü§ó Hugging Face, Kaggle, or create your own. For inspiration, refer to the GIF search engine and YouTube search projects. Delivery Create a dedicated GitHub repository for this assignment. Store all relevant materials, including the Colab notebook, in the repository. Provide a README.md file with a concise description of the assignment and its components. You may work individually or in groups of up to three members. Submit your work by emailing a link to the repository to Hamid (hamidb@business.aau.dk). "},{"uri":"https://aaubs.github.io/ds24/en/m3_eco/02_group_assignment/","title":"Group assignment 2","tags":[],"description":"","content":"Portfolio Exercise 2: Transformer Models Note: M3 - Group Assignment 2 Deadline: Wednesday, November 15th at 10:00 AM\nIntroduction This exercise is designed to deepen your understanding and skills in modern deep learning techniques. We have two main tasks for you. The first is focused on using SBERT for semantic search, and the second involves hands-on exercises with gradient descent and the attention mechanism.\nPart 1: SBERT and Semantic Search Task Description Create something innovative using SBERT and semantic search, or even more! The guidelines are intentionally broad to encourage creativity. Here are some ideas to get you started:\nImplement a GIF search engine or YouTube search function using images and CLIP. (Optional) Use SetFit for supervised tasks with SBERT models. Consider building a search engine using a Gradio or Streamlit app. Part 2: Gradient Descent and Attention Mechanism Exercises Task Description Gradient Descent Exercise: Execute the process of updating weights for two examples using Stochastic Gradient Descent (SGD). Document each step, including input calculation, prediction, loss assessment, weight adjustments, and updates.\nAttention Mechanism Exercise: Implement the attention mechanism on two distinct sentences. Choose sentences with polysmous words to demonstrate its functionality effectively.\nData You may utilize datasets from ü§ó Hugging Face, Kaggle, or create your own. For inspiration, refer to the GIF search engine and YouTube search projects. Delivery Create a dedicated GitHub repository for this assignment. Store all relevant materials, including the Colab notebook, in the repository. Provide a README.md file with a concise description of the assignment and its components. You may work individually or in groups of up to three members. Submit your work by emailing a link to the repository to Hamid (hamidb@business.aau.dk). "},{"uri":"https://aaubs.github.io/ds24/en/m3/03_intro-to-transformer-models/4_group_assignment-3/","title":"Group assignment 3","tags":[],"description":"","content":"Portfolio Exercise 3: Introduction Task Create something cool üöÄ using SBERT and semantic search or perhaps more?! That\u0026rsquo;s a bit of a vague description but there are many options.\nYou are welcome to use images and CLIP You can also use SetFit for supervised tasks with SBERT models. Here are some projects for inspiration:\nGIF search engine Youtube search some more ideas:\nget some podcast transcripts for a specific topic (or create transcripts with Whisper - search for OpenAI Whisper Colab) Finetune an SBERT model using domain adaptation Embed and build a search engine Build a Gradio app Feel free to choose any of these ideas or come up with your own. The goal is to use SBERT and semantic search (or other techniques) to create something interesting and useful.\nData ü§ó datasets Kaggle make your own Delivery Create a github repository (or use the existing one and adapt it) Create a Gradio demo of the model in inference mode Save colab notebook in the github. Provide a readme.md with brief description. Submission can be in groups up to 3. Submit by sending an email with link to repo to Hamid (hamidb@business.aau.dk) with Daniel \u0026amp; Roman in cc. (dsh@\u0026hellip;, roman@\u0026hellip;) "},{"uri":"https://aaubs.github.io/ds24/en/m3_eco/03_intro-to-transformer-models/4_group_assignment-3/","title":"Group assignment 3","tags":[],"description":"","content":"Portfolio Exercise 3: Introduction Task Create something cool üöÄ using SBERT and semantic search or perhaps more?! That\u0026rsquo;s a bit of a vague description but there are many options.\nYou are welcome to use images and CLIP You can also use SetFit for supervised tasks with SBERT models. Here are some projects for inspiration:\nGIF search engine Youtube search some more ideas:\nget some podcast transcripts for a specific topic (or create transcripts with Whisper - search for OpenAI Whisper Colab) Finetune an SBERT model using domain adaptation Embed and build a search engine Build a Gradio app Feel free to choose any of these ideas or come up with your own. The goal is to use SBERT and semantic search (or other techniques) to create something interesting and useful.\nData ü§ó datasets Kaggle make your own Delivery Create a github repository (or use the existing one and adapt it) Create a Gradio demo of the model in inference mode Save colab notebook in the github. Provide a readme.md with brief description. Submission can be in groups up to 3. Submit by sending an email with link to repo to Hamid (hamidb@business.aau.dk) with Daniel \u0026amp; Roman in cc. (dsh@\u0026hellip;, roman@\u0026hellip;) "},{"uri":"https://aaubs.github.io/ds24/en/m4/03_intro-to-transformer-models/4_group_assignment-3/","title":"Group assignment 3","tags":[],"description":"","content":"Portfolio Exercise 3: Introduction Task Create something cool üöÄ using SBERT and semantic search or perhaps more?! That\u0026rsquo;s a bit of a vague description but there are many options.\nYou are welcome to use images and CLIP You can also use SetFit for supervised tasks with SBERT models. Here are some projects for inspiration:\nGIF search engine Youtube search some more ideas:\nget some podcast transcripts for a specific topic (or create transcripts with Whisper - search for OpenAI Whisper Colab) Finetune an SBERT model using domain adaptation Embed and build a search engine Build a Gradio app Feel free to choose any of these ideas or come up with your own. The goal is to use SBERT and semantic search (or other techniques) to create something interesting and useful.\nData ü§ó datasets Kaggle make your own Delivery Create a github repository (or use the existing one and adapt it) Create a Gradio demo of the model in inference mode Save colab notebook in the github. Provide a readme.md with brief description. Submission can be in groups up to 3. Submit by sending an email with link to repo to Hamid (hamidb@business.aau.dk) with Daniel \u0026amp; Roman in cc. (dsh@\u0026hellip;, roman@\u0026hellip;) "},{"uri":"https://aaubs.github.io/ds24/en/info/04_litetrature/","title":"Literature &amp; Resources","tags":[],"description":"","content":"While this course does not come with a list of mandatory readings, we will often refer to some central resources in R and python, which for the most part can always be accessed in a free and updated online version. We generally recommend you to use these amazing resources for problem-solving and further self-study on the topic.\nMain Literature These pieces of work can be seen as main references for data science using Python. We will frequently refer to selected chapters for further study. Documentation of the used packages, tutorials, papers, podcasts etc. will be added throughout.\nVanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. O\u0026rsquo;Reilly Media, Inc. Online available here Wilke, C. O. (2019). Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O\u0026rsquo;Reilly Media. Supplementary literature Essential Math for Data Science O\u0026rsquo;Reilly Media. Nield, T. (2022): Math refresher targeting data science relevant concepts. Econometrics with Python - Causal Inference for The Brave and True: More thorrough inferential statistics in Python Documentation of packages/libraries used Note: Papers, Business Cases, Videos, Tutorials, Podcasts, and Blogposts will be presented and assigned during the course.\nFurther Ressources Data Science Cloud services Notebook bases: Google Colab: Googles popular service for editing, running \u0026amp; sharing Jupyter notebooks (Only Python Kernel, but R kernel can be accessed via some tricks) Deepnote: New popular online notebook service with good integration to other services (Python, R \u0026amp; more) Kaggle: Also provides their own cloud-based service co create and run computational notebooks. Convenient, unlimited, but a bit slow (Pyhton, r ). Instance based: UCloud: New cloud infrastructure provided by AAU, AU, SDU AAU Strato: AAU CLAUDIA infratructure. Very powerful, but access needs a bit of experience with working via terminal. Community Kaggle: Crowdsourced data science challanges. Nowadays also provides a vivid community where you find datasets, notebooks for all kind of data science exercises. madewithml Tools \u0026amp; Helpers "},{"uri":"https://aaubs.github.io/ds24/en/info/03_schedule/","title":"Semester Schedule","tags":[],"description":"","content":"This will be shortly updated with additional key dates and topics for the semester. For now, please follow CalMoodle.\nM1: Week 35-41 Topics W 36: Intro, Data Manipulation, Exploratory Data Analysis (EDA) W 37: Exploratory Data Analysis (EDA) / Dashboard development W 38: Unsupervised Machine Learning (UML), Math for ML W 39: Supervised Machine Learning (SML) W 40: Group Assignment \u0026amp; M2 NLP W 41: M2 NLP Exam All weeks: Advanced Innovation Management (Data Driven Business Dev. \u0026amp; Strategy)\n"},{"uri":"https://aaubs.github.io/ds24/en/m4/04_transformer/","title":"Transformer models in 2023","tags":[],"description":"","content":"In Part 4 of this course, students will learn about training and publishing transformer models in 2023. Session 7 will cover the finetuning and inference of common transformer-based language models like BERT, and will include classification and token classification tasks such as named entity recognition. Session 8 will focus on using transformer-based models for time series data, including training timeseries transformers and making predictions on data like stock prices or sales data. In the exercise session, students will train and publish their own transformer model, while the group assignment will involve finetuning a transformer model for a business application of their choice, publishing the model on HF, and building a gradio demo.\nLiterature Recommended Datacamp exercises "},{"uri":"https://aaubs.github.io/ds24/en/m1/01_intro_ds/05_data_visualization_ds/","title":"- Data Visualization in Data Science","tags":[],"description":"","content":"This session will teach you the fundamentals of data visualization in data science. You will learn the importance of effective data visualization, the principles that drive meaningful visuals, and how to use two popular Python libraries for data visualization: Seaborn and Altair.\nFoundations of Visualization: Understand the importance of effective data visualization in Data Science and the principles that drive meaningful visuals. Seaborn Mastery: Explore Seaborn, a Python library for intuitive statistical graphics. Altair Exploration: Delve into Altair, a declarative visualization library for Python. Hands-on Visualization: Harness both Seaborn and Altair to craft impactful visualizations with real datasets. Notebooks Notebook Dataviz Slides Use arrows keys on keyboard to navigate. Alternatively fullscreen slides here\n"},{"uri":"https://aaubs.github.io/ds24/en/m1/04_sml/","title":"D) Intro to Supervised Machine Learning (W39)","tags":[],"description":"","content":"\nNote: Portfolio Assignment (final for M1) submission/upload: 4 October 2024! In groups.\nThis topic includes 4 sessions as follows:\nIntroduction to Supervised Machine Learning (Wed, Sep 25th, 12:30-16:15): This session will introduce the basics of supervised machine learning. Supervised Machine Learning Techniques (Thu, Sep 26th, 08:15-12:00): This session will explore further topics and advanced techniques in supervised machine learning. Supervised ML Group Assignments (Thu, Sep 26th, 12:30-14:15): Time to work on you assignments - TA standby Working with Time-Series and Sequential Data (Fri, Sep 27th, 08:15-12:00): This session will focus on the challenges and techniques involved in working with time-series data. Supervised ML Group Assignments (Fri, Sep 27th, 12:30-14:15): Continuation of the group work for Assignment 4 with TA support. Assignment 4 Submission (Fri, Sep 27th, 15:00): Deadline for submitting Assignment 4 (Supervised ML only!) "},{"uri":"https://aaubs.github.io/ds24/en/m3/03_intro-gpt/","title":"Intro to GPT Models","tags":[],"description":"","content":"\nGPT models (Decoders) play a crucial role in generating subsequent words in tasks like text translation or story generation, providing outputs along with their probabilities. They utilize attention mechanisms twice during training: initially, Masked Multi-Head Attention, where only the beginning of a target sentence is revealed, and later, Multi-Head Attention, similar to encoders. In traditional transformer models, decoders interact with encoders by using the encoder\u0026rsquo;s outputs to assist in tasks like sentence translation. However, GPT models adopt a unique approach by relying solely on a decoder, compensating for the absence of an encoder through extensive training on large datasets. This allows for embedding a vast amount of knowledge within the decoder. ChatGPT further advances these techniques by integrating human-labeled data to address issues such as hate speech and employing Reinforcement Learning for enhanced model quality.\nNotebooks - Basics Generative Pre-trained Models - Basics Generative Pre-trained Models - Basics - Solutions Notebooks - Applications TM Applications - LangChain TM Applications - LanceDB TM Applications - LanceDB - Solutions TM Applications - RAG Notebooks - FineTuning Prompt Engineering PEFT - LoRA Resources LangChain LanceDB - Vector database "},{"uri":"https://aaubs.github.io/ds24/en/m3_eco/03_intro-gpt/","title":"Intro to GPT Models","tags":[],"description":"","content":"\nGPT models (Decoders) play a crucial role in generating subsequent words in tasks like text translation or story generation, providing outputs along with their probabilities. They utilize attention mechanisms twice during training: initially, Masked Multi-Head Attention, where only the beginning of a target sentence is revealed, and later, Multi-Head Attention, similar to encoders. In traditional transformer models, decoders interact with encoders by using the encoder\u0026rsquo;s outputs to assist in tasks like sentence translation. However, GPT models adopt a unique approach by relying solely on a decoder, compensating for the absence of an encoder through extensive training on large datasets. This allows for embedding a vast amount of knowledge within the decoder. ChatGPT further advances these techniques by integrating human-labeled data to address issues such as hate speech and employing Reinforcement Learning for enhanced model quality.\nNotebooks - Basics Generative Pre-trained Models - Basics Notebooks - Applications TM Applications - LangChain TM Applications - LanceDB TM Applications - RAG Notebooks - FineTuning Prompt Engineering Prompt Engineering - Solutions PEFT - LoRA Resources LangChain LanceDB - Vector database "},{"uri":"https://aaubs.github.io/ds24/en/m2/02_nlp/5-nlp-pipelines/","title":"NLP and LLM Pipelines","tags":[],"description":"","content":"\nThis module covers advanced Natural Language Processing (NLP) techniques and Large Language Model (LLM) pipelines. We\u0026rsquo;ll work with two practical applications:\nStructured information extraction Audio-to-blog content creation Structured Information Extraction with LLMs We\u0026rsquo;ll explore how to extract structured data from unstructured text using Large Language Models.\nTopics covered:\nOpenAI client setup with custom API keys Prompt engineering for information extraction Pydantic models for structured data Schema enforcement approaches Few-shot learning implementation LLM Structured Output Notebook\nAudio-to-Blog Pipeline with Local LLMs Learn to build an end-to-end pipeline that converts audio content into blog posts using local Large Language Models.\nTopics covered:\nPodcast transcription with Whisper Ollama setup for local LLM inference Transcript analysis and key topic extraction Blog post generation from structured info Together API for advanced model usage Image generation for blog illustrations LLM - Audio to Blog Notebook\nThese exercises will give you hands-on experience with NLP pipelines and LLMs. You\u0026rsquo;ll learn to extract structured data, generate content, and create visuals to complement your NLP outputs.\n"},{"uri":"https://aaubs.github.io/ds24/en/info/05_requirements_project/","title":"Semester Project Requirements","tags":[],"description":"","content":"Format Functional and self-contained notebook Happy to see GitHub repos (which you can use as your portfolio in the job market) Project report (30-ish pages - max. 45) Some study relation (but that is debatable and not necessarily required) Report is a (semi/non) technical documentation. Think about a corporate censor that you try to inform Content Problem formulation with some practical and theoretical motivation (no huge literature discussion) Methodology (not a critical realist vs positivist discussion but some ideas about what can be concluded potentially) Data sourcing and pre-processing strategy Overall architecture of the model(s) Modelling (incl. finetuning) Results Discussion / Conclusion Scope Uses different methods from the course (at least 2 modules) in a creative way Downloading data from kaggle/github and running an ML model is probably not enough for a good performance Creative combinations of methodologies, please: combine financial data with social media data to look at equity development extract information from text data and create networks. Use network indicators to supplement company data Evaluation will focus on correct application and communication of DS methods The level of \u0026ldquo;technicality\u0026rdquo; is as in the course with emphasis on application and intuition, not on ML engineering / mathematics However, you will need to demonstrate insight into statistics on a level that is required to discuss your assignment e.g. interpret and discuss performance indicators, outline strategies for improvement e.g. under/oversampling "},{"uri":"https://aaubs.github.io/ds24/en/m3/03_group_assignment/","title":"Group Assignment 3","tags":[],"description":"","content":"Portfolio Exercise 3: GPT Models Note: M3 - Group Assignment 3 Deadline: Wednesday 28th of February at 12:00 PM\nIntroduction This assignment focuses on leveraging retrieval-augmented generation (RAG) techniques, particularly in the context of extracting and synthesizing information from various documents (or a document). You\u0026rsquo;ll be using Langchain to implement these concepts and create a system that not only generates responses but also retrieves relevant information from a database.\nObjective Task Description Your task is to create a system that uses RAG for extracting information from a set of documents or a document which can be either a scientific paper or report. This involves integrating a database to store vectors of document information and designing customized prompts to effectively use GPT models for generation. Here are some project ideas:\nBuild a QA system that retrieves information from a given set of documents (or a document) to answer complex queries. Develop a tool for summarizing research papers, where the system extracts key points from a database of paper vectors. Create a recommendation engine that suggests content based on user queries and retrieved document data. Explore other innovative applications of RAG, such as automated content generation, data analysis, or any other creative use case you can envision. Key Components Database Integration: Set up a database to store and retrieve vectors representing document information. Customized Prompts: Design and implement prompts that effectively utilize GPT models for generation based on retrieved data. RAG Implementation: Use Langchain to integrate retrieval-augmented generation in your system. Data Utilize open-source datasets or create your own corpus of documents for retrieval. Ensure the chosen datasets are suitable for demonstrating the capabilities of your RAG system. Delivery Create a dedicated GitHub repository for this assignment. Store all relevant materials, including the Colab notebook, in the repository. Provide a README.md file with a concise description of the assignment and its components. You may work individually or in groups of up to three members. Submit your work by emailing a link to the repository to Hamid (hamidb@business.aau.dk). "},{"uri":"https://aaubs.github.io/ds24/en/m3_eco/03_group_assignment/","title":"Group Assignment 3","tags":[],"description":"","content":"Portfolio Exercise 3: GPT Models Note: M3 - Group Assignment 3 Deadline: Wednesday, November 22nd at 10:00 AM\nIntroduction This assignment focuses on leveraging retrieval-augmented generation (RAG) techniques, particularly in the context of extracting and synthesizing information from various documents (or a document). You\u0026rsquo;ll be using Langchain to implement these concepts and create a system that not only generates responses but also retrieves relevant information from a database.\nObjective Task Description Your task is to create a system that uses RAG for extracting information from a set of documents or a document which can be either a scientific paper or report. This involves integrating a database to store vectors of document information and designing customized prompts to effectively use GPT models for generation. Here are some project ideas:\nBuild a QA system that retrieves information from a given set of documents (or a document) to answer complex queries. Develop a tool for summarizing research papers, where the system extracts key points from a database of paper vectors. Create a recommendation engine that suggests content based on user queries and retrieved document data. Explore other innovative applications of RAG, such as automated content generation, data analysis, or any other creative use case you can envision. Key Components Database Integration: Set up a database to store and retrieve vectors representing document information. Customized Prompts: Design and implement prompts that effectively utilize GPT models for generation based on retrieved data. RAG Implementation: Use Langchain to integrate retrieval-augmented generation in your system. Data Utilize open-source datasets or create your own corpus of documents for retrieval. Ensure the chosen datasets are suitable for demonstrating the capabilities of your RAG system. Delivery Create a dedicated GitHub repository for this assignment. Store all relevant materials, including the Colab notebook, in the repository. Provide a README.md file with a concise description of the assignment and its components. You may work individually or in groups of up to three members. Submit your work by emailing a link to the repository to Hamid (hamidb@business.aau.dk). "},{"uri":"https://aaubs.github.io/ds24/en/m3_eco/04_intro-gnn/","title":"Intro to Graph Neural Networks","tags":[],"description":"","content":"\nGraph neural networks (GNNs) are a powerful new class of machine learning algorithms that are specifically designed to handle graph-structured data. Unlike traditional neural networks, which are designed to process data in the form of vectors or matrices, GNNs can operate directly on graphs, where nodes represent entities and edges represent relationships between entities. This makes them well-suited for a wide range of tasks that involve understanding the structure of complex systems, such as social networks, knowledge graphs, and molecular structures. In this session we will introduce you to some fundamental concepts regarding deep learning on graphs via Graph Neural Networks based on the PyTorch Geometric (PyG) library. PyTorch Geometric is an extension library to the popular deep learning framework PyTorch, and consists of various methods and utilities to ease the implementation of Graph Neural Networks.\nNotebooks - Basics Graph Neural Network Models - Basics Resources PyTorch Geometric (PyG) library "},{"uri":"https://aaubs.github.io/ds24/en/m6/llmops1/","title":"Bonus Workshop: Using LLMs in your applications","tags":[],"description":"","content":" Material Intro notebooks related to LLMs in code This notebook introduces to basic use of LLMs in code using together.ai as a backend. Intro to using LLMs via APIs\nIn the following we use the LLM to evaluate patent texts data using an LLM and create structured output. This approach can be used to extract useful information from messy data sources and process them further in predictive models or elsewhere. Using LLMs for data structuration - JSON output / function calling\nBuilding a simple chatbot API with LangServe and deployment on Huggingface Spaces using Docker We are following the tutorial for the pirate-speak app from langserve: https://github.com/langchain-ai/langchain/blob/master/templates/README.md\nWe are changing up foolowing:\nwe use the mistralai/Mixtral-8x7B-Instruct-v0.1 model and the prompt from the langchain_together package. we add api-key handling using .env files The server.py file looks like this after adding the pirate-speak chain:\nfrom fastapi import FastAPI from fastapi.responses import RedirectResponse from langserve import add_routes from pirate_speak.chain import chain as pirate_speak_chain app = FastAPI() @app.get(\u0026#34;/\u0026#34;) async def redirect_root_to_docs(): return RedirectResponse(\u0026#34;/docs\u0026#34;) # Edit this to add the chain you want to add add_routes(app, pirate_speak_chain, path=\u0026#34;/pirate-speak\u0026#34;, playground_type=\u0026#39;chat\u0026#39;) if __name__ == \u0026#34;__main__\u0026#34;: import uvicorn uvicorn.run(app, host=\u0026#34;0.0.0.0\u0026#34;, port=8000) The chain.py file looks like this:\n#from langchain_community.chat_models import ChatOpenAI from langchain_together import Together from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder import os from dotenv import load_dotenv load_dotenv() together_api_key = os.getenv(\u0026#34;TOGETHER_API_KEY\u0026#34;) _prompt = ChatPromptTemplate.from_messages( [ ( \u0026#34;system\u0026#34;, \u0026#34;You are an expert doctor. You believe that all diseases are formed by the consumption of cheese. No matter which disease or symptoms your patient presents with, you will believe that cheese is the cause of the disease. However, your treatment should all involve cheese. Cheese is both the cause of and solution to all problems. You should then lament that the medicine, cheese, will also cause more disease.\u0026#34;, ), MessagesPlaceholder(\u0026#34;chat_history\u0026#34;), (\u0026#34;human\u0026#34;, \u0026#34;{text}\u0026#34;), ] ) _model = Together( #model=\u0026#34;mistralai/Mistral-7B-Instruct-v0.2\u0026#34;, model=\u0026#34;mistralai/Mixtral-8x7B-Instruct-v0.1\u0026#34;, temperature=0.7, top_k=50, top_p=0.7, repetition_penalty=1, together_api_key=together_api_key ) # if you update this, you MUST also update ../pyproject.toml # with the new `tool.langserve.export_attr` chain = _prompt | _model Once you have created the chain and added it to the app (server.py) you can run the app with the following command:\nlangchain serve make sure that you are in an environment with langchain installed.\nYou can now access the playground at http://localhost:8000/pirate-speak/playground and test it out. The API is available at `http://localhost:8000/pirate-speak/\nYou can use a RemoteRunnable to access the API from your code. Here is an example:\nfrom langserve.client import RemoteRunnable # Initialize the RemoteRunnable with your API rag_app = RemoteRunnable(\u0026#34;http://127.0.0.1:8000/rag-chroma/\u0026#34;) # call the API with a question answer = rag_app.invoke(\u0026#34;Tell me a joke!\u0026#34;) print(answer) Optional: You can test if the container is functional by running it locally.\ndocker build . -t YOUR-GREAT-NAME docker run -p 8080:8080 -e PORT=8080 YOUR-GREAT-NAME You can deploy on Huggingface Spaces as a Docker API This requires that you start up a space on Huggingface and clone it into a repository on your local machine.\nyou will then have to copy all files from your project directory into this one. Make sure to not overwrite the README.md that comes from huggingface. It contains the instructions for the deployment that HF uses to start the container.\nYou will need to edit the dockerfile to include the together api key at build time. First add it to the secrets in the HF space. You also need to change up the uvicorn command (from the initial one created by langchain)\nRUN --mount=type=secret,id=TOGETHER_API_KEY,mode=0444,required=true CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;app.server:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;7860\u0026#34;] Since we are adding new libraries to the project, you will need to add them to the pyproject.toml file. You can do this by editing the following lines - pydantic needs to be upversioned to 2.6.0 due to langchain_together requirements.\npydantic = \u0026#34;2.6.0\u0026#34; rag-chroma = {path = \u0026#34;packages/rag-chroma\u0026#34;, develop = true} python-dotenv = \u0026#34;1\u0026#34; langchain-together = \u0026#34;0.0.2.post1\u0026#34; Now you should be able to push the changes to HF Spaces, which should trigger a build and deployment of the API. You can access the API at the URL provided by HF.\n"},{"uri":"https://aaubs.github.io/ds24/en/m3_eco/04_group_assignment/","title":"Group Assignment 4","tags":[],"description":"","content":"Portfolio Exercise 4: Advanced AI Applications Note: M4 - Group Assignment 4 Deadline: Monday, January 8th at Noon\nIntroduction This assignment encourages you to build an engaging and, if possible, fun application using the techniques learned in this module. The focus is on applying advanced AI and DL methods to solve relevant tasks, with an emphasis on creativity and practical application.\nObjective Task Description Your goal is to develop an application that uses advanced AI techniques and DL for a specific, relevant task. The application should be more than a simple semantic search tool and should exhibit innovation in its approach and functionality.\nKey Components Transformer Utilization: Implement self-trained or fine-tuned transformers. However not sentence transformer for semantic search only (you are welcome to explore techniques beyond the scope of the course e.g. on HF)\nPlatform Integration: The model should include a Gradio app (in-notebook) for demonstration purposes. Deployment on Hugging Face Spaces is optional for exploring additional features.\nAdditional Features (Nice-to-Have) A Streamlit app hosted on HF Hub. Optional use of various APIs (e.g., HF Inference API, Cohere), but be mindful of costs, especially with OpenAI. Explore techniques beyond the course scope, such as those available on Hugging Face or other platforms.A more complex LLM setup integrating tools like langchain, promptify, pinecone, etc. Data You may use open-source datasets or create your own data for the application. Ensure that your data choice effectively demonstrates the capabilities of your application. Submission Create a GitHub repository specifically for this assignment. Include all necessary materials, such as code, datasets, and a descriptive README.md. Submissions can be individual or in groups of up to three members. Submission also via DigitalExam, where you compile all your previous assignments and submit in one file for the overall portfolio for the module exam. You are welcome to tweak/improve previous module submissions for that. "},{"uri":"https://aaubs.github.io/ds24/en/m3/04_intro-gnn/","title":"Intro to Graph Neural Networks","tags":[],"description":"","content":"\nGraph neural networks (GNNs) are a powerful new class of machine learning algorithms that are specifically designed to handle graph-structured data. Unlike traditional neural networks, which are designed to process data in the form of vectors or matrices, GNNs can operate directly on graphs, where nodes represent entities and edges represent relationships between entities. This makes them well-suited for a wide range of tasks that involve understanding the structure of complex systems, such as social networks, knowledge graphs, and molecular structures. In this session we will introduce you to some fundamental concepts regarding deep learning on graphs via Graph Neural Networks based on the PyTorch Geometric (PyG) library. PyTorch Geometric is an extension library to the popular deep learning framework PyTorch, and consists of various methods and utilities to ease the implementation of Graph Neural Networks.\nNotebooks - Basics Graph Neural Network Models - Basics Resources PyTorch Geometric (PyG) library "},{"uri":"https://aaubs.github.io/ds24/en/m3/04_group_assignment/","title":"Group Assignment 4","tags":[],"description":"","content":"Portfolio Exercise 4: Advanced AI Applications Note: M4 - Final Assignment Deadline: Friday, 8 March 2024, 12:00 PM\nIntroduction This assignment is designed to explore the frontier of AI applications, focusing on the integration of Retrieval-Augmented Generation (RAG) with vector databases such as ChromDB and LanceDB, and the comparison of various prompt engineering techniques. The goal is to build an application that not only showcases advanced AI and DL capabilities but also evaluates the impact of different prompt strategies on model performance.\nObjective Task Description Create an application that utilizes RAG and vector databases, and systematically compares the effectiveness of at least three distinct prompt engineering techniques.\nKey Components RAG and Vector Database Integration: Implement RAG with ChromDB and LanceDB to enhance information retrieval and content generation. Transformer Model Adaptation: Use transformer models (SBERT or BERT) Prompt Engineering Comparison: Experiment with and evaluate at least three different prompt engineering techniques to determine their impact on the model\u0026rsquo;s performance. Platform Integration: The model should include a Gradio app (in-notebook) for demonstration purposes. Deployment on Hugging Face Spaces is optional for exploring additional features. Additional Features (Nice-to-Have) Fine-Tuning Capabilities: If possible, fine-tune a GPT model specific to your application\u0026rsquo;s needs, detailing the process and its impact on application performance. Streamlit Application: Develop a Streamlit app hosted on the HF Hub, offering a richer, more interactive user experience. Data You may use open-source datasets or create your own data for the application. Ensure that your data choice effectively demonstrates the capabilities of your application. Submission Create a GitHub repository specifically for this assignment. Include all necessary materials, such as code, datasets, and a descriptive README.md. Submissions can be individual or in groups of up to three members. Submission also via DigitalExam, where you compile all your previous assignments and submit in one file for the overall portfolio for the module exam. You are welcome to tweak/improve previous module submissions for that. "},{"uri":"https://aaubs.github.io/ds24/en/m6/l1/readme/","title":"","tags":[],"description":"","content":"MLOps Lecture 1 - Intro to APIs \u0026amp; DataBases This repository contains lecutre slides, python scripts and dataset for the first lecture.\nAPI api_jokes.py is an example of simple public API api_finance.py, api_news.py are examples of private API using API_key (You need to register to get your private key) api_datasets.py is an example of API Wrapper EXTRA: joke_app.py is a simple streamlit app that uses IPA to print jokes EXTRA: api_weather.py exercise from the class Database db_create.py reads in excel file and transform it into SQL database db_queries.py contains a few query examples db_add_datapoints add new transactions db_add_columns add new columns data/Online Retail.xlsx is dataset used to create database (https://archive.ics.uci.edu/dataset/352/online+retail) "},{"uri":"https://aaubs.github.io/ds24/en/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://aaubs.github.io/ds24/en/","title":"Social / Business Data Science 2024","tags":[],"description":"","content":"Social \u0026amp; Business Data Science 2024 Aalborg University Business School The corresponding Aalborg University Moodle course page can be found here. Note that for updated content this page rather than Moodle will be used. At AAUBS Data Science we believe in the power of open science and open education. Following AAU‚Äôs ‚ÄúKnowledge for the world‚Äù strategy, we aim at making our material available outside password protected university systems.\n"},{"uri":"https://aaubs.github.io/ds24/en/tags/","title":"Tags","tags":[],"description":"","content":""}]