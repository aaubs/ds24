<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Applied Deep Learning and Artificial Intelligence on Social / Business Data Science 2024</title><link>https://aaubs.github.io/ds24/en/m3/</link><description>Recent content in Applied Deep Learning and Artificial Intelligence on Social / Business Data Science 2024</description><generator>Hugo</generator><language>en-US</language><atom:link href="https://aaubs.github.io/ds24/en/m3/index.xml" rel="self" type="application/rss+xml"/><item><title>Intro to Traditional Deep Learning</title><link>https://aaubs.github.io/ds24/en/m3/01_intro-to-traditional-dl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds24/en/m3/01_intro-to-traditional-dl/</guid><description>This session provides an overview of the foundational elements of deep learning, including its historical context, key concepts, and practical applications. The course will delve into various types of neural networks, outlining their advantages and disadvantages. It will specifically focus on convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory (LSTM) networks, highlighting their unique characteristics and applicability to a range of problem-solving scenarios, including those in economics.</description></item><item><title>Group Assignment 1</title><link>https://aaubs.github.io/ds24/en/m3/01_group_assignment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds24/en/m3/01_group_assignment/</guid><description>Portfolio Exercise 1 Note: M3 - Group Assignment 1 Deadline: Wednesday 7th of February at 12.00 PM
Introduction In this assignment, you are required to delve into the practical aspects of Deep Learning by constructing and evaluating a neural network using PyTorch. This exercise is designed to deepen your understanding of neural network architectures, hyperparameter tuning, and the preprocessing steps necessary for effective model training and evaluation. You will have the freedom to choose a dataset from either the M1 or M2 module or select an external dataset that intrigues you.</description></item><item><title>Intro to Transformer Models</title><link>https://aaubs.github.io/ds24/en/m3/02_intro-tm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds24/en/m3/02_intro-tm/</guid><description>Literature Sutskever, I., Vinyals, O., &amp;amp;amp; Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., &amp;amp;hellip; &amp;amp;amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.
The illustrated transformer
Simple transformer LM
Notebooks - Basics Transformer Models - Basics Notebooks - Applications TM Applications - SBERT TM Applications - HF Simple transformer LM SBERT for Patent Search using PatentSBERTa in PyTorch Notebooks - FineTuning TM FineTuning - SimpleTransformers TM FineTuning - SBERT TM FineTuning - HF SetFit Hatespeech vs bert and distilroberta Seq2Seq - Neural Machine Translation Slides - Attention Mechanism Slides - SBERT Classification with various vectorization approaches TF-IDF and W2V Multi-Class Text Classification BERT Multi-Class Text Classification Implementing Multi-Class Text Classification LSTMs using PyTorch Resources OG SBERT-Paper Reimers, N.</description></item><item><title>Group assignment 2</title><link>https://aaubs.github.io/ds24/en/m3/02_group_assignment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds24/en/m3/02_group_assignment/</guid><description>Portfolio Exercise 2: Transformer Models Note: M3 - Group Assignment 2 Deadline: Wednesday, February 14th at 10:00 AM
Introduction This exercise is designed to deepen your understanding and skills in modern deep learning techniques. We have two main tasks for you. The first is focused on using SBERT for semantic search, and the second involves hands-on exercises with gradient descent and the attention mechanism.
Part 1: SBERT and Semantic Search Task Description Create something innovative using SBERT and semantic search, or even more!</description></item><item><title>Intro to GPT Models</title><link>https://aaubs.github.io/ds24/en/m3/03_intro-gpt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds24/en/m3/03_intro-gpt/</guid><description>GPT models (Decoders) play a crucial role in generating subsequent words in tasks like text translation or story generation, providing outputs along with their probabilities. They utilize attention mechanisms twice during training: initially, Masked Multi-Head Attention, where only the beginning of a target sentence is revealed, and later, Multi-Head Attention, similar to encoders. In traditional transformer models, decoders interact with encoders by using the encoder&amp;rsquo;s outputs to assist in tasks like sentence translation.</description></item><item><title>Group Assignment 3</title><link>https://aaubs.github.io/ds24/en/m3/03_group_assignment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds24/en/m3/03_group_assignment/</guid><description>Portfolio Exercise 3: GPT Models Note: M3 - Group Assignment 3 Deadline: Wednesday 28th of February at 12:00 PM
Introduction This assignment focuses on leveraging retrieval-augmented generation (RAG) techniques, particularly in the context of extracting and synthesizing information from various documents (or a document). You&amp;rsquo;ll be using Langchain to implement these concepts and create a system that not only generates responses but also retrieves relevant information from a database.
Objective Task Description Your task is to create a system that uses RAG for extracting information from a set of documents or a document which can be either a scientific paper or report.</description></item><item><title>Intro to Graph Neural Networks</title><link>https://aaubs.github.io/ds24/en/m3/04_intro-gnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds24/en/m3/04_intro-gnn/</guid><description>Graph neural networks (GNNs) are a powerful new class of machine learning algorithms that are specifically designed to handle graph-structured data. Unlike traditional neural networks, which are designed to process data in the form of vectors or matrices, GNNs can operate directly on graphs, where nodes represent entities and edges represent relationships between entities. This makes them well-suited for a wide range of tasks that involve understanding the structure of complex systems, such as social networks, knowledge graphs, and molecular structures.</description></item><item><title>Group Assignment 4</title><link>https://aaubs.github.io/ds24/en/m3/04_group_assignment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds24/en/m3/04_group_assignment/</guid><description>Portfolio Exercise 4: Advanced AI Applications Note: M4 - Final Assignment Deadline: Friday, 8 March 2024, 12:00 PM
Introduction This assignment is designed to explore the frontier of AI applications, focusing on the integration of Retrieval-Augmented Generation (RAG) with vector databases such as ChromDB and LanceDB, and the comparison of various prompt engineering techniques. The goal is to build an application that not only showcases advanced AI and DL capabilities but also evaluates the impact of different prompt strategies on model performance.</description></item></channel></rss>