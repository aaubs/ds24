<!doctype html><html lang=en class="js csstransforms3d"><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.135.0"><meta name=description content="Social / Business Data Science 2024"><meta name=author content="Roman Jurowetzki & Daniel Hain"><link rel=icon href=/ds24/images/favicon.png type=image/png><title>Basics of NLP :: Social / Business Data Science 2024</title>
<link href=/ds24/css/nucleus.css?1728934061 rel=stylesheet><link href=/ds24/css/fontawesome-all.min.css?1728934061 rel=stylesheet><link href=/ds24/css/hybrid.css?1728934061 rel=stylesheet><link href=/ds24/css/featherlight.min.css?1728934061 rel=stylesheet><link href=/ds24/css/perfect-scrollbar.min.css?1728934061 rel=stylesheet><link href=/ds24/css/auto-complete.css?1728934061 rel=stylesheet><link href=/ds24/css/atom-one-dark-reasonable.css?1728934061 rel=stylesheet><link href=/ds24/css/theme.css?1728934061 rel=stylesheet><link href=/ds24/css/tabs.css?1728934061 rel=stylesheet><link href=/ds24/css/hugo-theme.css?1728934061 rel=stylesheet><script src=/ds24/js/jquery-3.3.1.min.js?1728934061></script><style>:root #header+#content>#left>#rlblock_left{display:none!important}</style></head><body data-url=/ds24/en/m2/02_nlp/1-nlb-intro/><nav id=sidebar class=showVisitedLinks><div id=header-wrapper><div id=header><a href=https://aaubs.github.io/ds24/><img alt=logo src=/ds24/images/logo.png>
</a>AAUBS Data Science 2024</div><div class=searchbox><label for=search-by><i class="fas fa-search"></i></label>
<input data-search-input id=search-by type=search placeholder=Search...>
<span data-search-clear><i class="fas fa-times"></i></span></div><script type=text/javascript src=/ds24/js/lunr.min.js?1728934061></script><script type=text/javascript src=/ds24/js/auto-complete.js?1728934061></script><script type=text/javascript>var baseurl="https://aaubs.github.io/ds24/"</script><script type=text/javascript src=/ds24/js/search.js?1728934061></script></div><div class=highlightable><ul class=topics><li data-nav-id=/ds24/en/info/ title="Info, Schedule & Co" class=dd-item><a href=/ds24/en/info/>Info, Schedule & Co
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/ds24/en/info/02_modules/ title=Modules class=dd-item><a href=/ds24/en/info/02_modules/>Modules
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/info/04_litetrature/ title="Literature & Resources" class=dd-item><a href=/ds24/en/info/04_litetrature/>Literature & Resources
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/info/03_schedule/ title="Semester Schedule" class=dd-item><a href=/ds24/en/info/03_schedule/>Semester Schedule
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/info/05_requirements_project/ title="Semester Project Requirements" class=dd-item><a href=/ds24/en/info/05_requirements_project/>Semester Project Requirements
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/ds24/en/m1/ title="Applied Data Science and Machine Learning" class=dd-item><a href=/ds24/en/m1/><b>1. </b>Applied Data Science and Machine Learning
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/ds24/en/m1/01_intro_ds/ title="A) Introduction to Data Science (W35-36)" class=dd-item><a href=/ds24/en/m1/01_intro_ds/>A) Introduction to Data Science (W35-36)
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/ds24/en/m1/01_intro_ds/01_welcome_ds/ title="- Welcome Students!" class=dd-item><a href=/ds24/en/m1/01_intro_ds/01_welcome_ds/>- Welcome Students!
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/m1/01_intro_ds/02_data_handeling/ title="- Data Handling and Manipulation" class=dd-item><a href=/ds24/en/m1/01_intro_ds/02_data_handeling/>- Data Handling and Manipulation
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/m1/01_intro_ds/03_data_visualization_stat/ title="- Exploratory Data Analysis and Essential Statistics" class=dd-item><a href=/ds24/en/m1/01_intro_ds/03_data_visualization_stat/>- Exploratory Data Analysis and Essential Statistics
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/m1/01_intro_ds/05_data_visualization_ds/ title="- Data Visualization in Data Science" class=dd-item><a href=/ds24/en/m1/01_intro_ds/05_data_visualization_ds/>- Data Visualization in Data Science
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/ds24/en/m1/02_rapid_prototyping/ title="B) Rapid Prototyping (W37)" class=dd-item><a href=/ds24/en/m1/02_rapid_prototyping/>B) Rapid Prototyping (W37)
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/ds24/en/m1/02_rapid_prototyping/01_rapid_prototyping/ title="Introduction to Streamlit: Building an Employee Attrition Dashboard" class=dd-item><a href=/ds24/en/m1/02_rapid_prototyping/01_rapid_prototyping/>Introduction to Streamlit: Building an Employee Attrition Dashboard
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/m1/02_rapid_prototyping/02_online_dashboard/ title=GeoPandas class=dd-item><a href=/ds24/en/m1/02_rapid_prototyping/02_online_dashboard/>GeoPandas
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/ds24/en/m1/03_uml/ title="C) Intro to Unsupervised Machine Learning (W38)" class=dd-item><a href=/ds24/en/m1/03_uml/>C) Intro to Unsupervised Machine Learning (W38)
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/ds24/en/m1/03_uml/01_intro_uml/ title="- Introduction to Unsupervised ML" class=dd-item><a href=/ds24/en/m1/03_uml/01_intro_uml/>- Introduction to Unsupervised ML
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/m1/03_uml/02_recommender_simsea_uml/ title="- Recommendation and Similarity Search" class=dd-item><a href=/ds24/en/m1/03_uml/02_recommender_simsea_uml/>- Recommendation and Similarity Search
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/m1/03_uml/04_intro_kmeans/ title="- Introduction to Clustering: K-means and Hierarchical Approaches" class=dd-item><a href=/ds24/en/m1/03_uml/04_intro_kmeans/>- Introduction to Clustering: K-means and Hierarchical Approaches
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/ds24/en/m1/04_sml/ title="D) Intro to Supervised Machine Learning (W39)" class=dd-item><a href=/ds24/en/m1/04_sml/>D) Intro to Supervised Machine Learning (W39)
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/ds24/en/m1/04_sml/01-sml-intro/ title="- Introduction to Supervised ML" class=dd-item><a href=/ds24/en/m1/04_sml/01-sml-intro/>- Introduction to Supervised ML
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/m1/04_sml/02-sml-addon/ title="- SML - Further topics" class=dd-item><a href=/ds24/en/m1/04_sml/02-sml-addon/>- SML - Further topics
<i class="fas fa-check read-icon"></i></a></li></ul></li></ul></li><li data-nav-id=/ds24/en/m2/ title="NLP & Network Analysis" class="dd-item
parent"><a href=/ds24/en/m2/><b>2. </b>NLP & Network Analysis
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/ds24/en/m2/02_nlp/ title="Natural Language Processing" class="dd-item
parent"><a href=/ds24/en/m2/02_nlp/>Natural Language Processing
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/ds24/en/m2/02_nlp/1-nlb-intro/ title="Basics of NLP" class="dd-item active"><a href=/ds24/en/m2/02_nlp/1-nlb-intro/>Basics of NLP
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/m2/02_nlp/2-seq2seq/ title="Seq2Seq & Transformers" class=dd-item><a href=/ds24/en/m2/02_nlp/2-seq2seq/>Seq2Seq & Transformers
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/m2/02_nlp/5-nlp-pipelines/ title="NLP and LLM Pipelines" class=dd-item><a href=/ds24/en/m2/02_nlp/5-nlp-pipelines/>NLP and LLM Pipelines
<i class="fas fa-check read-icon"></i></a></li></ul></li></ul></li></ul><section id=prefooter><hr><ul><li><a class=padding href=# data-clear-history-toggle><i class="fas fa-history fa-fw"></i> Clear History</a></li></ul></section><section id=footer><center><p>Built with <a href=https://github.com/matcornic/hugo-theme-learn><i class="fas fa-heart"></i></a> from <a href=https://getgrav.org>Grav</a> and <a href=https://gohugo.io/>Hugo</a></p></center><script async defer src=https://buttons.github.io/buttons.js></script></section></div></nav><section id=body><div id=overlay></div><div class="padding highlightable"><div><div id=top-bar><div id=breadcrumbs itemscope itemtype=http://data-vocabulary.org/Breadcrumb><span id=sidebar-toggle-span><a href=# id=sidebar-toggle data-sidebar-toggle><i class="fas fa-bars"></i>
</a></span><span class=links><a href=/ds24/en/>Social / Business Data Science 2024</a> > <a href=/ds24/en/m2/>NLP & Network Analysis</a> > <a href=/ds24/en/m2/02_nlp/>Natural Language Processing</a> > Basics of NLP</span></div></div></div><div id=head-tags></div><div id=body-inner><h1>Basics of NLP</h1><p><strong>NOTEBOOKS:</strong></p><ul><li><a href=https://colab.research.google.com/github/aaubs/ds-master/blob/main/notebooks/M2_Basic_Bow_with_rf_svm_xgboost.ipynb target=_blank>Bag Of Words</a></li><li><a href=https://colab.research.google.com/github/aaubs/ds-master/blob/main/notebooks/M2_Basic_Tfidf_financial_sentiment.ipynb target=_blank>TF-IDF</a></li></ul><p>============================================</p><h2 id=how-computer-understands-text>How &lsquo;computer&rsquo; understands text?</h2><p>============================================</p><p>Imagine someone who cannot hear and uses sign language to communicate. To understand them, you need to learn the specific signs they use to express words and ideas.</p><p>The signs represent the meaning of words, but they aren&rsquo;t the words themselves—just symbols that convey the message.</p><p>Similarly, computers don&rsquo;t understand human language directly. Instead, we translate text into a format they understand—like sign language for machines. This &lsquo;machine sign language&rsquo; <strong>is a numerical representation of text, called &lsquo;vectors&rsquo; or &rsquo;embeddings.</strong>&rsquo;</p><p>Each word, phrase, or sentence is converted into a series of numbers that capture its meaning.</p><div style=text-align:center><img src=https://raw.githubusercontent.com/aaubs/ds24/refs/heads/master/static/images/book.png alt="Book transforming into numbers" style=width:8cm></div><p><br><br><br><br></p><p>============================================</p><h2 id=problems-language-is-weird-1>Problems? Language is weird! [1]</h2><p>============================================</p><p>Something that is easily understood by humans is not always easily understood by computers—and vice versa! Placing additional word in to sentences can change the main message, but it doesn&rsquo; always convey the same message when converted into a numerical representation of text—and vice versa!</p><div style=text-align:center><img src=https://raw.githubusercontent.com/aaubs/ds24/refs/heads/master/static/images/only.jpg alt=Only style=width:8cm></div><p><br><br></p><p>Languages are full of synonims and words that have several meanings. Humans understand the correct meaning by knowing the context.</p><div style=text-align:center><img src=https://raw.githubusercontent.com/aaubs/ds24/refs/heads/master/static/images/wods.png alt=Only style=width:8cm></div><p><br><br></p><p>Punctuation marks can change the meaning of the sentence.</p><div style=text-align:center><img src=https://raw.githubusercontent.com/aaubs/ds24/refs/heads/master/static/images/comma.jpg alt=Only style=width:8cm></div><p><br><br><br><br></p><p>============================================</p><h2 id=level-of-analysis---one-big-text-corpus-vs-many-short-texts>Level of analysis - One Big Text (Corpus) vs Many Short Texts</h2><p>============================================</p><p><strong>The difference between analyzing a large corpus vs. multiple short texts</strong></p><ul><li><p><strong>One Big Text or Corpus of Texts</strong>:</p><ul><li>Focuses on extracting meaning, patterns, or structure from a long or cohesive text.</li><li>Examples: Books, academic articles, or a collection of documents.</li><li><strong>Example</strong>: Analyzing <strong>academic article abstracts</strong> for topic modeling or sentiment analysis.</li></ul></li><li><p><strong>Many Short Texts</strong>:</p><ul><li>Involves processing multiple, often disconnected, texts that are short and concise.</li><li>Examples: Social media posts, product reviews, or comments.</li><li><strong>Example</strong>: Processing <strong>Amazon reviews</strong> to determine customer sentiment or product popularity.</li></ul></li></ul><p><br><br></p><h3 id=what-are-we-analyzing-in-nlp>What Are We Analyzing in NLP?</h3><p>============================================</p><h4 id=1-text-as-such>1. <strong>Text as such</strong></h4><ul><li>Analyzing broad characteristics of the entire text or corpus.</li><li><strong>Examples</strong>:<ul><li><strong>Topics</strong>: Identifying themes or subjects within the text (e.g., topic modeling).</li><li><strong>Sentiment</strong>: Understanding the emotional tone (e.g., positive, negative, neutral).</li><li><strong>Language Modeling</strong>: Predicting the likelihood of text sequences.</li><li><strong>Similarity</strong>: Measuring how similar two texts or documents are (e.g., document similarity, semantic similarity).</li></ul></li></ul><h4 id=2-elements-within-text>2. <strong>Elements Within Text</strong></h4><ul><li>Analyzing smaller components or structures in the text.</li><li><strong>Examples</strong>:<ul><li><strong>Entities</strong>: Identifying named entities (e.g., people, organizations, locations) within the text.</li><li><strong>Relations</strong>: Understanding relationships between entities (e.g., person works at a company).</li><li><strong>N-grams</strong>: Extracting consecutive word sequences (e.g., bigrams, trigrams) to analyze co-occurrences or phrase structures.</li></ul></li></ul><p><br><br><br><br></p><p>==========================================</p><h2 id=from-simple-towards-more-complicated-methods>From simple towards more complicated methods</h2><p>==========================================</p><p>In the next few slides, we will look at different methods used in NLP:</p><h4 id=1-bag-of-words-bow>1. Bag of Words (BoW)</h4><h4 id=2-tf-idf>2. TF-IDF</h4><h4 id=3-word2vec>3. Word2Vec</h4><p><br><br><br><br></p><h2 id=bag-of-words-bow>Bag of Words (BoW)</h2><p>============================================</p><p><strong>What is it?</strong></p><ul><li>It is very simple and easy to use.<br></li><li>It uses the numerical representation of text that can be easily understood by machines.</li><li>It has a wide range of applications in many different fields like Natural Language Processing, Machine Learning etc.</li><li>It provides an efficient way for computers to understand human language.</li></ul><div style=text-align:center><img src=https://raw.githubusercontent.com/aaubs/ds24/refs/heads/master/static/images/bow2.png alt=Only style=height:6cm></div><table style=width:100%;border:0><tr><td style=width:40%;border:0><b>Steps:</b><br>1. Tokenize the input sentence into individual tokens (words).<br><br>2. Remove stop words from the tokenized list if required.<br><br>3. Create a dictionary of each unique word.<br><br>4. Represent each text as a vector of word counts or frequencies.<br></td><td style=width:60%;border:0><div style=text-align:center><img src=https://raw.githubusercontent.com/aaubs/ds24/refs/heads/master/static/images/bow3.png alt=Only style=hight:5cm></div></td></tr></table><p><br><br><br><br></p><h3 id=tf-idf-term-frequency---inverse-document-frequency>TF-IDF (Term Frequency - Inverse Document Frequency)</h3><p>============================================</p><p><strong>A statistical measure used to evaluate how important a word is to a document within a corpus.</strong></p><div style=text-align:center><img src=https://raw.githubusercontent.com/aaubs/ds24/refs/heads/master/static/images/tf-idf-formula.png alt=Only style=hight:4cm></div><br><br><br><br><p><strong>Term Frequency (TF)</strong>: Measures how often a word appears in a document.</p><p><strong>Inverse Document Frequency (IDF)</strong>: Measures how common or rare a word is across all documents.</p><p><strong>Advantages</strong>:</p><ul><li>Highlights important words specific to a document while down-weighting frequent terms</li><li>More informative than BoW for distinguishing relevant terms in large corpora.<br><br><br><br></li></ul><table style=width:100%;border:0><tr><td style=width:60%;border:0><b>Example</b><br>BoW: A word like "the" might be very frequent, but it's not important.<br><br>TF-IDF: Words like "science" or "research" in an article about technology would get a higher score because they are less frequent in general but more important for the specific document.<br></td><td style=width:40%;border:0><div style=text-align:center><img src=https://raw.githubusercontent.com/aaubs/ds24/refs/heads/master/static/images/cartoon.png alt=Only style=width:5cm></div></td></tr></table><p><br><br><br><br></p><h2 id=word2vec>Word2Vec</h2><p>============================================</p><p><strong>A deep learning-based model that transforms words into continuous, dense vectors (embeddings) that capture semantic meaning.</strong></p><div style=text-align:center><img src=https://raw.githubusercontent.com/aaubs/ds24/refs/heads/master/static/images/word2vec.png alt=Only style=width:12cm></div><p><strong>Advantages:</strong></p><ul><li>Captures word meaning and context.</li><li>Efficient for representing large vocabularies in a low-dimensional space.</li><li>Can perform word arithmetic (e.g., &ldquo;king&rdquo; - &ldquo;man&rdquo; + &ldquo;woman&rdquo; ≈ &ldquo;queen&rdquo;).</li></ul><p><strong>Limitations:</strong></p><ul><li>Requires large corpora for good performance.</li><li>Doesn’t directly handle out-of-vocabulary words.</li><li>Context is limited to a fixed window size.</li></ul><p><br><br><br><br></p><h2 id=comparison-word2vec-vs-tf-idf-vs-bow>Comparison: Word2Vec vs. TF-IDF vs. BoW</h2><p>============================================</p><table><thead><tr><th style=text-align:left><strong>Feature</strong></th><th style=text-align:left><strong>Word2Vec</strong></th><th style=text-align:left><strong>TF-IDF</strong></th><th style=text-align:left><strong>BoW</strong></th></tr></thead><tbody><tr><td style=text-align:left><strong>Representation</strong></td><td style=text-align:left>Dense vector embeddings</td><td style=text-align:left>Weighted word frequencies</td><td style=text-align:left>Word counts</td></tr><tr><td style=text-align:left><strong>Context Awareness</strong></td><td style=text-align:left>Yes, considers surrounding words</td><td style=text-align:left>No, treats words independently</td><td style=text-align:left>No, treats words independently</td></tr><tr><td style=text-align:left><strong>Semantic Meaning</strong></td><td style=text-align:left>Captures semantic relationships</td><td style=text-align:left>No, only counts importance based on corpus</td><td style=text-align:left>No, only counts word frequencies</td></tr><tr><td style=text-align:left><strong>Dimensionality</strong></td><td style=text-align:left>Low (dense vectors)</td><td style=text-align:left>High (sparse matrix)</td><td style=text-align:left>High (sparse matrix)</td></tr><tr><td style=text-align:left><strong>Common Use Cases</strong></td><td style=text-align:left>Similarity, analogy, NLP tasks</td><td style=text-align:left>Text classification, relevance scoring</td><td style=text-align:left>Text classification, simple tasks</td></tr></tbody></table><footer class=footline></footer></div></div><div id=navigation><a class="nav nav-prev" href=/ds24/en/m2/02_nlp/ title="Natural Language Processing"><i class="fa fa-chevron-left"></i></a>
<a class="nav nav-next" href=/ds24/en/m2/02_nlp/2-seq2seq/ title="Seq2Seq & Transformers" style=margin-right:0><i class="fa fa-chevron-right"></i></a></div></section><div style=left:-1000px;overflow:scroll;position:absolute;top:-1000px;border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px><div style=border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px></div></div><script src=/ds24/js/clipboard.min.js?1728934061></script><script src=/ds24/js/perfect-scrollbar.min.js?1728934061></script><script src=/ds24/js/perfect-scrollbar.jquery.min.js?1728934061></script><script src=/ds24/js/jquery.sticky.js?1728934061></script><script src=/ds24/js/featherlight.min.js?1728934061></script><script src=/ds24/js/highlight.pack.js?1728934061></script><script>hljs.initHighlightingOnLoad()</script><script src=/ds24/js/modernizr.custom-3.6.0.js?1728934061></script><script src=/ds24/js/learn.js?1728934061></script><script src=/ds24/js/hugo-learn.js?1728934061></script><script src=/ds24/mermaid/mermaid.js?1728934061></script><script>mermaid.initialize({startOnLoad:!0})</script><script>(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105947713-1","auto"),ga("send","pageview")</script></body></html>