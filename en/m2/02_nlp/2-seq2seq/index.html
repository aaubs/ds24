<!doctype html><html lang=en class="js csstransforms3d"><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.135.0"><meta name=description content="Social / Business Data Science 2024"><meta name=author content="Roman Jurowetzki & Daniel Hain"><link rel=icon href=/ds24/images/favicon.png type=image/png><title>Seq2Seq & Transformers :: Social / Business Data Science 2024</title>
<link href=/ds24/css/nucleus.css?1728934061 rel=stylesheet><link href=/ds24/css/fontawesome-all.min.css?1728934061 rel=stylesheet><link href=/ds24/css/hybrid.css?1728934061 rel=stylesheet><link href=/ds24/css/featherlight.min.css?1728934061 rel=stylesheet><link href=/ds24/css/perfect-scrollbar.min.css?1728934061 rel=stylesheet><link href=/ds24/css/auto-complete.css?1728934061 rel=stylesheet><link href=/ds24/css/atom-one-dark-reasonable.css?1728934061 rel=stylesheet><link href=/ds24/css/theme.css?1728934061 rel=stylesheet><link href=/ds24/css/tabs.css?1728934061 rel=stylesheet><link href=/ds24/css/hugo-theme.css?1728934061 rel=stylesheet><script src=/ds24/js/jquery-3.3.1.min.js?1728934061></script><style>:root #header+#content>#left>#rlblock_left{display:none!important}</style></head><body data-url=/ds24/en/m2/02_nlp/2-seq2seq/><nav id=sidebar class=showVisitedLinks><div id=header-wrapper><div id=header><a href=https://aaubs.github.io/ds24/><img alt=logo src=/ds24/images/logo.png>
</a>AAUBS Data Science 2024</div><div class=searchbox><label for=search-by><i class="fas fa-search"></i></label>
<input data-search-input id=search-by type=search placeholder=Search...>
<span data-search-clear><i class="fas fa-times"></i></span></div><script type=text/javascript src=/ds24/js/lunr.min.js?1728934061></script><script type=text/javascript src=/ds24/js/auto-complete.js?1728934061></script><script type=text/javascript>var baseurl="https://aaubs.github.io/ds24/"</script><script type=text/javascript src=/ds24/js/search.js?1728934061></script></div><div class=highlightable><ul class=topics><li data-nav-id=/ds24/en/info/ title="Info, Schedule & Co" class=dd-item><a href=/ds24/en/info/>Info, Schedule & Co
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/ds24/en/info/02_modules/ title=Modules class=dd-item><a href=/ds24/en/info/02_modules/>Modules
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/info/04_litetrature/ title="Literature & Resources" class=dd-item><a href=/ds24/en/info/04_litetrature/>Literature & Resources
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/info/03_schedule/ title="Semester Schedule" class=dd-item><a href=/ds24/en/info/03_schedule/>Semester Schedule
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/info/05_requirements_project/ title="Semester Project Requirements" class=dd-item><a href=/ds24/en/info/05_requirements_project/>Semester Project Requirements
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/ds24/en/m1/ title="Applied Data Science and Machine Learning" class=dd-item><a href=/ds24/en/m1/><b>1. </b>Applied Data Science and Machine Learning
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/ds24/en/m1/01_intro_ds/ title="A) Introduction to Data Science (W35-36)" class=dd-item><a href=/ds24/en/m1/01_intro_ds/>A) Introduction to Data Science (W35-36)
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/ds24/en/m1/01_intro_ds/01_welcome_ds/ title="- Welcome Students!" class=dd-item><a href=/ds24/en/m1/01_intro_ds/01_welcome_ds/>- Welcome Students!
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/m1/01_intro_ds/02_data_handeling/ title="- Data Handling and Manipulation" class=dd-item><a href=/ds24/en/m1/01_intro_ds/02_data_handeling/>- Data Handling and Manipulation
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/m1/01_intro_ds/03_data_visualization_stat/ title="- Exploratory Data Analysis and Essential Statistics" class=dd-item><a href=/ds24/en/m1/01_intro_ds/03_data_visualization_stat/>- Exploratory Data Analysis and Essential Statistics
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/m1/01_intro_ds/05_data_visualization_ds/ title="- Data Visualization in Data Science" class=dd-item><a href=/ds24/en/m1/01_intro_ds/05_data_visualization_ds/>- Data Visualization in Data Science
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/ds24/en/m1/02_rapid_prototyping/ title="B) Rapid Prototyping (W37)" class=dd-item><a href=/ds24/en/m1/02_rapid_prototyping/>B) Rapid Prototyping (W37)
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/ds24/en/m1/02_rapid_prototyping/01_rapid_prototyping/ title="Introduction to Streamlit: Building an Employee Attrition Dashboard" class=dd-item><a href=/ds24/en/m1/02_rapid_prototyping/01_rapid_prototyping/>Introduction to Streamlit: Building an Employee Attrition Dashboard
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/m1/02_rapid_prototyping/02_online_dashboard/ title=GeoPandas class=dd-item><a href=/ds24/en/m1/02_rapid_prototyping/02_online_dashboard/>GeoPandas
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/ds24/en/m1/03_uml/ title="C) Intro to Unsupervised Machine Learning (W38)" class=dd-item><a href=/ds24/en/m1/03_uml/>C) Intro to Unsupervised Machine Learning (W38)
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/ds24/en/m1/03_uml/01_intro_uml/ title="- Introduction to Unsupervised ML" class=dd-item><a href=/ds24/en/m1/03_uml/01_intro_uml/>- Introduction to Unsupervised ML
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/m1/03_uml/02_recommender_simsea_uml/ title="- Recommendation and Similarity Search" class=dd-item><a href=/ds24/en/m1/03_uml/02_recommender_simsea_uml/>- Recommendation and Similarity Search
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/m1/03_uml/04_intro_kmeans/ title="- Introduction to Clustering: K-means and Hierarchical Approaches" class=dd-item><a href=/ds24/en/m1/03_uml/04_intro_kmeans/>- Introduction to Clustering: K-means and Hierarchical Approaches
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/ds24/en/m1/04_sml/ title="D) Intro to Supervised Machine Learning (W39)" class=dd-item><a href=/ds24/en/m1/04_sml/>D) Intro to Supervised Machine Learning (W39)
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/ds24/en/m1/04_sml/01-sml-intro/ title="- Introduction to Supervised ML" class=dd-item><a href=/ds24/en/m1/04_sml/01-sml-intro/>- Introduction to Supervised ML
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/m1/04_sml/02-sml-addon/ title="- SML - Further topics" class=dd-item><a href=/ds24/en/m1/04_sml/02-sml-addon/>- SML - Further topics
<i class="fas fa-check read-icon"></i></a></li></ul></li></ul></li><li data-nav-id=/ds24/en/m2/ title="NLP & Network Analysis" class="dd-item
parent"><a href=/ds24/en/m2/><b>2. </b>NLP & Network Analysis
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/ds24/en/m2/02_nlp/ title="Natural Language Processing" class="dd-item
parent"><a href=/ds24/en/m2/02_nlp/>Natural Language Processing
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/ds24/en/m2/02_nlp/1-nlb-intro/ title="Basics of NLP" class=dd-item><a href=/ds24/en/m2/02_nlp/1-nlb-intro/>Basics of NLP
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/m2/02_nlp/2-seq2seq/ title="Seq2Seq & Transformers" class="dd-item active"><a href=/ds24/en/m2/02_nlp/2-seq2seq/>Seq2Seq & Transformers
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/ds24/en/m2/02_nlp/5-nlp-pipelines/ title="NLP and LLM Pipelines" class=dd-item><a href=/ds24/en/m2/02_nlp/5-nlp-pipelines/>NLP and LLM Pipelines
<i class="fas fa-check read-icon"></i></a></li></ul></li></ul></li></ul><section id=prefooter><hr><ul><li><a class=padding href=# data-clear-history-toggle><i class="fas fa-history fa-fw"></i> Clear History</a></li></ul></section><section id=footer><center><p>Built with <a href=https://github.com/matcornic/hugo-theme-learn><i class="fas fa-heart"></i></a> from <a href=https://getgrav.org>Grav</a> and <a href=https://gohugo.io/>Hugo</a></p></center><script async defer src=https://buttons.github.io/buttons.js></script></section></div></nav><section id=body><div id=overlay></div><div class="padding highlightable"><div><div id=top-bar><div id=breadcrumbs itemscope itemtype=http://data-vocabulary.org/Breadcrumb><span id=sidebar-toggle-span><a href=# id=sidebar-toggle data-sidebar-toggle><i class="fas fa-bars"></i>
</a></span><span class=links><a href=/ds24/en/>Social / Business Data Science 2024</a> > <a href=/ds24/en/m2/>NLP & Network Analysis</a> > <a href=/ds24/en/m2/02_nlp/>Natural Language Processing</a> > Seq2Seq & Transformers
          
        </span></div></div></div><div id=head-tags></div><div id=body-inner><h1>Seq2Seq & Transformers</h1><p><strong>NOTEBOOKS:</strong></p><ul><li><a href=https://colab.research.google.com/github/aaubs/ds-master/blob/main/notebooks/M2_Seq2Seq_BERT_trans_learning.ipynb target=_blank>Trans learning with BERT</a></li></ul><h2 id=seq2seq-the-next-step-in-nlp-after-word2vec>Seq2Seq: The Next Step in NLP After Word2Vec</h2><p>==================================</p><p><strong>Word2Vec</strong> was great at learning word representations, but it lacked the ability to process or generate <strong>sequences</strong> (e.g., sentences).<br><br><br><br></p><p><strong>What Seq2Seq Introduced:</strong></p><ul><li>Seq2Seq models are designed for tasks where both <strong>input and output</strong> are sequences, such as:<ul><li><strong>Machine translation</strong> (English to French)</li><li><strong>Text summarization</strong></li><li><strong>Chatbot responses</strong></li></ul></li></ul><p><br><br><br><br></p><h2 id=how-seq2seq-works>How Seq2Seq Works:</h2><p>==================================</p><ol><li><strong>Encoder-Decoder Architecture</strong>:<ul><li><p><strong>Encoder</strong>: Processes the input sequence (e.g., a sentence) and converts it into a fixed-size vector (context vector).</p></li><li><p><strong>Decoder</strong>: Takes the context vector and generates the output sequence (e.g., the translated sentence).</p><div style=text-align:center></li></ul><img src=https://raw.githubusercontent.com/aaubs/ds24/refs/heads/master/static/images/seq2seq.png alt=Only style=width:12cm></li></ol></div><p><br><br><br><br></p><p>==================================</p><h2 id=from-rnn-to-lstm-to-transformers-seq2seq>From RNN to LSTM to Transformers (seq2seq)</h2><p>==================================</p><h3 id=1-rnn-recurrent-neural-networks>1. RNN (Recurrent Neural Networks)</h3><p><strong>Introduced</strong>: 1980s</p><p><strong>Strength</strong>: First model to handle sequences by using a &ldquo;hidden state&rdquo; to retain information.</p><p><strong>Limitation</strong>:</p><ul><li><strong>Vanishing Gradient Problem</strong>: Struggles to retain information over long sequences.</li><li>Difficult to process long sentences or paragraphs.</li><li>Sequential processing makes them slow to train.</li></ul><p><br><br><br><br></p><h3 id=2-lstm-long-short-term-memory>2. LSTM (Long Short-Term Memory)</h3><p><strong>Introduced</strong>: Paper published 1997</p><p><strong>Strength</strong>: Improved version of RNN designed to solve the vanishing gradient problem.</p><p><strong>Key Feature</strong>:</p><ul><li><strong>Gating Mechanism</strong> (input, forget, output gates) allows selective retention of information.</li><li>Better at handling <strong>long-term dependencies</strong>.</li></ul><p><strong>Limitation</strong>:</p><ul><li>Still processes words <strong>sequentially</strong>, limiting training speed and efficiency.</li></ul><p><br><br><br><br></p><h3 id=3-transformers>3. Transformers</h3><p><strong>Introduced</strong>: 2017 (by Google researchers)</p><p><strong>Strength</strong>: Revolutionized sequence processing with <strong>self-attention</strong> and <strong>parallelization</strong>.</p><p><strong>Key Features</strong>:</p><ul><li><strong>Self-Attention</strong>: Enables the model to focus on all parts of the input sequence at once, handling long-range dependencies effectively.</li><li><strong>Parallel Processing</strong>: Unlike RNNs/LSTMs, Transformers process all words simultaneously, making them much faster to train.</li></ul><p><strong>Results</strong>: Can handle massive datasets and generate <strong>state-of-the-art results</strong> in translation, summarization, and text generation (e.g., GPT-3, BERT).</p><p><br><br><br><br></p><h3 id=why-transformers-are-important>Why Transformers Are Important</h3><p>==================================</p><div style=text-align:center><img src=https://raw.githubusercontent.com/aaubs/ds24/refs/heads/master/static/images/tramsformers.png alt=Only style=width:6cm></div><p><strong>Transformers</strong> overcame these challenges:</p><ul><li>Highly <strong>parallelizable</strong>, allowing efficient training on large datasets.</li><li>Scales well with dataâ€”models like <strong>GPT-3</strong> were trained on <strong>45 terabytes</strong> of text!<br><br><br><br></li></ul><h2 id=key-elements-of-transformers-1>Key elements of Transformers [1]</h2><p>==================================</p><ol><li><p><strong>Positional Encodings</strong>:</p><ul><li>Instead of sequential processing like RNNs, Transformers use <strong>positional encodings</strong> to capture the order of words.</li><li>Words are tagged with a position number (e.g., 1, 2, 3), and the model learns how to interpret the order during training.</li></ul><p><strong>Example:</strong> &lsquo;hot dog&rsquo; together have differnt meaning as &lsquo;dog that is hot&rsquo;</p></li><li><p><strong>Attention Mechanism</strong>:</p></li></ol><table style=width:100%;border:0><tr><td style=width:55%;border:0><b></b><br>Attention allows the model to focus on specific parts of the input sentence when making predictions<br><br>It works like a heatmap for how much focus the model gives to certain words.<br><br><br><br><br><br><br><br></td><td style=width:45%;border:0><div style=text-align:center><img src=https://raw.githubusercontent.com/aaubs/ds24/refs/heads/master/static/images/heatmap.png alt=Only style=width:7cm></div></td></tr></table><ol start=3><li><strong>Self-Attention</strong>:<ul><li>An extension of attention where the model looks at the whole input sequence to understand the meaning of a word in context.</li></ul></li></ol><p><strong>Example:</strong></p><p><em><strong>The animal didn&rsquo;t cross the street because <span style=color:blue>it</span> was too <span style=color:red>tired</span>.</strong></em></p><p><em><strong>The animal didn&rsquo;t cross the street because <span style=color:blue>it</span> was too <span style=color:red>wide</span>.</strong></em></p><p>Self-attention allows the model to recognize that &ldquo;it&rdquo; refers to &ldquo;the animal&rdquo; or &ldquo;the street&rdquo;!</p><p><br><br><br><br></p><h2 id=transfer-learning-fine-tuning-pre-trained-models>Transfer Learning: Fine-tuning Pre-trained Models</h2><p>==================================</p><p>Transfer learning is a crucial technique in NLP, especially with the advent of large pre-trained models like <strong>BERT</strong> (Bidirectional Encoder Representations from Transformers), <strong>GPT</strong>, and others. These models are trained on massive datasets and can be fine-tuned on specific tasks such as <strong>binary classification</strong> (e.g., classifying reviews as positive or negative) by leveraging their learned knowledge.</p><h3 id=how-transfer-learning-works>How Transfer Learning Works:</h3><ol><li><p><strong>Pre-training</strong>:</p><ul><li>A large model like BERT is first trained on a massive corpus of text to learn general language patterns.</li><li>The model learns rich representations of language, such as grammar, semantics, and context.</li></ul></li><li><p><strong>Fine-tuning</strong>:</p><ul><li>After pre-training, the model is fine-tuned on your specific dataset (e.g., sentiment analysis or classification tasks).</li><li>During this phase, the model adjusts its weights based on the specific examples in your dataset to learn the target task.</li></ul></li></ol><h3 id=example-binary-classification-with-bert>Example: Binary Classification with BERT</h3><ol><li><strong>Dataset Preparation</strong>: Prepare a dataset with two classes (e.g., positive and negative reviews).</li><li><strong>Model Setup</strong>: Use a pre-trained BERT model from libraries like <strong>HuggingFace&rsquo;s Transformers</strong>.</li><li><strong>Fine-tuning</strong>: Train the BERT model on your dataset, adjusting the final layer for a binary classification task.</li><li><strong>Evaluation</strong>: After fine-tuning, evaluate the model on unseen test data to check performance.</li></ol><h3 id=advantages-of-transfer-learning>Advantages of Transfer Learning:</h3><ul><li><strong>Faster Training</strong>: Pre-trained models already know a lot about language, so they require less data and time to train on specific tasks.</li><li><strong>Better Accuracy</strong>: Transfer learning often leads to improved performance, especially when data is scarce.</li></ul><div style=text-align:center><img src=https://raw.githubusercontent.com/aaubs/ds24/refs/heads/master/static/images/trans.jfif alt=Only style=width:10cm></div><footer class=footline></footer></div></div><div id=navigation><a class="nav nav-prev" href=/ds24/en/m2/02_nlp/1-nlb-intro/ title="Basics of NLP"><i class="fa fa-chevron-left"></i></a>
<a class="nav nav-next" href=/ds24/en/m2/02_nlp/5-nlp-pipelines/ title="NLP and LLM Pipelines" style=margin-right:0><i class="fa fa-chevron-right"></i></a></div></section><div style=left:-1000px;overflow:scroll;position:absolute;top:-1000px;border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px><div style=border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px></div></div><script src=/ds24/js/clipboard.min.js?1728934061></script><script src=/ds24/js/perfect-scrollbar.min.js?1728934061></script><script src=/ds24/js/perfect-scrollbar.jquery.min.js?1728934061></script><script src=/ds24/js/jquery.sticky.js?1728934061></script><script src=/ds24/js/featherlight.min.js?1728934061></script><script src=/ds24/js/highlight.pack.js?1728934061></script><script>hljs.initHighlightingOnLoad()</script><script src=/ds24/js/modernizr.custom-3.6.0.js?1728934061></script><script src=/ds24/js/learn.js?1728934061></script><script src=/ds24/js/hugo-learn.js?1728934061></script><script src=/ds24/mermaid/mermaid.js?1728934061></script><script>mermaid.initialize({startOnLoad:!0})</script><script>(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105947713-1","auto"),ga("send","pageview")</script></body></html>